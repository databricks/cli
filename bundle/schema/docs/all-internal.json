{
  "openapi" : "3.1.0",
  "paths" : {
    "/api/1.2/commands/cancel" : {
      "post" : {
        "summary" : "Cancel a command",
        "operationId" : "CommandExecution.cancel",
        "tags" : [ "Command Execution" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Status was returned successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/commands.CancelCommand"
              }
            }
          },
          "description" : ""
        },
        "description" : "Cancels a currently running command within an execution context.\n\nThe command ID is obtained from a prior successful call to __execute__. \n",
        "x-databricks-wait" : {
          "poll" : "commandStatus",
          "binding" : {
            "clusterId" : {
              "request" : "clusterId"
            },
            "commandId" : {
              "request" : "commandId"
            },
            "contextId" : {
              "request" : "contextId"
            }
          },
          "field" : [ "status" ],
          "success" : [ "Cancelled" ],
          "failure" : [ "Error" ],
          "message" : [ "results", "cause" ]
        }
      }
    },
    "/api/1.2/commands/execute" : {
      "post" : {
        "summary" : "Run a command",
        "operationId" : "CommandExecution.execute",
        "tags" : [ "Command Execution" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/commands.Created"
                },
                "example" : {
                  "id" : "1234ab56-7890-1cde-234f-5abcdef67890"
                }
              }
            },
            "description" : "Status was returned successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/commands.Command"
              },
              "example" : {
                "clusterId" : "1234-567890-span123",
                "command" : "print('Hello, World!')",
                "contextId" : "1234567890123456789",
                "language" : "python"
              }
            }
          },
          "description" : ""
        },
        "description" : "Runs a cluster command in the given execution context, using the provided language.\n\nIf successful, it returns an ID for tracking the status of the command's execution.\n",
        "x-databricks-wait" : {
          "poll" : "commandStatus",
          "binding" : {
            "clusterId" : {
              "request" : "clusterId"
            },
            "commandId" : {
              "response" : "id"
            },
            "contextId" : {
              "request" : "contextId"
            }
          },
          "field" : [ "status" ],
          "success" : [ "Finished", "Error" ],
          "failure" : [ "Cancelled", "Cancelling" ]
        }
      }
    },
    "/api/1.2/commands/status" : {
      "get" : {
        "summary" : "Get command info",
        "operationId" : "CommandExecution.commandStatus",
        "tags" : [ "Command Execution" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "clusterId",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "query",
          "name" : "contextId",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "query",
          "name" : "commandId",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/commands.CommandStatusResponse"
                },
                "example" : {
                  "id" : "1234ab56-7890-1cde-234f-5abcdef67890",
                  "results" : {
                    "data" : "Hello, World!",
                    "resultType" : "text"
                  },
                  "status" : "Finished"
                }
              }
            },
            "description" : "Status was returned successfully."
          }
        },
        "description" : "Gets the status of and, if available, the results from a currently executing command.\n\nThe command ID is obtained from a prior successful call to __execute__.\n"
      }
    },
    "/api/1.2/contexts/create" : {
      "post" : {
        "summary" : "Create an execution context",
        "operationId" : "CommandExecution.create",
        "tags" : [ "Command Execution" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/commands.Created"
                },
                "example" : {
                  "id" : "1234567890123456789"
                }
              }
            },
            "description" : "Status was returned successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/commands.CreateContext"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates an execution context for running cluster commands.\n\nIf successful, this method returns the ID of the new execution context.\n",
        "x-databricks-wait" : {
          "poll" : "contextStatus",
          "binding" : {
            "clusterId" : {
              "request" : "clusterId"
            },
            "contextId" : {
              "response" : "id"
            }
          },
          "field" : [ "status" ],
          "success" : [ "Running" ],
          "failure" : [ "Error" ]
        }
      }
    },
    "/api/1.2/contexts/destroy" : {
      "post" : {
        "summary" : "Delete an execution context",
        "operationId" : "CommandExecution.destroy",
        "tags" : [ "Command Execution" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Status was returned successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/commands.DestroyContext"
              }
            }
          },
          "description" : ""
        },
        "description" : "Deletes an execution context."
      }
    },
    "/api/1.2/contexts/status" : {
      "get" : {
        "summary" : "Get status",
        "operationId" : "CommandExecution.contextStatus",
        "tags" : [ "Command Execution" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "clusterId",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "query",
          "name" : "contextId",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/commands.ContextStatusResponse"
                },
                "example" : {
                  "id" : "1234567890123456789",
                  "status" : "Running"
                }
              }
            },
            "description" : "Status was returned successfully."
          }
        },
        "description" : "Gets the status for an execution context."
      }
    },
    "/api/2.0/accounts/{account_id}/budget" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/billing.account_id"
      } ],
      "get" : {
        "summary" : "Get all budgets",
        "operationId" : "Budgets.list",
        "tags" : [ "Budgets" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/billing.BudgetList"
                }
              }
            },
            "description" : "The list of budgets was successfully returned."
          }
        },
        "description" : "Gets all budgets associated with this account, including noncumulative status for each day that the budget is configured to include.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "budgets"
        }
      },
      "post" : {
        "summary" : "Create a new budget",
        "operationId" : "Budgets.create",
        "tags" : [ "Budgets" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/billing.WrappedBudgetWithStatus"
                }
              }
            },
            "description" : "The budget was successfully created."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/billing.WrappedBudget"
              }
            }
          },
          "description" : "Properties of the new budget"
        },
        "description" : "Creates a new budget in the specified account.",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/accounts/{account_id}/budget/{budget_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/billing.account_id"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "budget_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Budget ID"
      } ],
      "get" : {
        "summary" : "Get budget and its status",
        "operationId" : "Budgets.get",
        "tags" : [ "Budgets" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/billing.WrappedBudgetWithStatus"
                }
              }
            },
            "description" : "The budget was successfully returned."
          }
        },
        "description" : "Gets the budget specified by its UUID, including noncumulative status for each day that the budget is configured to include.",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Modify budget",
        "operationId" : "Budgets.update",
        "tags" : [ "Budgets" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Budget has been modified successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/billing.WrappedBudget"
              }
            }
          },
          "description" : "Properties to set the budget to"
        },
        "description" : "Modifies a budget in this account. Budget properties are completely overwritten."
      },
      "delete" : {
        "summary" : "Delete budget",
        "operationId" : "Budgets.delete",
        "tags" : [ "Budgets" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The budget that was successfully deleted."
          }
        },
        "description" : "Deletes the budget specified by its UUID.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/accounts/{account_id}/credentials" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      } ],
      "get" : {
        "summary" : "Get all credential configurations",
        "operationId" : "Credentials.list",
        "tags" : [ "Credential configurations" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.ListCredentialsResponse"
                }
              }
            },
            "description" : "Credential configurations were returned successfully."
          }
        },
        "description" : "Gets all Databricks credential configurations associated with an account specified by ID.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "inline" : true
        }
      },
      "post" : {
        "summary" : "Create credential configuration",
        "operationId" : "Credentials.create",
        "tags" : [ "Credential configurations" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.Credential"
                }
              }
            },
            "description" : "The credential configuration creation request succeeded."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/deployment.CreateCredentialRequest"
              }
            }
          },
          "description" : "Properties of the new credential configuration."
        },
        "description" : "Creates a Databricks credential configuration that represents cloud cross-account credentials for a specified account. Databricks uses this to set up network infrastructure properly to host Databricks clusters. For your AWS IAM role, you need to trust the External ID (the Databricks Account API account ID)  in the returned credential object, and configure the required access policy.\n\nSave the response's `credentials_id` field, which is the ID for your new credential configuration object.\n\nFor information about how to create a new workspace with this API, see [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html)",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "variable \"databricks_account_id\" {\n  description = \"Account Id that could be found in the bottom left corner of https://accounts.cloud.databricks.com/\"\n}\n\ndata \"databricks_aws_assume_role_policy\" \"this\" {\n  external_id = var.databricks_account_id\n}\n\nresource \"aws_iam_role\" \"cross_account_role\" {\n  name               = \"${local.prefix}-crossaccount\"\n  assume_role_policy = data.databricks_aws_assume_role_policy.this.json\n  tags               = var.tags\n}\n\ndata \"databricks_aws_crossaccount_policy\" \"this\" {\n}\n\nresource \"aws_iam_role_policy\" \"this\" {\n  name   = \"${local.prefix}-policy\"\n  role   = aws_iam_role.cross_account_role.id\n  policy = data.databricks_aws_crossaccount_policy.this.json\n}\n\nresource \"databricks_mws_credentials\" \"this\" {\n  provider         = databricks.mws\n  account_id       = var.databricks_account_id\n  credentials_name = \"${local.prefix}-creds\"\n  role_arn         = aws_iam_role.cross_account_role.arn\n}\n"
        } ]
      }
    },
    "/api/2.0/accounts/{account_id}/credentials/{credentials_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "credentials_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Databricks Account API credential configuration ID"
      } ],
      "get" : {
        "summary" : "Get credential configuration",
        "operationId" : "Credentials.get",
        "tags" : [ "Credential configurations" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.Credential"
                }
              }
            },
            "description" : "The credential configuration was successfully returned."
          }
        },
        "description" : "Gets a Databricks credential configuration object for an account, both specified by ID.",
        "x-databricks-crud" : "read"
      },
      "delete" : {
        "summary" : "Delete credential configuration",
        "operationId" : "Credentials.delete",
        "tags" : [ "Credential configurations" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The credential configuration was successfully deleted."
          }
        },
        "description" : "Deletes a Databricks credential configuration object for an account, both specified by ID. You cannot delete a credential that is associated with any workspace.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/accounts/{account_id}/customer-managed-keys" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      } ],
      "get" : {
        "summary" : "Get all encryption key configurations",
        "operationId" : "EncryptionKeys.list",
        "tags" : [ "Key configurations" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.ListCustomerManagedKeysResponse"
                }
              }
            },
            "description" : "The encryption key configurations were successfully returned."
          }
        },
        "description" : "Gets all customer-managed key configuration objects for an account. If the key is specified as a workspace's managed services customer-managed key, Databricks uses the key to encrypt the workspace's notebooks and secrets in the control plane, in addition to Databricks SQL queries and query history. If the key is specified as a workspace's storage customer-managed key, the key is used to encrypt the workspace's root S3 bucket and optionally can encrypt cluster EBS volumes data in the data plane.\n\n**Important**: Customer-managed keys are supported only for some deployment types, subscription types, and AWS regions.\n\nThis operation is available only if your account is on the E2 version of the platform.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "inline" : true
        }
      },
      "post" : {
        "summary" : "Create encryption key configuration",
        "operationId" : "EncryptionKeys.create",
        "tags" : [ "Key configurations" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.CustomerManagedKey"
                },
                "examples" : {
                  "All" : {
                    "value" : {
                      "creation_time" : 0,
                      "account_id" : "449e7a5c-69d3-4b8a-aaaf-5c9b713ebc65",
                      "customer_managed_key_id" : "680290f4-6931-497c-a6b4-514f6694e228",
                      "use_cases" : [ "MANAGED_SERVICES", "STORAGE" ],
                      "aws_key_info" : {
                        "key_alias" : "alias/projectKey",
                        "key_arn" : "arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321",
                        "key_region" : "us-west-2",
                        "reuse_key_for_cluster_volumes" : true
                      }
                    }
                  },
                  "Managed Services" : {
                    "value" : {
                      "creation_time" : 0,
                      "account_id" : "449e7a5c-69d3-4b8a-aaaf-5c9b713ebc65",
                      "customer_managed_key_id" : "680290f4-6931-497c-a6b4-514f6694e228",
                      "use_cases" : [ "MANAGED_SERVICES" ],
                      "aws_key_info" : {
                        "key_alias" : "alias/projectKey",
                        "key_arn" : "arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321",
                        "key_region" : "us-west-2"
                      }
                    }
                  },
                  "Storage" : {
                    "value" : {
                      "creation_time" : 0,
                      "account_id" : "449e7a5c-69d3-4b8a-aaaf-5c9b713ebc65",
                      "customer_managed_key_id" : "680290f4-6931-497c-a6b4-514f6694e228",
                      "use_cases" : [ "STORAGE" ],
                      "aws_key_info" : {
                        "key_alias" : "alias/projectKey",
                        "key_arn" : "arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321",
                        "key_region" : "us-west-2",
                        "reuse_key_for_cluster_volumes" : false
                      }
                    }
                  }
                }
              }
            },
            "description" : "The encryption key configuration was successfully created."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/deployment.CreateCustomerManagedKeyRequest"
              },
              "examples" : {
                "All" : {
                  "value" : {
                    "aws_key_info" : {
                      "key_alias" : "alias/projectKey",
                      "key_arn" : "arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321",
                      "reuse_key_for_cluster_volumes" : true
                    },
                    "use_cases" : [ "MANAGED_SERVICES", "STORAGE" ]
                  }
                },
                "Managed Services" : {
                  "value" : {
                    "aws_key_info" : {
                      "key_alias" : "alias/projectKey",
                      "key_arn" : "arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321"
                    },
                    "use_cases" : [ "MANAGED_SERVICES" ]
                  }
                },
                "Storage" : {
                  "value" : {
                    "aws_key_info" : {
                      "key_alias" : "alias/projectKey",
                      "key_arn" : "arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321",
                      "reuse_key_for_cluster_volumes" : false
                    },
                    "use_cases" : [ "STORAGE" ]
                  }
                }
              }
            }
          },
          "description" : "Properties of the encryption key configuration."
        },
        "description" : "Creates a customer-managed key configuration object for an account, specified by ID. This operation uploads a reference to a customer-managed key to Databricks. If the key is assigned as a workspace's customer-managed key for managed services, Databricks uses the key to encrypt the workspaces notebooks and secrets in the control plane, in addition to Databricks SQL queries and query history. If it is specified as a workspace's customer-managed key for workspace storage, the key encrypts the workspace's root S3 bucket (which contains the workspace's root DBFS and system data) and, optionally, cluster EBS volume data.\n\n**Important**: Customer-managed keys are supported only for some deployment types, subscription types, and AWS regions.\n\nThis operation is available only if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "variable \"databricks_account_id\" {\n  description = \"Account Id that could be found in the bottom left corner of https://accounts.cloud.databricks.com/\"\n}\n\ndata \"aws_caller_identity\" \"current\" {}\n\ndata \"aws_iam_policy_document\" \"databricks_managed_services_cmk\" {\n  version = \"2012-10-17\"\n  statement {\n    sid    = \"Enable IAM User Permissions\"\n    effect = \"Allow\"\n    principals {\n      type        = \"AWS\"\n      identifiers = [data.aws_caller_identity.current.account_id]\n    }\n    actions   = [\"kms:*\"]\n    resources = [\"*\"]\n  }\n  statement {\n    sid    = \"Allow Databricks to use KMS key for control plane managed services\"\n    effect = \"Allow\"\n    principals {\n      type        = \"AWS\"\n      identifiers = [\"arn:aws:iam::414351767826:root\"]\n    }\n    actions = [\n      \"kms:Encrypt\",\n      \"kms:Decrypt\"\n    ]\n    resources = [\"*\"]\n  }\n}\n\nresource \"aws_kms_key\" \"managed_services_customer_managed_key\" {\n  policy = data.aws_iam_policy_document.databricks_managed_services_cmk.json\n}\n\nresource \"aws_kms_alias\" \"managed_services_customer_managed_key_alias\" {\n  name          = \"alias/managed-services-customer-managed-key-alias\"\n  target_key_id = aws_kms_key.managed_services_customer_managed_key.key_id\n}\n\nresource \"databricks_mws_customer_managed_keys\" \"managed_services\" {\n  account_id = var.databricks_account_id\n  aws_key_info {\n    key_arn   = aws_kms_key.managed_services_customer_managed_key.arn\n    key_alias = aws_kms_alias.managed_services_customer_managed_key_alias.name\n  }\n  use_cases = [\"MANAGED_SERVICES\"]\n}\n"
        } ]
      }
    },
    "/api/2.0/accounts/{account_id}/customer-managed-keys/{customer_managed_key_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "customer_managed_key_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Databricks encryption key configuration ID."
      } ],
      "get" : {
        "summary" : "Get encryption key configuration",
        "operationId" : "EncryptionKeys.get",
        "tags" : [ "Key configurations" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.CustomerManagedKey"
                }
              }
            },
            "description" : "The encryption key configuration was successfully returned."
          }
        },
        "description" : "Gets a customer-managed key configuration object for an account, specified by ID. This operation uploads a reference to a customer-managed key to Databricks. If assigned as a workspace's customer-managed key for managed services, Databricks uses the key to encrypt the workspaces notebooks and secrets in the control plane, in addition to Databricks SQL queries and query history. If it is specified as a workspace's customer-managed key for storage, the key encrypts the workspace's root S3 bucket (which contains the workspace's root DBFS and system data) and, optionally, cluster EBS volume data.\n\n**Important**: Customer-managed keys are supported only for some deployment types, subscription types, and AWS regions.\n\nThis operation is available only if your account is on the E2 version of the platform.",
        "x-databricks-crud" : "read"
      },
      "delete" : {
        "summary" : "Delete encryption key configuration",
        "operationId" : "EncryptionKeys.delete",
        "tags" : [ "Key configurations" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The encryption key configuration was successfully deleted."
          }
        },
        "description" : "Deletes a customer-managed key configuration object for an account. You cannot delete a configuration that is associated with a running workspace.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/accounts/{account_id}/log-delivery" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/billing.account_id"
      } ],
      "get" : {
        "summary" : "Get all log delivery configurations",
        "operationId" : "LogDelivery.list",
        "tags" : [ "Log delivery configurations" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "status",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.LogDeliveryConfigStatus"
          },
          "description" : "Filter by status `ENABLED` or `DISABLED`."
        }, {
          "in" : "query",
          "name" : "credentials_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "Filter by credential configuration ID."
        }, {
          "in" : "query",
          "name" : "storage_configuration_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "Filter by storage configuration ID."
        }, {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/billing.WrappedLogDeliveryConfigurations"
                }
              }
            },
            "description" : "Log delivery configurations were returned successfully."
          }
        },
        "description" : "Gets all Databricks log delivery configurations associated with an account specified by ID.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "log_delivery_configurations"
        }
      },
      "post" : {
        "summary" : "Create a new log delivery configuration",
        "operationId" : "LogDelivery.create",
        "tags" : [ "Log delivery configurations" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/billing.WrappedLogDeliveryConfiguration"
                }
              }
            },
            "description" : "The log delivery configuration creation request succeeded."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/billing.WrappedCreateLogDeliveryConfiguration"
              }
            }
          },
          "description" : "Properties of the new log delivery configuration."
        },
        "description" : "Creates a new Databricks log delivery configuration to enable delivery of the specified type of logs to your storage location. This requires that you already created a [credential object](#operation/create-credential-config) (which encapsulates a cross-account service IAM role) and a [storage configuration object](#operation/create-storage-config) (which encapsulates an S3 bucket).\n\nFor full details, including the required IAM role policies and bucket policies, see [Deliver and access billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) or [Configure audit logging](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n\n**Note**: There is a limit on the number of log delivery configurations available per account (each limit applies separately to each log type including billable usage and audit logs). You can create a maximum of two enabled account-level delivery configurations (configurations without a workspace filter) per type. Additionally, you can create two enabled workspace-level delivery configurations per workspace for each log type, which means that the same workspace ID can occur in the workspace filter for no more than two delivery configurations per log type.\n\nYou cannot delete a log delivery configuration, but you can disable it (see [Enable or disable log delivery configuration](#operation/patch-log-delivery-config-status)).",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/accounts/{account_id}/log-delivery/{log_delivery_configuration_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/billing.account_id"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "log_delivery_configuration_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Databricks log delivery configuration ID"
      } ],
      "get" : {
        "summary" : "Get log delivery configuration",
        "operationId" : "LogDelivery.get",
        "tags" : [ "Log delivery configurations" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/billing.WrappedLogDeliveryConfiguration"
                }
              }
            },
            "description" : "The log delivery configuration was successfully returned."
          }
        },
        "description" : "Gets a Databricks log delivery configuration object for an account, both specified by ID.",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Enable or disable log delivery configuration",
        "operationId" : "LogDelivery.patchStatus",
        "tags" : [ "Log delivery configurations" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The log delivery configuration was successfully updated."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/billing.UpdateLogDeliveryConfigurationStatusRequest"
              }
            }
          },
          "description" : "The new status for this log delivery configuration object."
        },
        "description" : "Enables or disables a log delivery configuration. Deletion of delivery configurations is not supported, so disable log delivery configurations that are no longer needed. Note that you can't re-enable a delivery configuration if this would violate the delivery configuration limits described under [Create log delivery](#operation/create-log-delivery-config).",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.0/accounts/{account_id}/metastores" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.account_id"
      } ],
      "get" : {
        "summary" : "Get all metastores associated with an account",
        "operationId" : "AccountMetastores.list",
        "tags" : [ "Account Metastores" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListMetastoresResponse"
                }
              }
            },
            "description" : "Metastores were returned successfully."
          }
        },
        "description" : "Gets all Unity Catalog metastores associated with an account specified by ID.\n",
        "x-databricks-crud" : "list",
        "x-databricks-preview" : "PRIVATE"
      },
      "post" : {
        "summary" : "Create metastore",
        "operationId" : "AccountMetastores.create",
        "tags" : [ "Account Metastores" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.MetastoreInfo"
                }
              }
            },
            "description" : "The metastore was successfully created."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateMetastore"
              }
            }
          },
          "description" : "Properties of the new metastore."
        },
        "description" : "Creates a Unity Catalog metastore.",
        "x-databricks-crud" : "create",
        "x-databricks-preview" : "PRIVATE"
      }
    },
    "/api/2.0/accounts/{account_id}/metastores/{metastore_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.account_id"
      }, {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.metastore_id"
      } ],
      "get" : {
        "summary" : "Get a metastore",
        "operationId" : "AccountMetastores.get",
        "tags" : [ "Account Metastores" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "path",
          "name" : "metastore_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.MetastoreInfo"
                }
              }
            },
            "description" : "The metastore was successfully returned."
          }
        },
        "description" : "Gets a Databricks Unity Catalog metastore from an account, both specified by ID.",
        "x-databricks-crud" : "read",
        "x-databricks-preview" : "PRIVATE"
      },
      "put" : {
        "summary" : "Update a metastore",
        "operationId" : "AccountMetastores.update",
        "tags" : [ "Account Metastores" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.MetastoreInfo"
                }
              }
            },
            "description" : "The metastore update request succeeded."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateMetastore"
              }
            }
          },
          "description" : "Properties of the metastore to change."
        },
        "description" : "Updates an existing Unity Catalog metastore.",
        "x-databricks-crud" : "update",
        "x-databricks-preview" : "PRIVATE"
      },
      "delete" : {
        "summary" : "Delete a metastore",
        "operationId" : "AccountMetastores.delete",
        "tags" : [ "Account Metastores" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The metastore was successfully deleted."
          }
        },
        "description" : "Deletes a Databricks Unity Catalog metastore for an account, both specified by ID.",
        "x-databricks-crud" : "delete",
        "x-databricks-preview" : "PRIVATE"
      }
    },
    "/api/2.0/accounts/{account_id}/metastores/{metastore_id}/storage-credentials" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.account_id"
      }, {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.metastore_id"
      } ],
      "get" : {
        "summary" : "Get all storage credentials assigned to a metastore",
        "operationId" : "AccountStorageCredentials.list",
        "tags" : [ "Account Storage Credentials" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "path",
          "name" : "metastore_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListStorageCredentialsResponse"
                }
              }
            },
            "description" : "The metastore storage credentials were successfully returned."
          }
        },
        "description" : "Gets a list of all storage credentials that have been assigned to given metastore.",
        "x-databricks-crud" : "list",
        "x-databricks-preview" : "PRIVATE"
      },
      "post" : {
        "summary" : "Create a storage credential",
        "operationId" : "AccountStorageCredentials.create",
        "tags" : [ "Account Storage Credentials" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.StorageCredentialInfo"
                }
              }
            },
            "description" : "The new storage credential was successfully created."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateStorageCredential"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new storage credential.\nThe request object is specific to the cloud:\n\n  * **AwsIamRole** for AWS credentials\n  * **AzureServicePrincipal** for Azure credentials\n  * **GcpServiceAcountKey** for GCP credentials.\n\nThe caller must be a metastore admin and have the **CREATE_STORAGE_CREDENTIAL** privilege on the metastore.\n",
        "x-databricks-crud" : "create",
        "x-databricks-preview" : "PRIVATE"
      }
    },
    "/api/2.0/accounts/{account_id}/metastores/{metastore_id}/storage-credentials/{storage_credential_name}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.account_id"
      }, {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.metastore_id"
      }, {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.storage_credential_name"
      } ],
      "get" : {
        "summary" : "Gets the named storage credential",
        "operationId" : "AccountStorageCredentials.get",
        "tags" : [ "Account Storage Credentials" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "path",
          "name" : "metastore_id",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "path",
          "name" : "storage_credential_name",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.StorageCredentialInfo"
                }
              }
            },
            "description" : "The storage credential was successfully retrieved."
          }
        },
        "description" : "Gets a storage credential from the metastore.\nThe caller must be a metastore admin, the owner of the storage credential, or have a level of privilege on the storage credential.\n",
        "x-databricks-crud" : "read",
        "x-databricks-preview" : "PRIVATE"
      }
    },
    "/api/2.0/accounts/{account_id}/metastores/{metastore_id}/workspaces" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.account_id"
      }, {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.metastore_id"
      } ],
      "get" : {
        "summary" : "Get all workspaces assigned to a metastore",
        "operationId" : "AccountMetastoreAssignments.list",
        "tags" : [ "Account Metastore Assignments" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "path",
          "name" : "metastore_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListMetastoreAssignmentsResponse"
                }
              }
            },
            "description" : "The metastore assignments were successfully returned."
          }
        },
        "description" : "Gets a list of all Databricks workspace IDs that have been assigned to given metastore.",
        "x-databricks-crud" : "list",
        "x-databricks-preview" : "PRIVATE"
      }
    },
    "/api/2.0/accounts/{account_id}/networks" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      } ],
      "get" : {
        "summary" : "Get all network configurations",
        "operationId" : "Networks.list",
        "tags" : [ "Network configurations" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.ListNetworksResponse"
                }
              }
            },
            "description" : "The network configurations were successfully returned."
          }
        },
        "description" : "Gets a list of all Databricks network configurations for an account, specified by ID.\n\nThis operation is available only if your account is on the E2 version of the platform.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "inline" : true
        }
      },
      "post" : {
        "summary" : "Create network configuration",
        "operationId" : "Networks.create",
        "tags" : [ "Network configurations" ],
        "parameters" : [ {
          "ref" : true,
          "extRef" : true,
          "x-databricks-cloud" : "gcp",
          "$ref" : "#/components/parameters/deployment.GcpAccessToken"
        } ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.Network"
                }
              }
            },
            "description" : "The network configuration was successfully created."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/deployment.CreateNetworkRequest"
              }
            }
          },
          "description" : "Properties of the new network configuration."
        },
        "description" : "Creates a Databricks network configuration that represents an VPC and its resources. The VPC will be used for new Databricks clusters. This requires a pre-existing VPC and subnets.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "variable \"databricks_account_id\" {\n  description = \"Account Id that could be found in the bottom left corner of https://accounts.cloud.databricks.com/\"\n}\n\ndata \"aws_availability_zones\" \"available\" {}\n\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"2.70.0\"\n\n  name                  = local.prefix\n  cidr                  = var.cidr_block\n  secondary_cidr_blocks = [var.cidr_block_public]\n  azs                   = data.aws_availability_zones.available.names\n  tags                  = var.tags\n\n  enable_dns_hostnames = true\n  enable_nat_gateway   = true\n  create_igw           = true\n\n  public_subnets = [cidrsubnet(var.cidr_block_public, 6, 0)]\n  private_subnets = [cidrsubnet(var.cidr_block, 3, 1),\n  cidrsubnet(var.cidr_block, 3, 2)]\n\n  default_security_group_egress = [{\n    cidr_blocks = \"0.0.0.0/0\"\n  }]\n\n  default_security_group_ingress = [{\n    description = \"Allow all internal TCP and UDP\"\n    self        = true\n  }]\n}\n\nresource \"databricks_mws_networks\" \"this\" {\n  provider           = databricks.mws\n  account_id         = var.databricks_account_id\n  network_name       = \"${local.prefix}-network\"\n  security_group_ids = [module.vpc.default_security_group_id]\n  subnet_ids         = module.vpc.private_subnets\n  vpc_id             = module.vpc.vpc_id\n}\n"
        } ]
      }
    },
    "/api/2.0/accounts/{account_id}/networks/{network_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "network_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Databricks Account API network configuration ID."
      } ],
      "get" : {
        "summary" : "Get a network configuration",
        "operationId" : "Networks.get",
        "tags" : [ "Network configurations" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.Network"
                }
              }
            },
            "description" : "The network configuration was successfully returned."
          }
        },
        "description" : "Gets a Databricks network configuration, which represents a cloud VPC and its resources.",
        "x-databricks-crud" : "read"
      },
      "delete" : {
        "summary" : "Delete a network configuration",
        "operationId" : "Networks.delete",
        "tags" : [ "Network configurations" ],
        "parameters" : [ {
          "ref" : true,
          "extRef" : true,
          "x-databricks-cloud" : "gcp",
          "$ref" : "#/components/parameters/deployment.GcpAccessToken"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The network configuration was successfully deleted."
          }
        },
        "description" : "Deletes a Databricks network configuration, which represents a cloud VPC and its resources. You cannot delete a network that is associated with a workspace.\n\nThis operation is available only if your account is on the E2 version of the platform.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/accounts/{account_id}/private-access-settings" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      } ],
      "get" : {
        "summary" : "Get all private access settings objects",
        "operationId" : "PrivateAccess.list",
        "tags" : [ "Private Access Settings" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.ListPrivateAccessSettingsResponse"
                }
              }
            },
            "description" : "The private access settings object was successfully returned."
          }
        },
        "description" : "Gets a list of all private access settings objects for an account, specified by ID.\n\nThis operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for AWS PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "inline" : true
        }
      },
      "post" : {
        "summary" : "Create private access settings",
        "operationId" : "PrivateAccess.create",
        "tags" : [ "Private Access Settings" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.PrivateAccessSettings"
                }
              }
            },
            "description" : "The private access settings object was successfully created."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/deployment.UpsertPrivateAccessSettingsRequest"
              }
            }
          },
          "description" : "Properties of the new private access settings object."
        },
        "description" : "Creates a private access settings object, which specifies how your workspace is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink). To use AWS PrivateLink, a workspace must have a private access settings object referenced by ID in the workspace's `private_access_settings_id` property.\n\nYou can share one private access settings with multiple workspaces in a single account. However, private access settings are specific to AWS regions, so only workspaces in the same AWS region can use a given private access settings object.\n\nBefore configuring PrivateLink, read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n\nThis operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_mws_private_access_settings\" \"pas\" {\n  provider                     = databricks.mws\n  account_id                   = var.databricks_account_id\n  private_access_settings_name = \"Private Access Settings for ${local.prefix}\"\n  region                       = var.region\n  public_access_enabled        = true\n}\n"
        } ]
      }
    },
    "/api/2.0/accounts/{account_id}/private-access-settings/{private_access_settings_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "private_access_settings_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Databricks Account API private access settings ID."
      } ],
      "get" : {
        "summary" : "Get a private access settings object",
        "operationId" : "PrivateAccess.get",
        "tags" : [ "Private Access Settings" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.PrivateAccessSettings"
                }
              }
            },
            "description" : "The private access settings object was successfully returned."
          }
        },
        "description" : "Gets a private access settings object, which specifies how your workspace is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink).\n\nBefore configuring PrivateLink, read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n\nThis operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.",
        "x-databricks-crud" : "read"
      },
      "put" : {
        "summary" : "Replace private access settings",
        "operationId" : "PrivateAccess.replace",
        "tags" : [ "Private Access Settings" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The private access settings object was successfully updated."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/deployment.UpsertPrivateAccessSettingsRequest"
              }
            }
          },
          "description" : "Properties of the new private access settings object."
        },
        "description" : "Updates an existing private access settings object, which specifies how your workspace is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink). To use AWS PrivateLink, a workspace must have a private access settings object referenced by ID in the workspace's `private_access_settings_id` property.\n\nThis operation completely overwrites your existing private access settings object attached to your workspaces. All workspaces attached to the private access settings are affected by any change. If `public_access_enabled`, `private_access_level`, or `allowed_vpc_endpoint_ids` are updated, effects of these changes might take several minutes to propagate to the workspace API.\nYou can share one private access settings object with multiple workspaces in a single account. However, private access settings are specific to AWS regions, so only workspaces in the same AWS region can use a given private access settings object.\n\nBefore configuring PrivateLink, read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n\nThis operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a private access settings object",
        "operationId" : "PrivateAccess.delete",
        "tags" : [ "Private Access Settings" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The private access settings was successfully deleted."
          }
        },
        "description" : "Deletes a private access settings object, which determines how your workspace is accessed over [AWS PrivateLink](https://aws.amazon.com/privatelink).\n\nBefore configuring PrivateLink, read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n\nThis operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/accounts/{account_id}/scim/v2/Groups" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "type" : "string"
        },
        "description" : "Databricks Account ID"
      } ],
      "get" : {
        "summary" : "List group details",
        "operationId" : "AccountGroups.list",
        "tags" : [ "Account Groups" ],
        "parameters" : [ {
          "examples" : {
            "equals" : {
              "summary" : "Groups whose displayName equals admins.",
              "value" : "displayName eq admins"
            },
            "starts-with" : {
              "summary" : "Groups whose displayName starts with bar.",
              "value" : "displayName sw bar"
            },
            "or" : {
              "summary" : "Group whose displayName contains foo or displayName contains bar.",
              "value" : "displayName co foo or displayName co bar"
            },
            "not-equals" : {
              "summary" : "Groups whose displayName not equals baz.",
              "value" : "displayName ne baz"
            },
            "contains" : {
              "summary" : "Groups whose displayName contains foo.",
              "value" : "displayName co foo"
            },
            "and" : {
              "summary" : "Group whose displayName contains foo and displayName contains bar.",
              "value" : "displayName co foo and displayName co bar"
            }
          },
          "in" : "query",
          "name" : "filter",
          "schema" : {
            "type" : "string"
          },
          "description" : "Query by which the results have to be filtered. Supported operators are equals(`eq`), contains(`co`), starts with(`sw`) and not equals(`ne`). Additionally, simple expressions can be formed using logical operators - `and` and `or`. The [SCIM RFC](https://tools.ietf.org/html/rfc7644#section-3.4.2.2) has more details but we currently only support simple expressions."
        }, {
          "in" : "query",
          "name" : "attributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to return in response."
        }, {
          "in" : "query",
          "name" : "excludedAttributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to exclude in response."
        }, {
          "in" : "query",
          "name" : "startIndex",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Specifies the index of the first result. First item is number 1."
        }, {
          "in" : "query",
          "name" : "count",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Desired number of results per page."
        }, {
          "in" : "query",
          "name" : "sortBy",
          "schema" : {
            "type" : "string"
          },
          "description" : "Attribute to sort the results."
        }, {
          "in" : "query",
          "name" : "sortOrder",
          "schema" : {
            "type" : "string",
            "enum" : [ "ascending", "descending" ]
          },
          "description" : "The order to sort the results."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.ListGroupsResponse"
                }
              }
            },
            "description" : "List groups operation was succesful."
          }
        },
        "description" : "Gets all details of the groups associated with the Databricks Account.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "Resources"
        }
      },
      "post" : {
        "summary" : "Create a new group",
        "operationId" : "AccountGroups.create",
        "tags" : [ "Account Groups" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.Group"
                }
              }
            },
            "description" : "The group creation request succeeded."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.Group"
              }
            }
          },
          "description" : "Properties of the new group."
        },
        "description" : "Creates a group in the Databricks Account with a unique name, using the supplied group details.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_group\" \"this\" {\n  display_name               = \"Some Group\"\n  allow_cluster_create       = true\n  allow_instance_pool_create = true\n}\n\nresource \"databricks_user\" \"this\" {\n  user_name = \"someone@example.com\"\n}\n\nresource \"databricks_group_member\" \"vip_member\" {\n  group_id  = databricks_group.this.id\n  member_id = databricks_user.this.id\n}\n"
        } ]
      }
    },
    "/api/2.0/accounts/{account_id}/scim/v2/Groups/{id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "type" : "string"
        },
        "description" : "Databricks Account ID"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "id",
        "schema" : {
          "format" : "int64",
          "type" : "string"
        },
        "description" : "Unique ID for a group in the Databricks Account."
      } ],
      "get" : {
        "summary" : "Get group details",
        "operationId" : "AccountGroups.get",
        "tags" : [ "Account Groups" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.Group"
                }
              }
            },
            "description" : "Group information was returned successfully."
          }
        },
        "description" : "Gets the information for a specific group in the Databricks Account.",
        "x-databricks-crud" : "read"
      },
      "put" : {
        "summary" : "Replace a group",
        "operationId" : "AccountGroups.update",
        "tags" : [ "Account Groups" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Group information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.Group"
              }
            }
          },
          "description" : "Operations to be applied on group information."
        },
        "description" : "Updates the details of a group by replacing the entire group entity.",
        "x-databricks-crud" : "update"
      },
      "patch" : {
        "summary" : "Update group details",
        "operationId" : "AccountGroups.patch",
        "tags" : [ "Account Groups" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Group information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.PartialUpdate"
              }
            }
          },
          "description" : "Operations to be applied on group information."
        },
        "description" : "Partially updates the details of a group."
      },
      "delete" : {
        "summary" : "Delete a group",
        "operationId" : "AccountGroups.delete",
        "tags" : [ "Account Groups" ],
        "responses" : {
          "204" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Group was deleted successfully."
          }
        },
        "description" : "Deletes a group from the Databricks Account.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/accounts/{account_id}/scim/v2/ServicePrincipals" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "type" : "string"
        },
        "description" : "Databricks Account ID"
      } ],
      "get" : {
        "summary" : "List service principals",
        "operationId" : "AccountServicePrincipals.list",
        "tags" : [ "Account Service Principals" ],
        "parameters" : [ {
          "examples" : {
            "equals" : {
              "summary" : "Service principals whose displayName equals foo.",
              "value" : "displayName eq foo"
            },
            "starts-with" : {
              "summary" : "Service principals whose displayName starts with baz.",
              "value" : "displayName sw baz"
            },
            "or" : {
              "summary" : "Service principals whose displayName contains sp or displayName contains foo.",
              "value" : "displayName co sp or displayName co foo"
            },
            "not-equals" : {
              "summary" : "Service principals whose displayName not equals qux.",
              "value" : "displayName ne qux"
            },
            "contains" : {
              "summary" : "Service principals whose displayName contains bar.",
              "value" : "displayName co bar"
            },
            "and" : {
              "summary" : "Service principals whose displayName contains sp and displayName contains foo.",
              "value" : "displayName co sp and displayName co foo"
            }
          },
          "in" : "query",
          "name" : "filter",
          "schema" : {
            "type" : "string"
          },
          "description" : "Query by which the results have to be filtered. Supported operators are equals(`eq`), contains(`co`), starts with(`sw`) and not equals(`ne`). Additionally, simple expressions can be formed using logical operators - `and` and `or`. The [SCIM RFC](https://tools.ietf.org/html/rfc7644#section-3.4.2.2) has more details but we currently only support simple expressions."
        }, {
          "in" : "query",
          "name" : "attributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to return in response."
        }, {
          "in" : "query",
          "name" : "excludedAttributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to exclude in response."
        }, {
          "in" : "query",
          "name" : "startIndex",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Specifies the index of the first result. First item is number 1."
        }, {
          "in" : "query",
          "name" : "count",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Desired number of results per page."
        }, {
          "in" : "query",
          "name" : "sortBy",
          "schema" : {
            "type" : "string"
          },
          "description" : "Attribute to sort the results."
        }, {
          "in" : "query",
          "name" : "sortOrder",
          "schema" : {
            "type" : "string",
            "enum" : [ "ascending", "descending" ]
          },
          "description" : "The order to sort the results."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.ListServicePrincipalResponse"
                }
              }
            },
            "description" : "List service principals operation was succesful."
          }
        },
        "description" : "Gets the set of service principals associated with a Databricks Account.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "Resources"
        }
      },
      "post" : {
        "summary" : "Create a service principal",
        "operationId" : "AccountServicePrincipals.create",
        "tags" : [ "Account Service Principals" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.ServicePrincipal"
                }
              }
            },
            "description" : "The user creation request succeeded."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.ServicePrincipal"
              }
            }
          },
          "description" : "Properties of the new service principal."
        },
        "description" : "Creates a new service principal in the Databricks Account.",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/accounts/{account_id}/scim/v2/ServicePrincipals/{id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "type" : "string"
        },
        "description" : "Databricks Account ID"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "id",
        "schema" : {
          "format" : "int64",
          "type" : "string"
        },
        "description" : "Unique ID for a service principal in the Databricks Account."
      } ],
      "get" : {
        "summary" : "Get service principal details",
        "operationId" : "AccountServicePrincipals.get",
        "tags" : [ "Account Service Principals" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.ServicePrincipal"
                }
              }
            },
            "description" : "Service principal information was returned successfully."
          }
        },
        "description" : "Gets the details for a single service principal define in the Databricks Account.",
        "x-databricks-crud" : "read"
      },
      "put" : {
        "summary" : "Replace service principal",
        "operationId" : "AccountServicePrincipals.update",
        "tags" : [ "Account Service Principals" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Service principal information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.ServicePrincipal"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the details of a single service principal. \n\nThis action replaces the existing service principal with the same name.\n",
        "x-databricks-crud" : "update"
      },
      "patch" : {
        "summary" : "Update service principal details",
        "operationId" : "AccountServicePrincipals.patch",
        "tags" : [ "Account Service Principals" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Service principal information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.PartialUpdate"
              }
            }
          },
          "description" : "Operations to be applied on service principal information."
        },
        "description" : "Partially updates the details of a single service principal in the Databricks Account."
      },
      "delete" : {
        "summary" : "Delete a service principal",
        "operationId" : "AccountServicePrincipals.delete",
        "tags" : [ "Account Service Principals" ],
        "responses" : {
          "204" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Service principal was deleted successfully."
          }
        },
        "description" : "Delete a single service principal in the Databricks Account.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/accounts/{account_id}/scim/v2/Users" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "type" : "string"
        },
        "description" : "Databricks Account ID"
      } ],
      "get" : {
        "summary" : "List users",
        "operationId" : "AccountUsers.list",
        "tags" : [ "Account Users" ],
        "parameters" : [ {
          "examples" : {
            "equals" : {
              "summary" : "Users whose displayName equals john.",
              "value" : "displayName eq john"
            },
            "starts-with" : {
              "summary" : "Users whose displayName starts with jo.",
              "value" : "displayName sw jo"
            },
            "or" : {
              "summary" : "Users whose displayName contains john or userName contains doe.",
              "value" : "displayName co john or userName co doe"
            },
            "not-equals" : {
              "summary" : "Users whose displayName not equals john.",
              "value" : "displayName ne john"
            },
            "contains" : {
              "summary" : "Users whose displayName contains doe.",
              "value" : "displayName co doe"
            },
            "and" : {
              "summary" : "Users whose displayName contains john and userName contains doe.",
              "value" : "displayName co john and userName co doe"
            }
          },
          "in" : "query",
          "name" : "filter",
          "schema" : {
            "type" : "string"
          },
          "description" : "Query by which the results have to be filtered. Supported operators are equals(`eq`), contains(`co`), starts with(`sw`) and not equals(`ne`). Additionally, simple expressions can be formed using logical operators - `and` and `or`. The [SCIM RFC](https://tools.ietf.org/html/rfc7644#section-3.4.2.2) has more details but we currently only support simple expressions."
        }, {
          "in" : "query",
          "name" : "attributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to return in response."
        }, {
          "in" : "query",
          "name" : "excludedAttributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to exclude in response."
        }, {
          "in" : "query",
          "name" : "startIndex",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Specifies the index of the first result. First item is number 1."
        }, {
          "in" : "query",
          "name" : "count",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Desired number of results per page."
        }, {
          "in" : "query",
          "name" : "sortBy",
          "schema" : {
            "type" : "string"
          },
          "description" : "Attribute to sort the results. Multi-part paths are supported. For example, `userName`, `name.givenName`, and `emails`."
        }, {
          "in" : "query",
          "name" : "sortOrder",
          "schema" : {
            "type" : "string",
            "enum" : [ "ascending", "descending" ]
          },
          "description" : "The order to sort the results."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.ListUsersResponse"
                }
              }
            },
            "description" : "List users operation was succesful."
          }
        },
        "description" : "Gets details for all the users associated with a Databricks Account.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "Resources"
        }
      },
      "post" : {
        "summary" : "Create a new user",
        "operationId" : "AccountUsers.create",
        "tags" : [ "Account Users" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.User"
                }
              }
            },
            "description" : "The user creation request succeeded."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.User"
              }
            }
          },
          "description" : "Properties of the new user."
        },
        "description" : "Creates a new user in the Databricks Account. This new user will also be added to the Databricks account.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "data \"databricks_group\" \"admins\" {\n  display_name = \"admins\"\n}\n\nresource \"databricks_user\" \"me\" {\n  user_name = \"me@example.com\"\n}\n\nresource \"databricks_group_member\" \"i-am-admin\" {\n  group_id  = data.databricks_group.admins.id\n  member_id = databricks_user.me.id\n}\n"
        } ]
      }
    },
    "/api/2.0/accounts/{account_id}/scim/v2/Users/{id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "type" : "string"
        },
        "description" : "Databricks Account ID"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "id",
        "schema" : {
          "format" : "int64",
          "type" : "string"
        },
        "description" : "Unique ID for a user in the Databricks Account."
      } ],
      "get" : {
        "summary" : "Get user details",
        "operationId" : "AccountUsers.get",
        "tags" : [ "Account Users" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.User"
                }
              }
            },
            "description" : "User information was returned successfully."
          }
        },
        "description" : "Gets information for a specific user in Databricks Account.",
        "x-databricks-crud" : "read"
      },
      "put" : {
        "summary" : "Replace a user",
        "operationId" : "AccountUsers.update",
        "tags" : [ "Account Users" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "User information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.User"
              }
            }
          },
          "description" : ""
        },
        "description" : "Replaces a user's information with the data supplied in request.",
        "x-databricks-crud" : "update"
      },
      "patch" : {
        "summary" : "Update user details",
        "operationId" : "AccountUsers.patch",
        "tags" : [ "Account Users" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "User information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.PartialUpdate"
              }
            }
          },
          "description" : "Operations to be applied on user information."
        },
        "description" : "Partially updates a user resource by applying the supplied operations on specific user attributes."
      },
      "delete" : {
        "summary" : "Delete a user",
        "operationId" : "AccountUsers.delete",
        "tags" : [ "Account Users" ],
        "responses" : {
          "204" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "User was deleted successfully."
          }
        },
        "description" : "Deletes a user. Deleting a user from a Databricks Account also removes objects associated with the user.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/accounts/{account_id}/storage-configurations" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      } ],
      "get" : {
        "summary" : "Get all storage configurations",
        "operationId" : "Storage.list",
        "tags" : [ "Storage configurations" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.ListStorageConfigurationsResponse"
                }
              }
            },
            "description" : "The storage configurations were successfully returned."
          }
        },
        "description" : "Gets a list of all Databricks storage configurations for your account, specified by ID.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "inline" : true
        }
      },
      "post" : {
        "summary" : "Create new storage configuration",
        "operationId" : "Storage.create",
        "tags" : [ "Storage configurations" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.StorageConfiguration"
                }
              }
            },
            "description" : "The storage configuration was successfully created."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/deployment.CreateStorageConfigurationRequest"
              }
            }
          },
          "description" : "Properties of the new storage configuration."
        },
        "description" : "Creates new storage configuration for an account, specified by ID. Uploads a storage configuration object that represents the root AWS S3 bucket in your account. Databricks stores related workspace assets including DBFS, cluster logs, and job results. For the AWS S3 bucket, you need to configure the required bucket policy.\n\nFor information about how to create a new workspace with this API, see [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html)",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "variable \"databricks_account_id\" {\n  description = \"Account Id that could be found in the bottom left corner of https://accounts.cloud.databricks.com/\"\n}\n\nresource \"aws_s3_bucket\" \"root_storage_bucket\" {\n  bucket = \"${var.prefix}-rootbucket\"\n  acl    = \"private\"\n  versioning {\n    enabled = false\n  }\n}\n\nresource \"databricks_mws_storage_configurations\" \"this\" {\n  provider                   = databricks.mws\n  account_id                 = var.databricks_account_id\n  storage_configuration_name = \"${var.prefix}-storage\"\n  bucket_name                = aws_s3_bucket.root_storage_bucket.bucket\n}\n"
        } ]
      }
    },
    "/api/2.0/accounts/{account_id}/storage-configurations/{storage_configuration_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "storage_configuration_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Databricks Account API storage configuration ID."
      } ],
      "get" : {
        "summary" : "Get storage configuration",
        "operationId" : "Storage.get",
        "tags" : [ "Storage configurations" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.StorageConfiguration"
                }
              }
            },
            "description" : "The storage configuration was successfully returned."
          }
        },
        "description" : "Gets a Databricks storage configuration for an account, both specified by ID.",
        "x-databricks-crud" : "read"
      },
      "delete" : {
        "summary" : "Delete storage configuration",
        "operationId" : "Storage.delete",
        "tags" : [ "Storage configurations" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The storage configuration was successfully deleted."
          }
        },
        "description" : "Deletes a Databricks storage configuration. You cannot delete a storage configuration that is associated with any workspace.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/accounts/{account_id}/usage/download" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/billing.account_id"
      } ],
      "get" : {
        "summary" : "Return billable usage logs",
        "operationId" : "BillableUsage.download",
        "tags" : [ "Billable usage download" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "start_month",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.UsageDownloadMonth"
          },
          "description" : "Format: `YYYY-MM`. First month to return billable usage logs for. This field is required."
        }, {
          "required" : true,
          "in" : "query",
          "name" : "end_month",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.UsageDownloadMonth"
          },
          "description" : "Format: `YYYY-MM`. Last month to return billable usage logs for. This field is required."
        }, {
          "in" : "query",
          "name" : "personal_data",
          "schema" : {
            "type" : "boolean"
          },
          "description" : "Specify whether to include personally identifiable information in the billable usage logs, for example the email addresses of cluster creators. Handle this information with care. Defaults to false."
        }, {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/csv" : {
                "schema" : {
                  "type" : "string"
                }
              }
            },
            "description" : "Billable usage data was returned successfully."
          }
        },
        "description" : "Returns billable usage logs in CSV format for the specified account and date range. For the data schema, see [CSV file schema](https://docs.databricks.com/administration-guide/account-settings/usage-analysis.html#schema). Note that this method might take multiple seconds to complete."
      }
    },
    "/api/2.0/accounts/{account_id}/vpc-endpoints" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      } ],
      "get" : {
        "summary" : "Get all VPC endpoint configurations",
        "operationId" : "VpcEndpoints.list",
        "tags" : [ "VPC Endpoint Configurations" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.ListVPCEndpointsResponse"
                }
              }
            },
            "description" : "The VPC endpoints were successfully returned."
          }
        },
        "description" : "Gets a list of all VPC endpoints for an account, specified by ID.\n\nBefore configuring PrivateLink, read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n\nThis operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "inline" : true
        }
      },
      "post" : {
        "summary" : "Create VPC endpoint configuration",
        "operationId" : "VpcEndpoints.create",
        "tags" : [ "VPC Endpoint Configurations" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.VPCEndpoint"
                }
              }
            },
            "description" : "The VPC endpoint configuration was successfully created."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/deployment.CreateVPCEndpointRequest"
              }
            }
          },
          "description" : "Properties of the new VPC endpoint configuration."
        },
        "description" : "Creates a VPC endpoint configuration, which represents a [VPC endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints.html) object in AWS used to communicate privately with Databricks over [AWS PrivateLink](https://aws.amazon.com/privatelink).\n\n**Important**: When you register a VPC endpoint to the Databricks workspace VPC endpoint service for any workspace, **in this release Databricks enables front-end (web application and REST API) access from the source network of the VPC endpoint to all workspaces in that AWS region in your Databricks account if the workspaces have any PrivateLink connections in their workspace configuration**. If you have questions about this behavior, contact your Databricks representative.\n\nWithin AWS, your VPC endpoint stays in `pendingAcceptance` state until you register it in a VPC endpoint configuration through the Account API. After you register the VPC endpoint configuration, the Databricks [endpoint service](https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink-share-your-services.html) automatically accepts the VPC endpoint and it eventually transitions to the `available` state.\n\nBefore configuring PrivateLink, read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n\nThis operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_mws_vpc_endpoint\" \"workspace\" {\n  provider            = databricks.mws\n  account_id          = var.databricks_account_id\n  aws_vpc_endpoint_id = aws_vpc_endpoint.workspace.id\n  vpc_endpoint_name   = \"VPC Relay for ${module.vpc.vpc_id}\"\n  region              = var.region\n  depends_on          = [aws_vpc_endpoint.workspace]\n}\n"
        } ]
      }
    },
    "/api/2.0/accounts/{account_id}/vpc-endpoints/{vpc_endpoint_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "vpc_endpoint_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Databricks VPC endpoint ID."
      } ],
      "get" : {
        "summary" : "Get a VPC endpoint configuration",
        "operationId" : "VpcEndpoints.get",
        "tags" : [ "VPC Endpoint Configurations" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.VPCEndpoint"
                }
              }
            },
            "description" : "The VPC endpoint was successfully returned."
          }
        },
        "description" : "Gets a VPC endpoint configuration, which represents a [VPC endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html) object in AWS used to communicate privately with Databricks over [AWS PrivateLink](https://aws.amazon.com/privatelink).\n\nThis operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.",
        "x-databricks-crud" : "read"
      },
      "delete" : {
        "summary" : "Delete VPC endpoint configuration",
        "operationId" : "VpcEndpoints.delete",
        "tags" : [ "VPC Endpoint Configurations" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The VPC endpoint configuration was successfully deleted."
          }
        },
        "description" : "Deletes a VPC endpoint configuration, which represents an [AWS VPC endpoint](https://docs.aws.amazon.com/vpc/latest/privatelink/concepts.html) that can communicate privately with Databricks over [AWS PrivateLink](https://aws.amazon.com/privatelink).\n\nUpon deleting a VPC endpoint configuration, the VPC endpoint in AWS changes its state from `accepted` to `rejected`, which means that it is no longer usable from your VPC.\n\nBefore configuring PrivateLink, read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n\nThis operation is available only if your account is on the E2 version of the platform and your Databricks account is enabled for PrivateLink (Public Preview). Contact your Databricks representative to enable your account for PrivateLink.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/accounts/{account_id}/workspaces" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      } ],
      "get" : {
        "summary" : "Get all workspaces",
        "operationId" : "Workspaces.list",
        "tags" : [ "Workspaces" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.ListWorkspacesResponse"
                }
              }
            },
            "description" : "The workspaces were returned successfully."
          }
        },
        "description" : "Gets a list of all workspaces associated with an account, specified by ID.\n\nThis operation is available only if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "inline" : true
        }
      },
      "post" : {
        "summary" : "Create a new workspace",
        "operationId" : "Workspaces.create",
        "tags" : [ "Workspaces" ],
        "parameters" : [ {
          "ref" : true,
          "extRef" : true,
          "x-databricks-cloud" : "gcp",
          "$ref" : "#/components/parameters/deployment.GcpAccessToken"
        } ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.Workspace"
                },
                "examples" : {
                  "Workspace with Managed Network Config" : {
                    "ref" : true,
                    "extRef" : true,
                    "x-databricks-cloud" : "gcp",
                    "$ref" : "#/components/responses/deployment.WorkspaceWithManagedNetworkConfig"
                  },
                  "Workspace with NetworkId" : {
                    "ref" : true,
                    "extRef" : true,
                    "x-databricks-cloud" : "gcp",
                    "$ref" : "#/components/responses/deployment.WorkspaceWithNetworkIdExample"
                  }
                }
              }
            },
            "description" : "Workspace creation request was received. Check workspace status."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/deployment.CreateWorkspaceRequest"
              }
            }
          },
          "description" : "Properties of the new workspace."
        },
        "description" : "Creates a new workspace.\n\n**Important**: This operation is asynchronous. A response with HTTP status code 200 means\nthe request has been accepted and is in progress, but does not mean that the workspace\ndeployed successfully and is running. The initial workspace status is typically\n`PROVISIONING`. Use the workspace ID (`workspace_id`) field in the response to identify\nthe new workspace and make repeated `GET` requests with the workspace ID and check\nits status. The workspace becomes available when the status changes to `RUNNING`.\n",
        "x-databricks-crud" : "create",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "workspace_id",
          "field" : [ "workspace_status" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "BANNED", "FAILED" ],
          "message" : [ "workspace_status_message" ]
        }
      }
    },
    "/api/2.0/accounts/{account_id}/workspaces/{workspace_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/deployment.account_id"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "workspace_id",
        "schema" : {
          "format" : "int64",
          "type" : "integer"
        },
        "description" : "Workspace ID."
      } ],
      "get" : {
        "summary" : "Get a workspace",
        "operationId" : "Workspaces.get",
        "tags" : [ "Workspaces" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/deployment.Workspace"
                },
                "examples" : {
                  "Workspace with Managed Network Config" : {
                    "ref" : true,
                    "extRef" : true,
                    "x-databricks-cloud" : "gcp",
                    "$ref" : "#/components/responses/deployment.WorkspaceWithManagedNetworkConfig"
                  },
                  "Workspace with NetworkId" : {
                    "ref" : true,
                    "extRef" : true,
                    "x-databricks-cloud" : "gcp",
                    "$ref" : "#/components/responses/deployment.WorkspaceWithNetworkIdExample"
                  }
                }
              }
            },
            "description" : "The workspace configuration was successfully returned."
          }
        },
        "description" : "Gets information including status for a Databricks workspace, specified by ID. In the response, the `workspace_status` field indicates the current status. After initial workspace creation (which is asynchronous), make repeated `GET` requests with the workspace ID and check its status. The workspace becomes available when the status changes to `RUNNING`.\n\nFor information about how to create a new workspace with this API **including error handling**, see [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\n\nThis operation is available only if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update workspace configuration",
        "operationId" : "Workspaces.update",
        "tags" : [ "Workspaces" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The workspace update request is accepted."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/deployment.UpdateWorkspaceRequest"
              }
            }
          },
          "description" : "Changes of the workspace properties."
        },
        "description" : "Updates a workspace configuration for either a running workspace or a failed workspace. The elements that can be updated varies between these two use cases.\n\n### Update a failed workspace\nYou can update a Databricks workspace configuration for failed workspace deployment for some fields, but not all fields. For a failed workspace, this request supports updates to the following fields only:\n- Credential configuration ID\n- Storage configuration ID\n- Network configuration ID. Used only if you use customer-managed VPC.\n- Key configuration ID for managed services (control plane storage, such as notebook source and Databricks SQL queries). Used only if you use customer-managed keys for managed services.\n- Key configuration ID for workspace storage (root S3 bucket and, optionally, EBS volumes). Used only if you use customer-managed keys for workspace storage. **Important**: If the workspace was ever in the running state, even if briefly before becoming a failed workspace, you cannot add a new key configuration ID for workspace storage.\n\nAfter calling the `PATCH` operation to update the workspace configuration, make repeated `GET` requests with the workspace ID and check the workspace status. The workspace is successful if the status changes to `RUNNING`.\n\nFor information about how to create a new workspace with this API **including error handling**, see [Create a new workspace using the Account API](http://docs.databricks.com/administration-guide/account-api/new-workspace.html).\n\n### Update a running workspace\nYou can update a Databricks workspace configuration for running workspaces for some fields, but not all fields. For a running workspace, this request supports updating the following fields only:\n- Credential configuration ID\n\n- Network configuration ID. Used only if you already use use customer-managed VPC. This change is supported only if you specified a network configuration ID in your original workspace creation. In other words, you cannot switch from a Databricks-managed VPC to a customer-managed VPC. **Note**: You cannot use a network configuration update in this API to add support for PrivateLink (in Public Preview). To add PrivateLink to an existing workspace, contact your Databricks representative. \n\n- Key configuration ID for managed services (control plane storage, such as notebook source and Databricks SQL queries). Databricks does not directly encrypt the data with the customer-managed key (CMK). Databricks uses both the CMK and the Databricks managed key (DMK) that is unique to your workspace to encrypt the Data Encryption Key (DEK). Databricks uses the DEK to encrypt your workspace's managed services persisted data. If the workspace does not already have a CMK for managed services, adding this ID enables managed services encryption for new or updated data. Existing managed services data that existed before adding the key remains not encrypted with the DEK until it is modified. If the workspace already has customer-managed keys for managed services, this request rotates (changes) the CMK keys and the DEK is re-encrypted with the DMK and the new CMK.\n- Key configuration ID for workspace storage (root S3 bucket and, optionally, EBS volumes). You can set this only if the workspace does not already have a customer-managed key configuration for workspace storage. \n\n**Important**: For updating running workspaces, this API is unavailable on Mondays, Tuesdays, and Thursdays from 4:30pm-7:30pm PST due to routine maintenance. Plan your workspace updates accordingly. For questions about this schedule, contact your Databricks representative.\n\n**Important**: To update a running workspace, your workspace must have no running cluster instances, which includes all-purpose clusters, job clusters, and pools that might have running clusters. Terminate all cluster instances in the workspace before calling this API. \n\n### Wait until changes take effect.\nAfter calling the `PATCH` operation to update the workspace configuration, make repeated `GET` requests with the workspace ID and check the workspace status and the status of the fields.\n* For workspaces with a Databricks-managed VPC, the workspace status becomes `PROVISIONING` temporarily (typically under 20 minutes). If the workspace update is successful, the workspace status changes to `RUNNING`. Note that you can also check the workspace status in the [Account Console](https://docs.databricks.com/administration-guide/account-settings-e2/account-console-e2.html). However, you cannot use or create clusters for another 20 minutes after that status change. This results in a total of up to 40 minutes in which you cannot create clusters. If you create or use clusters before this time interval elapses, clusters do not launch successfully, fail, or could cause other unexpected behavior.  \n\n* For workspaces with a customer-managed VPC, the workspace status stays at status `RUNNING` and the VPC change happens immediately. A change to the storage customer-managed key configuration ID might take a few minutes to update, so continue to check the workspace until you observe that it has been updated. If the update fails, the workspace might revert silently to its original configuration. After the workspace has been updated, you cannot use or create clusters for another 20 minutes. If you create or use clusters before this time interval elapses, clusters do not launch successfully, fail, or could cause other unexpected behavior.\n\nIf you update the _storage_ customer-managed key configurations, it takes 20 minutes for the changes to fully take effect. During the 20 minute wait, it is important that you stop all REST API calls to the DBFS API. If you are modifying _only the managed services key configuration_, you can omit the 20 minute wait.\n\n**Important**: Customer-managed keys and customer-managed VPCs are supported by only some deployment types and subscription types. If you have questions about availability, contact your Databricks representative.\n\nThis operation is available only if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.",
        "x-databricks-crud" : "update",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "workspace_id",
          "field" : [ "workspace_status" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "BANNED", "FAILED" ],
          "message" : [ "workspace_status_message" ]
        },
        "x-databricks-cloud" : "aws"
      },
      "delete" : {
        "summary" : "Delete a workspace",
        "operationId" : "Workspaces.delete",
        "tags" : [ "Workspaces" ],
        "parameters" : [ {
          "ref" : true,
          "extRef" : true,
          "x-databricks-cloud" : "gcp",
          "$ref" : "#/components/parameters/deployment.GcpAccessToken"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The workspace was successfully deleted."
          }
        },
        "description" : "Terminates and deletes a Databricks workspace. From an API perspective, deletion is immediate. However, it might take a few minutes for all workspaces resources to be deleted, depending on the size and number of workspace resources.\n\nThis operation is available only if your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/accounts/{account_id}/workspaces/{workspace_id}/metastore" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.account_id"
      }, {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.workspace_id"
      } ],
      "get" : {
        "summary" : "Gets the metastore assignment for a workspace",
        "operationId" : "AccountMetastoreAssignments.get",
        "tags" : [ "Account Metastore Assignments" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "path",
          "name" : "workspace_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.MetastoreAssignment"
                }
              }
            },
            "description" : "The workspace metastore assignment was successfully returned."
          }
        },
        "description" : "Gets the metastore assignment, if any, for the workspace specified by ID. If the workspace\nis assigned a metastore, the mappig will be returned. If no metastore is assigned to the\nworkspace, the assignment will not be found and a 404 returned.\n",
        "x-databricks-crud" : "read",
        "x-databricks-preview" : "PRIVATE"
      }
    },
    "/api/2.0/accounts/{account_id}/workspaces/{workspace_id}/metastores/{metastore_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.account_id"
      }, {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.workspace_id"
      }, {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/unitycatalog.metastore_id"
      } ],
      "post" : {
        "summary" : "Assigns a workspace to a metastore",
        "operationId" : "AccountMetastoreAssignments.create",
        "tags" : [ "Account Metastore Assignments" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "account_id",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "path",
          "name" : "workspace_id",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "path",
          "name" : "metastore_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.MetastoreAssignment"
                }
              }
            },
            "description" : "The metastore assignment was successfully created."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateMetastoreAssignment"
              }
            }
          },
          "description" : "The mapping from workspace to metastore."
        },
        "description" : "Creates an assignment to a metastore for a workspace",
        "x-databricks-crud" : "create",
        "x-databricks-preview" : "PRIVATE"
      },
      "put" : {
        "summary" : "Updates a metastore assignment to a workspaces",
        "operationId" : "AccountMetastoreAssignments.update",
        "tags" : [ "Account Metastore Assignments" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.MetastoreAssignment"
                }
              }
            },
            "description" : "The metastore assignment was successfully updated."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateMetastoreAssignment"
              }
            }
          },
          "description" : "The metastore assignment to update."
        },
        "description" : "Updates an assignment to a metastore for a workspace. Currently, only the default catalog\nmay be updated\n",
        "x-databricks-crud" : "update",
        "x-databricks-preview" : "PRIVATE"
      },
      "delete" : {
        "summary" : "Delete a metastore assignment",
        "operationId" : "AccountMetastoreAssignments.delete",
        "tags" : [ "Account Metastore Assignments" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The metastore assignment was successfully deleted."
          }
        },
        "description" : "Deletes a metastore assignment to a workspace, leaving the workspace with no metastore.",
        "x-databricks-crud" : "delete",
        "x-databricks-preview" : "PRIVATE"
      }
    },
    "/api/2.0/clusters/change-owner" : {
      "post" : {
        "summary" : "Change cluster owner",
        "operationId" : "Clusters.changeOwner",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.ChangeClusterOwnerResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.ChangeClusterOwner"
              }
            }
          },
          "description" : ""
        },
        "description" : "Change the owner of the cluster. You must be an admin to perform this operation."
      }
    },
    "/api/2.0/clusters/create" : {
      "post" : {
        "summary" : "Create new cluster",
        "operationId" : "Clusters.create",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.CreateClusterResponse"
                },
                "example" : {
                  "cluster_id" : "1234-567890-cited123"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.CreateCluster"
              },
              "examples" : {
                "autoscaling" : {
                  "summary" : "Autoscaling cluster",
                  "value" : {
                    "autoscale" : {
                      "max_workers" : 50,
                      "min_workers" : 2
                    },
                    "cluster_name" : "autoscaling-cluster",
                    "node_type_id" : "r3.xlarge",
                    "spark_version" : "2.0.x-scala2.10"
                  },
                  "x-databricks-cloud" : "aws"
                },
                "efs_mounts" : {
                  "summary" : "Cluster that mounts an Amazon EFS file system",
                  "value" : {
                    "cluster_name" : "efs-cluster",
                    "node_type_id" : "i3.xlarge",
                    "cluster_mount_infos" : [ {
                      "local_mount_dir_path" : "/mnt/efs-mount",
                      "network_filesystem_info" : {
                        "mount_options" : "rsize=1048576,wsize=1048576,hard,timeo=600",
                        "server_address" : "hostname.efs.us-east-1.amazonaws.com"
                      },
                      "remote_mount_dir_path" : "/"
                    } ],
                    "spark_version" : "7.6.x-scala2.12",
                    "num_workers" : 25,
                    "aws_attributes" : {
                      "availability" : "SPOT",
                      "zone_id" : "us-east-2"
                    },
                    "instance_type" : "i3.xlarge"
                  },
                  "x-databricks-cloud" : "aws"
                },
                "single_node" : {
                  "summary" : "Create Single Node Cluster",
                  "value" : {
                    "cluster_name" : "single-node-cluster",
                    "node_type_id" : "i3.xlarge",
                    "spark_version" : "7.6.x-scala2.12",
                    "num_workers" : 0,
                    "custom_tags" : {
                      "ResourceClass" : "SingleNode"
                    },
                    "spark_conf" : {
                      "spark.databricks.cluster.profile" : "singleNode",
                      "spark.master" : "[*, 4]"
                    }
                  }
                },
                "spot" : {
                  "summary" : "Cluster with spot instances",
                  "value" : {
                    "cluster_name" : "my-cluster",
                    "node_type_id" : "r3.xlarge",
                    "spark_version" : "2.0.x-scala2.10",
                    "num_workers" : 25,
                    "spark_conf" : {
                      "spark.speculation" : true
                    },
                    "aws_attributes" : {
                      "availability" : "SPOT",
                      "zone_id" : "us-west-2a"
                    }
                  },
                  "x-databricks-cloud" : "aws"
                }
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new Spark cluster. This method will acquire new instances from the cloud provider if necessary. \nThis method is asynchronous; the returned `cluster_id` can be used to poll the cluster status.\nWhen this method returns, the cluster will be in\\na `PENDING` state.\nThe cluster will be usable once it enters a `RUNNING` state.\n\nNote: Databricks may not be able to acquire some of the requested nodes, due to cloud provider limitations \n(account limits, spot price, etc.) or transient network issues. \n\nIf Databricks acquires at least 85% of the requested on-demand nodes, cluster creation will succeed.\nOtherwise the cluster will terminate with an informative error message.\n",
        "x-databricks-crud" : "create",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "cluster_id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "ERROR", "TERMINATED" ],
          "message" : [ "state_message" ]
        },
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "data \"databricks_node_type\" \"smallest\" {\n  local_disk = true\n}\n\ndata \"databricks_spark_version\" \"latest_lts\" {\n  long_term_support = true\n}\n\nresource \"databricks_cluster\" \"shared_autoscaling\" {\n  cluster_name            = \"Shared Autoscaling\"\n  spark_version           = data.databricks_spark_version.latest_lts.id\n  node_type_id            = data.databricks_node_type.smallest.id\n  autotermination_minutes = 20\n  autoscale {\n    min_workers = 1\n    max_workers = 50\n  }\n}\n"
        } ]
      }
    },
    "/api/2.0/clusters/delete" : {
      "post" : {
        "summary" : "Terminate cluster",
        "operationId" : "Clusters.delete",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.DeleteClusterResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.DeleteCluster"
              },
              "example" : {
                "cluster_id" : "1234-567890-frays123"
              }
            }
          },
          "description" : ""
        },
        "description" : "Terminates the Spark cluster with the specified ID. The cluster is removed asynchronously. \nOnce the termination has completed, the cluster will be in a `TERMINATED` state.\nIf the cluster is already in a `TERMINATING` or `TERMINATED` state, nothing will happen.\n",
        "x-databricks-crud" : "delete",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "cluster_id",
          "field" : [ "state" ],
          "success" : [ "TERMINATED" ],
          "failure" : [ "ERROR" ],
          "message" : [ "state_message" ]
        }
      }
    },
    "/api/2.0/clusters/edit" : {
      "post" : {
        "summary" : "Update cluster configuration",
        "operationId" : "Clusters.edit",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.EditClusterResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.EditCluster"
              },
              "examples" : {
                "aws_edit" : {
                  "value" : {
                    "cluster_id" : "1202-211320-brick1",
                    "node_type_id" : "i3.2xlarge",
                    "num_workers" : 10,
                    "spark_version" : "3.3.x-scala2.11"
                  },
                  "x-databricks-cloud" : "aws"
                }
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the configuration of a cluster to match the provided attributes and size.\nA cluster can be updated if it is in a `RUNNING` or `TERMINATED` state.\n\nIf a cluster is updated while in a `RUNNING` state, it will be restarted so that the new attributes can take effect.\n\nIf a cluster is updated while in a `TERMINATED` state, it will remain `TERMINATED`.\nThe next time it is started using the `clusters/start` API, the new attributes will take effect.\nAny attempt to update a cluster in any other state will be rejected with an `INVALID_STATE` error code.\n\nClusters created by the Databricks Jobs service cannot be edited.\n",
        "x-databricks-crud" : "update",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "cluster_id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "ERROR", "TERMINATED" ],
          "message" : [ "state_message" ]
        }
      }
    },
    "/api/2.0/clusters/events" : {
      "post" : {
        "summary" : "List cluster activity events",
        "operationId" : "Clusters.events",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.GetEventsResponse"
                },
                "example" : {
                  "events" : [ {
                    "cluster_id" : "1202-211320-brick1",
                    "event_details" : {
                      "username" : "admin"
                    },
                    "event_type" : "RESTARTING",
                    "timestamp" : 1509572145487
                  }, {
                    "cluster_id" : "1202-211320-brick1",
                    "event_details" : {
                      "termination_reason" : {
                        "code" : "USER_REQUEST",
                        "parameters" : {
                          "username" : "admin"
                        }
                      }
                    },
                    "event_type" : "TERMINATING",
                    "timestamp" : 1509505807923
                  } ],
                  "next_page" : {
                    "cluster_id" : "1202-211320-brick1",
                    "end_time" : 1509572145487,
                    "offset" : 50,
                    "order" : "DESC"
                  },
                  "total_count" : 303
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.GetEvents"
              },
              "example" : {
                "start_time" : 1617238800000,
                "end_time" : 1619485200000,
                "offset" : 5,
                "order" : "DESC",
                "event_types" : [ "RUNNING" ],
                "cluster_id" : "1234-567890-reef123",
                "limit" : 5
              }
            }
          },
          "description" : ""
        },
        "description" : "Retrieves a list of events about the activity of a cluster.\nThis API is paginated. If there are more events to read, the response includes all the nparameters necessary to request\nthe next page of events.\n",
        "x-databricks-pagination" : {
          "offset" : "offset",
          "limit" : "limit",
          "results" : "events",
          "increment" : 50
        }
      }
    },
    "/api/2.0/clusters/get" : {
      "get" : {
        "summary" : "Get cluster info",
        "operationId" : "Clusters.get",
        "tags" : [ "Clusters" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "cluster_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "The cluster about which to retrieve information."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.ClusterInfo"
                },
                "examples" : {
                  "aws_example" : {
                    "value" : {
                      "cluster_name" : "my-cluster",
                      "start_time" : 1618263108824,
                      "state" : "TERMINATED",
                      "node_type_id" : "i3.xlarge",
                      "spark_version" : "8.2.x-scala2.12",
                      "num_workers" : 30,
                      "init_scripts_safe_mode" : false,
                      "instance_source" : {
                        "node_type_id" : "i3.xlarge"
                      },
                      "last_state_loss_time" : 1619739324740,
                      "enable_local_disk_encryption" : false,
                      "driver_instance_source" : {
                        "node_type_id" : "i3.xlarge"
                      },
                      "creator_user_name" : "someone@example.com",
                      "state_message" : "Inactive cluster terminated (inactive for 120 minutes).",
                      "driver_node_type_id" : "i3.xlarge",
                      "cluster_source" : "UI",
                      "terminated_time" : 1619746525713,
                      "spark_context_id" : 4020997813441461760,
                      "aws_attributes" : {
                        "ebs_volume_count" : 0,
                        "availability" : "SPOT_WITH_FALLBACK",
                        "first_on_demand" : 1,
                        "spot_bid_price_percent" : 100,
                        "zone_id" : "us-west-2c"
                      },
                      "cluster_id" : "1234-567890-reef123",
                      "autotermination_minutes" : 120,
                      "termination_reason" : {
                        "code" : "INACTIVITY",
                        "parameters" : {
                          "inactivity_duration_min" : "120"
                        },
                        "type" : "SUCCESS"
                      },
                      "enable_elastic_disk" : false,
                      "default_tags" : {
                        "ClusterId" : "1234-567890-reef123",
                        "ClusterName" : "my-cluster",
                        "Creator" : "someone@example.com",
                        "Vendor" : "Databricks"
                      },
                      "disk_spec" : {
                        "disk_count" : 0
                      }
                    },
                    "x-databricks-cloud" : "aws"
                  }
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "\"Retrieves the information for a cluster given its identifier.\nClusters can be described while they are running, or up to 60 days after they are terminated.\n",
        "x-databricks-crud" : "read",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "cluster_id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "ERROR", "TERMINATED" ],
          "message" : [ "state_message" ]
        }
      }
    },
    "/api/2.0/clusters/list" : {
      "get" : {
        "summary" : "List all clusters",
        "operationId" : "Clusters.list",
        "tags" : [ "Clusters" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "can_use_client",
          "schema" : {
            "type" : "string"
          },
          "description" : "Filter clusters based on what type of client it can be used for. Could be either NOTEBOOKS or JOBS.\nNo input for this field will get all clusters in the workspace without filtering on its supported client"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.ListClustersResponse"
                },
                "examples" : {
                  "aws_list" : {
                    "value" : {
                      "clusters" : [ {
                        "cluster_name" : "my-cluster",
                        "start_time" : 1618263108824,
                        "state" : "TERMINATED",
                        "node_type_id" : "i3.xlarge",
                        "spark_version" : "8.2.x-scala2.12",
                        "num_workers" : 30,
                        "init_scripts_safe_mode" : false,
                        "instance_source" : {
                          "node_type_id" : "i3.xlarge"
                        },
                        "last_state_loss_time" : 1619739324740,
                        "enable_local_disk_encryption" : false,
                        "driver_instance_source" : {
                          "node_type_id" : "i3.xlarge"
                        },
                        "creator_user_name" : "someone@example.com",
                        "state_message" : "Inactive cluster terminated (inactive for 120 minutes).",
                        "driver_node_type_id" : "i3.xlarge",
                        "cluster_source" : "UI",
                        "terminated_time" : 1619746525713,
                        "spark_context_id" : 4020997813441461760,
                        "aws_attributes" : {
                          "ebs_volume_count" : 0,
                          "availability" : "SPOT_WITH_FALLBACK",
                          "first_on_demand" : 1,
                          "spot_bid_price_percent" : 100,
                          "zone_id" : "us-west-2c"
                        },
                        "cluster_id" : "1234-567890-reef123",
                        "autotermination_minutes" : 120,
                        "termination_reason" : {
                          "code" : "INACTIVITY",
                          "parameters" : {
                            "inactivity_duration_min" : "120"
                          },
                          "type" : "SUCCESS"
                        },
                        "enable_elastic_disk" : false,
                        "default_tags" : {
                          "ClusterId" : "1234-567890-reef123",
                          "ClusterName" : "my-cluster",
                          "Creator" : "someone@example.com",
                          "Vendor" : "Databricks"
                        },
                        "disk_spec" : {
                          "disk_count" : 0
                        }
                      } ]
                    },
                    "x-databricks-cloud" : "aws"
                  }
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Returns information about all pinned clusters, currently active clusters, up to 70 of the most recently terminated \ninteractive clusters in the past 7 days, and up to 30 of the most recently terminated job clusters in the past 7 days. \n\nFor example, if there is 1 pinned cluster, 4 active clusters, 45 terminated interactive clusters in the past 7 days, \nand 50 terminated job clusters\\nin the past 7 days, then this API returns the 1 pinned cluster, 4 active clusters, all 45 \nterminated interactive clusters, and the 30 most recently terminated job clusters.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "clusters"
        }
      }
    },
    "/api/2.0/clusters/list-node-types" : {
      "get" : {
        "summary" : "List node types",
        "operationId" : "Clusters.listNodeTypes",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.ListNodeTypesResponse"
                },
                "examples" : {
                  "aws_nodes" : {
                    "value" : {
                      "node_types" : [ {
                        "display_order" : 0,
                        "num_gpus" : 0,
                        "instance_type_id" : "r4.xlarge",
                        "node_type_id" : "r4.xlarge",
                        "description" : "r4.xlarge",
                        "support_cluster_tags" : true,
                        "node_instance_type" : {
                          "instance_type_id" : "r4.xlarge",
                          "instance_family" : "EC2 r4 Family vCPUs",
                          "local_disk_size_gb" : 0,
                          "swap_size" : "10g",
                          "local_disks" : 0
                        },
                        "memory_mb" : 31232,
                        "is_hidden" : false,
                        "category" : "Memory Optimized",
                        "num_cores" : 4,
                        "is_io_cache_enabled" : false,
                        "support_port_forwarding" : true,
                        "support_ebs_volumes" : true,
                        "is_deprecated" : false
                      } ]
                    },
                    "x-databricks-cloud" : "aws"
                  }
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Returns a list of supported Spark node types. These node types can be used to launch a cluster."
      }
    },
    "/api/2.0/clusters/list-zones" : {
      "get" : {
        "summary" : "List availability zones",
        "operationId" : "Clusters.listZones",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.ListAvailableZonesResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Returns a list of availability zones where clusters can be created in (For example, us-west-2a).\nThese zones can be used to launch a cluster.\n"
      }
    },
    "/api/2.0/clusters/permanent-delete" : {
      "post" : {
        "summary" : "Permanently delete cluster",
        "operationId" : "Clusters.permanentDelete",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.PermanentDeleteClusterResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.PermanentDeleteCluster"
              },
              "example" : {
                "cluster_id" : "1202-211320-brick1"
              }
            }
          },
          "description" : ""
        },
        "description" : "Permanently deletes a Spark cluster. This cluster is terminated and resources are asynchronously removed. \n\nIn addition, users will no longer see permanently deleted clusters in the cluster list, and API users can no longer \nperform any action on permanently deleted clusters.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/clusters/pin" : {
      "post" : {
        "summary" : "Pin cluster",
        "operationId" : "Clusters.pin",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.PinClusterResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.PinCluster"
              },
              "example" : {
                "cluster_id" : "1234-567890-reef123"
              }
            }
          },
          "description" : ""
        },
        "description" : "Pinning a cluster ensures that the cluster will always be returned by the ListClusters API. \nPinning a cluster that is already pinned will have no effect.\nThis API can only be called by workspace admins.\n"
      }
    },
    "/api/2.0/clusters/resize" : {
      "post" : {
        "summary" : "Resize cluster",
        "operationId" : "Clusters.resize",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.ResizeClusterResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.ResizeCluster"
              },
              "example" : {
                "cluster_id" : "1202-211320-brick1",
                "num_workers" : 30
              }
            }
          },
          "description" : ""
        },
        "description" : "Resizes a cluster to have a desired number of workers. This will fail unless the cluster is in a `RUNNING` state.\n",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "cluster_id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "ERROR", "TERMINATED" ],
          "message" : [ "state_message" ]
        }
      }
    },
    "/api/2.0/clusters/restart" : {
      "post" : {
        "summary" : "Restart cluster",
        "operationId" : "Clusters.restart",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.RestartClusterResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.RestartCluster"
              },
              "example" : {
                "cluster_id" : "1202-211320-brick1"
              }
            }
          },
          "description" : ""
        },
        "description" : "Restarts a Spark cluster with the supplied ID. If the cluster is not currently in a `RUNNING` state, nothing will happen.\n",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "cluster_id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "ERROR", "TERMINATED" ],
          "message" : [ "state_message" ]
        }
      }
    },
    "/api/2.0/clusters/spark-versions" : {
      "get" : {
        "summary" : "List available Spark versions",
        "operationId" : "Clusters.sparkVersions",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.GetSparkVersionsResponse"
                },
                "example" : {
                  "versions" : [ {
                    "key" : "8.2.x-scala2.12",
                    "name" : "8.2 (includes Apache Spark 3.1.1, Scala 2.12)"
                  } ]
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Returns the list of available Spark versions. These versions can be used to launch a cluster."
      }
    },
    "/api/2.0/clusters/start" : {
      "post" : {
        "summary" : "Start terminated cluster",
        "operationId" : "Clusters.start",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.StartClusterResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.StartCluster"
              },
              "example" : {
                "cluster_id" : "1202-211320-brick1"
              }
            }
          },
          "description" : ""
        },
        "description" : "Starts a terminated Spark cluster with the supplied ID.\nThis works similar to `createCluster` except:\n\n* The previous cluster id and attributes are preserved.\n* The cluster starts with the last specified cluster size.\n* If the previous cluster was an autoscaling cluster, the current cluster starts with the minimum number of nodes.\n* If the cluster is not currently in a `TERMINATED` state, nothing will happen.\n* Clusters launched to run a job cannot be started.\n",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "cluster_id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "ERROR", "TERMINATED" ],
          "message" : [ "state_message" ]
        }
      }
    },
    "/api/2.0/clusters/unpin" : {
      "post" : {
        "summary" : "Unpin cluster",
        "operationId" : "Clusters.unpin",
        "tags" : [ "Clusters" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.UnpinClusterResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.UnpinCluster"
              },
              "example" : {
                "cluster_id" : "1234-567890-reef123"
              }
            }
          },
          "description" : ""
        },
        "description" : "Unpinning a cluster will allow the cluster to eventually be removed from the ListClusters API.\nUnpinning a cluster that is not pinned will have no effect.\nThis API can only be called by workspace admins.\n"
      }
    },
    "/api/2.0/dbfs/add-block" : {
      "post" : {
        "summary" : "Append data block",
        "operationId" : "Dbfs.addBlock",
        "tags" : [ "Dbfs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/dbfs.AddBlockResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/dbfs.AddBlock"
              },
              "example" : {
                "data" : "ZGF0YWJyaWNrcwo=",
                "handle" : 7904256
              }
            }
          },
          "description" : ""
        },
        "description" : "Appends a block of data to the stream specified by the input handle. If the handle does not exist, \nthis call will throw an exception with `RESOURCE_DOES_NOT_EXIST`.\n\nIf the block of data exceeds 1 MB, this call will throw an exception with `MAX_BLOCK_SIZE_EXCEEDED`.\n"
      }
    },
    "/api/2.0/dbfs/close" : {
      "post" : {
        "summary" : "Close the stream",
        "operationId" : "Dbfs.close",
        "tags" : [ "Dbfs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/dbfs.CloseResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/dbfs.Close"
              },
              "example" : {
                "handle" : 1234567890123456
              }
            }
          },
          "description" : ""
        },
        "description" : "Closes the stream specified by the input handle. If the handle does not exist, \nthis call throws an exception with `RESOURCE_DOES_NOT_EXIST`.\n"
      }
    },
    "/api/2.0/dbfs/create" : {
      "post" : {
        "summary" : "Open a stream",
        "operationId" : "Dbfs.create",
        "tags" : [ "Dbfs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/dbfs.CreateResponse"
                },
                "example" : {
                  "handle" : 1234567890123456
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/dbfs.Create"
              },
              "example" : {
                "overwrite" : true,
                "path" : "/tmp/HelloWorld.txt"
              }
            }
          },
          "description" : ""
        },
        "description" : "\"Opens a stream to write to a file and returns a handle to this stream. \nThere is a 10 minute idle timeout on this handle. If a file or directory already exists on the given path \nand __overwrite__ is set to `false`, this call throws an exception with `RESOURCE_ALREADY_EXISTS`.\n\nA typical workflow for file upload would be:\n\n1. Issue a `create` call and get a handle.\n2. Issue one or more `add-block` calls with the handle you have.\n3. Issue a `close` call with the handle you have.\n",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_dbfs_file\" \"this\" {\n  source = \"${path.module}/main.tf\"\n  path   = \"/tmp/main.tf\"\n}\n"
        } ]
      }
    },
    "/api/2.0/dbfs/delete" : {
      "post" : {
        "summary" : "Delete a file/directory",
        "operationId" : "Dbfs.delete",
        "tags" : [ "Dbfs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/dbfs.DeleteResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/dbfs.Delete"
              },
              "example" : {
                "path" : "/tmp/HelloWorld.txt"
              }
            }
          },
          "description" : ""
        },
        "description" : "Delete the file or directory (optionally recursively delete all files in the directory).\nThis call throws an exception with `IO_ERROR` if the path is a non-empty directory and `recursive` is set to\n`false` or on other similar errors.\n\nWhen you delete a large number of files, the delete operation is done in increments. The call returns\na response after approximately 45 seconds with an error message (503 Service Unavailable) asking you to\nre-invoke the delete operation until the directory structure is fully deleted.\n\nFor operations that delete more than 10K files, we discourage using the DBFS REST API, but advise you to\nperform such operations in the context of a cluster, using\nthe [File system utility (dbutils.fs)](/dev-tools/databricks-utils.html#dbutils-fs). `dbutils.fs`\ncovers the functional scope of the DBFS REST API, but from notebooks. Running such operations using notebooks\nprovides better control and manageability, such as selective deletes, and the possibility to automate periodic\ndelete jobs.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/dbfs/get-status" : {
      "get" : {
        "summary" : "Get the information of a file or directory",
        "operationId" : "Dbfs.getStatus",
        "tags" : [ "Dbfs" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "path",
          "schema" : {
            "example" : "/mnt/foo",
            "type" : "string"
          },
          "description" : "The path of the file or directory. The path should be the absolute DBFS path."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/dbfs.FileInfo"
                },
                "example" : {
                  "file_size" : 13,
                  "is_dir" : false,
                  "modification_time" : 1622054945000,
                  "path" : "/tmp/HelloWorld.txt"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets the file information for a file or directory. \nIf the file or directory does not exist, this call throws an exception with `RESOURCE_DOES_NOT_EXIST`.\n"
      }
    },
    "/api/2.0/dbfs/list" : {
      "get" : {
        "summary" : "List directory contents or file details",
        "operationId" : "Dbfs.list",
        "tags" : [ "Dbfs" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "path",
          "schema" : {
            "example" : "/mnt/foo",
            "type" : "string"
          },
          "description" : "The path of the file or directory. The path should be the absolute DBFS path."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/dbfs.ListStatusResponse"
                },
                "example" : {
                  "files" : [ {
                    "file_size" : 261,
                    "is_dir" : false,
                    "modification_time" : 1669033493,
                    "path" : "/a.cpp"
                  }, {
                    "file_size" : 0,
                    "is_dir" : true,
                    "modification_time" : 1669033493,
                    "path" : "/databricks-results"
                  } ]
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "List the contents of a directory, or details of the file. If the file or directory does not exist, this call\nthrows an exception with `RESOURCE_DOES_NOT_EXIST`.\n\nWhen calling list on a large directory, the list operation will time out after approximately 60 seconds.\nWe strongly recommend using list only on directories containing less than 10K files and discourage using\nthe DBFS REST API for operations that list more than 10K files. Instead, we recommend that you perform such\noperations in the context of a cluster, using\nthe [File system utility (dbutils.fs)](/dev-tools/databricks-utils.html#dbutils-fs), which provides the same\nfunctionality without timing out.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "files"
        }
      }
    },
    "/api/2.0/dbfs/mkdirs" : {
      "post" : {
        "summary" : "Create a directory",
        "operationId" : "Dbfs.mkdirs",
        "tags" : [ "Dbfs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/dbfs.MkDirsResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/dbfs.MkDirs"
              },
              "example" : {
                "path" : "/tmp/my-new-dir"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates the given directory and necessary parent directories if they do not exist. \nIf a file (not a directory) exists at any prefix of the input path, this call throws an exception with `RESOURCE_ALREADY_EXISTS`.\n**Note**: If this operation fails, it might have succeeded in creating some of the necessary parent directories.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/dbfs/move" : {
      "post" : {
        "summary" : "Move a file",
        "operationId" : "Dbfs.move",
        "tags" : [ "Dbfs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/dbfs.MoveResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/dbfs.Move"
              },
              "example" : {
                "destination_path" : "/tmp/my-new-dir/HelloWorld.txt",
                "source_path" : "/tmp/HelloWorld.txt"
              }
            }
          },
          "description" : ""
        },
        "description" : "Moves a file from one location to another location within DBFS. \nIf the source file does not exist, this call throws an exception with `RESOURCE_DOES_NOT_EXIST`.\nIf a file already exists in the destination path, this call throws an exception with `RESOURCE_ALREADY_EXISTS`.\nIf the given source path is a directory, this call always recursively moves all files.\",\n",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.0/dbfs/put" : {
      "post" : {
        "summary" : "Upload a file",
        "operationId" : "Dbfs.put",
        "tags" : [ "Dbfs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/dbfs.PutResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/dbfs.Put"
              },
              "example" : {
                "contents" : "SGVsbG8sIFdvcmxkIQ==",
                "overwrite" : true,
                "path" : "/tmp/HelloWorld.txt"
              }
            }
          },
          "description" : ""
        },
        "description" : "Uploads a file through the use of multipart form post. \nIt is mainly used for streaming uploads, but can also be used as a convenient single call for data upload.\n\nAlternatively you can pass contents as base64 string.\n\nThe amount of data that can be passed (when not streaming) using the __contents__  parameter is limited to 1 MB.\n`MAX_BLOCK_SIZE_EXCEEDED` will be thrown if this limit is exceeded.\n\nIf you want to upload large files, use the streaming upload. For details, see :method:dbfs/create,\n:method:dbfs/addBlock, :method:dbfs/close.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/dbfs/read" : {
      "get" : {
        "summary" : "Get the contents of a file",
        "operationId" : "Dbfs.read",
        "tags" : [ "Dbfs" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "path",
          "schema" : {
            "example" : "/mnt/foo",
            "type" : "string"
          },
          "description" : "The path of the file to read. The path should be the absolute DBFS path."
        }, {
          "in" : "query",
          "name" : "offset",
          "schema" : {
            "type" : "integer"
          },
          "description" : "The offset to read from in bytes."
        }, {
          "in" : "query",
          "name" : "length",
          "schema" : {
            "type" : "integer"
          },
          "description" : "The number of bytes to read starting from the offset. This has a limit of 1 MB, and a default value of 0.5 MB."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/dbfs.ReadResponse"
                },
                "example" : {
                  "bytes_read" : 8,
                  "data" : "ZWxsbywgV28="
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "\"Returns the contents of a file. If the file does not exist, this call throws an exception with `RESOURCE_DOES_NOT_EXIST`.\nIf the path is a directory, the read length is negative, or if the offset is negative, this call throws an exception with \n`INVALID_PARAMETER_VALUE`. If the read length exceeds 1 MB, this call throws an\\nexception with `MAX_READ_SIZE_EXCEEDED`.\n\nIf `offset + length` exceeds the number of bytes in a file, it reads the contents until the end of file.\",\n",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/git-credentials" : {
      "get" : {
        "summary" : "Get Git credentials",
        "operationId" : "GitCredentials.list",
        "tags" : [ "Git Credentials" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/gitcredentials.GetCredentialsResponse"
                }
              }
            },
            "description" : "Git credentials were successfully returned."
          }
        },
        "description" : "Lists the calling user's Git credentials. One credential per user is supported.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "credentials"
        }
      },
      "post" : {
        "summary" : "Create a credential entry",
        "operationId" : "GitCredentials.create",
        "tags" : [ "Git Credentials" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/gitcredentials.CreateCredentialsResponse"
                }
              }
            },
            "description" : "The credential was successfully configured."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/gitcredentials.CreateCredentials"
              }
            }
          },
          "description" : "Details required to create a Git credential entry."
        },
        "description" : "Creates a Git credential entry for the user. Only one Git credential per user is \nsupported, so any attempts to create credentials if an entry already exists will \nfail. Use the PATCH endpoint to update existing credentials, or the DELETE endpoint to \ndelete existing credentials.\n",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_git_credential\" \"ado\" {\n  git_username          = \"myuser\"\n  git_provider          = \"azureDevOpsServices\"\n  personal_access_token = \"sometoken\"\n}\n"
        } ]
      },
      "description" : "This endpoint manages Git credentials for the calling user."
    },
    "/api/2.0/git-credentials/{credential_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "credential_id",
        "schema" : {
          "format" : "int64",
          "type" : "integer"
        },
        "description" : "The ID for the corresponding credential to access."
      } ],
      "get" : {
        "summary" : "Get a credential entry",
        "operationId" : "GitCredentials.get",
        "tags" : [ "Git Credentials" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/gitcredentials.CredentialInfo"
                }
              }
            },
            "description" : "The credential was successfully returned."
          }
        },
        "description" : "Gets the Git credential with the specified credential ID.",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update a credential",
        "operationId" : "GitCredentials.update",
        "tags" : [ "Git Credentials" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The credential was successfully updated."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/gitcredentials.UpdateCredentials"
              }
            }
          },
          "description" : "Details required to update the credential"
        },
        "description" : "Updates the specified Git credential.",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a credential",
        "operationId" : "GitCredentials.delete",
        "tags" : [ "Git Credentials" ],
        "responses" : {
          "200" : {
            "description" : "The credential was successfully deleted."
          }
        },
        "description" : "Deletes the specified Git credential.",
        "x-databricks-crud" : "delete"
      },
      "description" : "This endpoint manages a specific Git credential entry."
    },
    "/api/2.0/global-init-scripts" : {
      "get" : {
        "summary" : "Get init scripts",
        "operationId" : "GlobalInitScripts.list",
        "tags" : [ "Global Init Scripts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/globalinitscripts.ListGlobalInitScriptsResponse"
                }
              }
            },
            "description" : "Response indicating that the scripts were retrieved successfully."
          }
        },
        "description" : "\"Get a list of all global init scripts for this workspace. This returns all properties for each script but **not** the script contents.\nTo retrieve the contents of a script, use the [get a global init script](#operation/get-script) operation.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "scripts"
        }
      },
      "post" : {
        "summary" : "Create init script",
        "operationId" : "GlobalInitScripts.create",
        "tags" : [ "Global Init Scripts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "script_id" : {
                      "example" : "714B166709FBD56F",
                      "description" : "The global init script ID.",
                      "type" : "string"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Response indicating that the script was created successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/globalinitscripts.GlobalInitScriptCreateRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new global init script in this workspace.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_global_init_script\" \"init2\" {\n  content_base64 = base64encode(<<-EOT\n    #!/bin/bash\n    echo \"hello world\"\n    EOT\n  )\n  name = \"hello script\"\n}\n"
        } ]
      },
      "description" : "Manages global init scripts for this workspace."
    },
    "/api/2.0/global-init-scripts/{script_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "script_id",
        "schema" : {
          "example" : "714B166709FBD56F",
          "description" : "The global init script ID.",
          "type" : "string"
        },
        "description" : "The ID of the global init script."
      } ],
      "get" : {
        "summary" : "Get an init script",
        "operationId" : "GlobalInitScripts.get",
        "tags" : [ "Global Init Scripts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/globalinitscripts.GlobalInitScriptDetailsWithContent"
                }
              }
            },
            "description" : "Response indicating that the script was retrieved successfully."
          }
        },
        "description" : "Gets all the details of a script, including its Base64-encoded contents.",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update init script",
        "operationId" : "GlobalInitScripts.update",
        "tags" : [ "Global Init Scripts" ],
        "responses" : {
          "200" : {
            "description" : "The script was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/globalinitscripts.GlobalInitScriptUpdateRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates a global init script, specifying only the fields to change. All fields are optional. \nUnspecified fields retain their current value.\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete init script",
        "operationId" : "GlobalInitScripts.delete",
        "tags" : [ "Global Init Scripts" ],
        "responses" : {
          "200" : {
            "description" : "The script was deleted successfully."
          }
        },
        "description" : "Deletes a global init script.",
        "x-databricks-crud" : "delete"
      },
      "description" : "Manage a specific global init script with ID `script_id`."
    },
    "/api/2.0/instance-pools/create" : {
      "post" : {
        "summary" : "Create a new instance pool",
        "operationId" : "InstancePools.create",
        "tags" : [ "Instance Pools" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/instancepools.CreateInstancePoolResponse"
                },
                "example" : {
                  "instance_pool_id" : "1234-567890-fetch12-pool-A3BcdEFg"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/instancepools.CreateInstancePool"
              },
              "examples" : {
                "aws_zones" : {
                  "value" : {
                    "node_type_id" : "i3.xlarge",
                    "custom_tags" : [ {
                      "key" : "my-key",
                      "value" : "my-value"
                    } ],
                    "min_idle_instances" : 10,
                    "aws_attributes" : {
                      "availability" : "SPOT"
                    },
                    "instance_pool_name" : "my-pool"
                  },
                  "x-databricks-cloud" : "aws"
                }
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new instance pool using idle and ready-to-use cloud instances.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "data \"databricks_node_type\" \"smallest\" {\n}\n\nresource \"databricks_instance_pool\" \"smallest_nodes\" {\n  instance_pool_name = \"Smallest Nodes\"\n  min_idle_instances = 0\n  max_capacity       = 300\n  node_type_id       = data.databricks_node_type.smallest.id\n  aws_attributes {\n    availability           = \"ON_DEMAND\"\n    zone_id                = \"us-east-1a\"\n    spot_bid_price_percent = \"100\"\n  }\n  idle_instance_autotermination_minutes = 10\n  disk_spec {\n    disk_type {\n      ebs_volume_type = \"GENERAL_PURPOSE_SSD\"\n    }\n    disk_size  = 80\n    disk_count = 1\n  }\n}\n"
        } ]
      }
    },
    "/api/2.0/instance-pools/delete" : {
      "post" : {
        "summary" : "Delete an instance pool",
        "operationId" : "InstancePools.delete",
        "tags" : [ "Instance Pools" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/instancepools.DeleteInstancePoolResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/instancepools.DeleteInstancePool"
              },
              "example" : {
                "instance_pool_id" : "1234-567890-fetch12-pool-A3BcdEFg"
              }
            }
          },
          "description" : ""
        },
        "description" : "Deletes the instance pool permanently. The idle instances in the pool are terminated asynchronously. ",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/instance-pools/edit" : {
      "post" : {
        "summary" : "Edit an existing instance pool",
        "operationId" : "InstancePools.edit",
        "tags" : [ "Instance Pools" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/instancepools.EditInstancePoolResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/instancepools.EditInstancePool"
              },
              "example" : {
                "max_capacity" : 200,
                "idle_instance_autotermination_minutes" : 30,
                "min_idle_instances" : 5,
                "instance_pool_id" : "1234-567890-fetch12-pool-A3BcdEFg",
                "instance_pool_name" : "my-edited-pool"
              }
            }
          },
          "description" : ""
        },
        "description" : "Modifies the configuration of an existing instance pool.",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.0/instance-pools/get" : {
      "get" : {
        "summary" : "Get instance pool information",
        "operationId" : "InstancePools.get",
        "tags" : [ "Instance Pools" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "instance_pool_id",
          "schema" : {
            "example" : "1234-567890-fetch12-pool-A3BcdEFg",
            "type" : "string"
          },
          "description" : "The canonical unique identifier for the instance pool."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/instancepools.GetInstancePool"
                },
                "examples" : {
                  "aws_example" : {
                    "value" : {
                      "stats" : {
                        "idle_count" : 5,
                        "pending_idle_count" : 5,
                        "pending_used_count" : 5,
                        "used_count" : 10
                      },
                      "state" : "ACTIVE",
                      "node_type_id" : "c4.2xlarge",
                      "idle_instance_autotermination_minutes" : 60,
                      "custom_tags" : {
                        "my-key" : "my-value"
                      },
                      "status" : { },
                      "min_idle_instances" : 0,
                      "instance_pool_id" : "1234-567890-fetch12-pool-A3BcdEFg",
                      "preloaded_spark_versions" : [ "5.4.x-scala2.11" ],
                      "aws_attributes" : {
                        "availability" : "SPOT",
                        "spot_bid_price_percent" : 100,
                        "zone_id" : "us-west-2a"
                      },
                      "enable_elastic_disk" : false,
                      "instance_pool_name" : "mypool",
                      "default_tags" : {
                        "DatabricksInstancePoolCreatorId" : "100125",
                        "DatabricksInstancePoolId" : "1234-567890-fetch12-pool-A3BcdEFg",
                        "Vendor" : "Databricks"
                      },
                      "disk_spec" : {
                        "disk_count" : 1,
                        "disk_size" : 100,
                        "disk_type" : {
                          "ebs_volume_type" : "GENERAL_PURPOSE_SSD"
                        }
                      }
                    },
                    "x-databricks-cloud" : "aws"
                  }
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Retrieve the information for an instance pool based on its identifier.",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/instance-pools/list" : {
      "get" : {
        "summary" : "List instance pool info",
        "operationId" : "InstancePools.list",
        "tags" : [ "Instance Pools" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/instancepools.ListInstancePools"
                },
                "examples" : {
                  "aws_example" : {
                    "value" : {
                      "instance_pools" : [ {
                        "stats" : {
                          "idle_count" : 5,
                          "pending_idle_count" : 5,
                          "pending_used_count" : 5,
                          "used_count" : 10
                        },
                        "state" : "ACTIVE",
                        "node_type_id" : "c4.2xlarge",
                        "idle_instance_autotermination_minutes" : 60,
                        "status" : { },
                        "min_idle_instances" : 0,
                        "instance_pool_id" : "1234-567890-fetch12-pool-A3BcdEFg",
                        "preloaded_spark_versions" : [ "5.4.x-scala2.11" ],
                        "aws_attributes" : {
                          "availability" : "SPOT",
                          "spot_bid_price_percent" : 100,
                          "zone_id" : "us-west-2a"
                        },
                        "enable_elastic_disk" : false,
                        "instance_pool_name" : "mypool",
                        "default_tags" : {
                          "DatabricksInstancePoolCreatorId" : "100125",
                          "DatabricksInstancePoolId" : "1234-567890-fetch12-pool-A3BcdEFg",
                          "Vendor" : "Databricks"
                        },
                        "disk_spec" : {
                          "disk_count" : 1,
                          "disk_size" : 100,
                          "disk_type" : {
                            "ebs_volume_type" : "GENERAL_PURPOSE_SSD"
                          }
                        }
                      } ]
                    },
                    "x-databricks-cloud" : "aws"
                  }
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets a list of instance pools with their statistics.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "instance_pools"
        }
      }
    },
    "/api/2.0/instance-profiles/add" : {
      "post" : {
        "summary" : "Register an instance profile",
        "operationId" : "InstanceProfiles.add",
        "tags" : [ "Instance Profiles" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.AddInstanceProfile"
              },
              "example" : {
                "instance_profile_arn" : "arn:aws:iam::123456789012:instance-profile/my-profile"
              }
            }
          },
          "description" : ""
        },
        "description" : "In the UI, you can select the instance profile when launching clusters. This API is only available to admin users.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "data \"aws_iam_policy_document\" \"sql_serverless_assume_role\" {\n  statement {\n    actions = [\"sts:AssumeRole\"]\n    principals {\n      type        = \"AWS\"\n      identifiers = [\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\"]\n    }\n    condition {\n      test     = \"StringEquals\"\n      variable = \"sts:ExternalID\"\n      values = [\n        \"databricks-serverless-<YOUR_WORKSPACE_ID1>\",\n        \"databricks-serverless-<YOUR_WORKSPACE_ID2>\"\n      ]\n    }\n  }\n}\n\nresource \"aws_iam_role\" \"this\" {\n  name               = \"my-databricks-sql-serverless-role\"\n  assume_role_policy = data.aws_iam_policy_document.sql_serverless_assume_role.json\n}\n\nresource \"aws_iam_instance_profile\" \"this\" {\n  name = \"my-databricks-sql-serverless-instance-profile\"\n  role = aws_iam_role.this.name\n}\n\nresource \"databricks_instance_profile\" \"this\" {\n  instance_profile_arn = aws_iam_instance_profile.this.arn\n  iam_role_arn         = aws_iam_role.this.arn\n}\n"
        } ]
      }
    },
    "/api/2.0/instance-profiles/edit" : {
      "post" : {
        "summary" : "Edit an instance profile",
        "operationId" : "InstanceProfiles.edit",
        "tags" : [ "Instance Profiles" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.InstanceProfile"
              },
              "example" : {
                "iam_role_arn" : "arn:aws:iam::123456789012:role/my-profile2",
                "instance_profile_arn" : "arn:aws:iam::123456789012:instance-profile/my-profile"
              }
            }
          },
          "description" : ""
        },
        "description" : "The only supported field to change is the optional IAM role ARN associated with\nthe instance profile. It is required to specify the IAM role ARN if both of\nthe following are true:\n\n * Your role name and instance profile name do not match. The name is the part\n   after the last slash in each ARN.\n * You want to use the instance profile with [Databricks SQL Serverless](https://docs.databricks.com/sql/admin/serverless.html).\n\nTo understand where these fields are in the AWS console, see\n[Enable serverless SQL warehouses](https://docs.databricks.com/sql/admin/serverless.html).\n\nThis API is only available to admin users.\n",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.0/instance-profiles/list" : {
      "get" : {
        "summary" : "List available instance profiles",
        "operationId" : "InstanceProfiles.list",
        "tags" : [ "Instance Profiles" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusters.ListInstanceProfilesResponse"
                },
                "example" : {
                  "instance_profiles" : [ {
                    "instance_profile_arn" : "arn:aws:iam::123456789012:instance-profile/my-profile",
                    "is_meta_instance_profile" : false
                  } ]
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "List the instance profiles that the calling user can use to launch a cluster.\n\nThis API is available to all users.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "instance_profiles"
        }
      }
    },
    "/api/2.0/instance-profiles/remove" : {
      "post" : {
        "summary" : "Remove the instance profile",
        "operationId" : "InstanceProfiles.remove",
        "tags" : [ "Instance Profiles" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusters.RemoveInstanceProfile"
              },
              "example" : {
                "instance_profile_arn" : "arn:aws:iam::123456789012:instance-profile/my-profile"
              }
            }
          },
          "description" : ""
        },
        "description" : "Remove the instance profile with the provided ARN.\nExisting clusters with this instance profile will continue to function.\n\nThis API is only accessible to admin users.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/ip-access-lists" : {
      "get" : {
        "summary" : "Get access lists",
        "operationId" : "IpAccessLists.list",
        "tags" : [ "IP Access Lists" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/ipaccesslists.GetIPAccessListResponse"
                }
              }
            },
            "description" : "IP access lists were successfully returned."
          }
        },
        "description" : "Gets all IP access lists for the specified workspace.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "ip_access_lists"
        }
      },
      "post" : {
        "summary" : "Create access list",
        "operationId" : "IpAccessLists.create",
        "tags" : [ "IP Access Lists" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/ipaccesslists.CreateIPAccessListResponse"
                }
              }
            },
            "description" : "An IP access list was successfully created."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/ipaccesslists.CreateIPAccessList"
              }
            }
          },
          "description" : "Details required to configure a block list or allow list."
        },
        "description" : "Creates an IP access list for this workspace. \n\nA list can be an allow list or a block list.\nSee the top of this file for a description of how the server treats allow lists and block lists at runtime.\n\nWhen creating or updating an IP access list:\n\n  * For all allow lists and block lists combined, the API supports a maximum of 1000 IP/CIDR values,\n  where one CIDR counts as a single value. Attempts to exceed that number return error 400 with `error_code` value `QUOTA_EXCEEDED`.\n  * If the new list would block the calling user's current IP, error 400 is returned with `error_code` value `INVALID_STATE`.\n\nIt can take a few minutes for the changes to take effect.\n**Note**: Your new IP access list has no effect until you enable the feature. See :method:workspaceconf/setStatus\n",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_workspace_conf\" \"this\" {\n  custom_config = {\n    \"enableIpAccessLists\" : true\n  }\n}\n\nresource \"databricks_ip_access_list\" \"allowed-list\" {\n  label     = \"allow_in\"\n  list_type = \"ALLOW\"\n  ip_addresses = [\n    \"1.2.3.0/24\",\n    \"1.2.5.0/24\"\n  ]\n  depends_on = [databricks_workspace_conf.this]\n}\n"
        } ]
      },
      "description" : "This endpoint manages IP access lists for a workspace."
    },
    "/api/2.0/ip-access-lists/{ip_access_list_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "ip_access_list_id",
        "schema" : {
          "type" : "string"
        },
        "description" : "The ID for the corresponding IP access list to modify."
      } ],
      "get" : {
        "summary" : "Get access list",
        "operationId" : "IpAccessLists.get",
        "tags" : [ "IP Access Lists" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/ipaccesslists.FetchIpAccessListResponse"
                }
              }
            },
            "description" : "An IP access list was successfully returned."
          }
        },
        "description" : "Gets an IP access list, specified by its list ID.",
        "x-databricks-crud" : "read"
      },
      "put" : {
        "summary" : "Replace access list",
        "operationId" : "IpAccessLists.replace",
        "tags" : [ "IP Access Lists" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The IP access list was successfully replaced."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/ipaccesslists.ReplaceIPAccessList"
              }
            }
          },
          "description" : "Details required to replace an IP access list"
        },
        "description" : "Replaces an IP access list, specified by its ID. \n\nA list can include allow lists and block lists. See the top\nof this file for a description of how the server treats allow lists and block lists at run time. When\nreplacing an IP access list:\n * For all allow lists and block lists combined, the API supports a maximum of 1000 IP/CIDR values,\n   where one CIDR counts as a single value. Attempts to exceed that number return error 400 with `error_code`\n   value `QUOTA_EXCEEDED`.\n * If the resulting list would block the calling user's current IP, error 400 is returned with `error_code`\n   value `INVALID_STATE`.\nIt can take a few minutes for the changes to take effect. Note that your resulting IP access list has no\neffect until you enable the feature. See :method:workspaceconf/setStatus.\n",
        "x-databricks-crud" : "update"
      },
      "patch" : {
        "summary" : "Update access list",
        "operationId" : "IpAccessLists.update",
        "tags" : [ "IP Access Lists" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The IP access list was successfully updated."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/ipaccesslists.UpdateIPAccessList"
              }
            }
          },
          "description" : "Details required to update an IP access list."
        },
        "description" : "Updates an existing IP access list, specified by its ID. \n\nA list can include allow lists and block lists. \nSee the top of this file for a description of how the server treats allow lists and block lists at run time.\n\nWhen updating an IP access list:\n\n  * For all allow lists and block lists combined, the API supports a maximum of 1000 IP/CIDR values, \n  where one CIDR counts as a single value. Attempts to exceed that number return error 400 with `error_code` value `QUOTA_EXCEEDED`.\n  * If the updated list would block the calling user's current IP, error 400 is returned with `error_code` value `INVALID_STATE`.\n\nIt can take a few minutes for the changes to take effect. Note that your resulting IP access list has no effect until you enable \nthe feature. See :method:workspaceconf/setStatus.\n"
      },
      "delete" : {
        "summary" : "Delete access list",
        "operationId" : "IpAccessLists.delete",
        "tags" : [ "IP Access Lists" ],
        "responses" : {
          "200" : {
            "description" : "The IP access list was successfully deleted."
          }
        },
        "description" : "Deletes an IP access list, specified by its list ID.",
        "x-databricks-crud" : "delete"
      },
      "description" : "This endpoint manages a specific IP access list."
    },
    "/api/2.0/libraries/all-cluster-statuses" : {
      "get" : {
        "summary" : "Get all statuses",
        "operationId" : "Libraries.allClusterLibraryStatuses",
        "tags" : [ "ManagedLibraries" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/libraries.ListAllClusterLibraryStatusesResponse"
                },
                "example" : {
                  "statuses" : [ {
                    "cluster_id" : "11203-my-cluster",
                    "library_statuses" : [ {
                      "is_library_for_all_clusters" : false,
                      "library" : {
                        "jar" : "dbfs:/mnt/libraries/library.jar"
                      },
                      "messages" : [ ],
                      "status" : "INSTALLING"
                    } ]
                  }, {
                    "cluster_id" : "20131-my-other-cluster",
                    "library_statuses" : [ {
                      "is_library_for_all_clusters" : false,
                      "library" : {
                        "egg" : "dbfs:/mnt/libraries/library.egg"
                      },
                      "messages" : [ "Could not download library" ],
                      "status" : "ERROR"
                    } ]
                  } ]
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Get the status of all libraries on all clusters. A status will be available for all libraries installed on this cluster \nvia the API or the libraries UI as well as libraries set to be installed on all clusters via the libraries UI.\n"
      }
    },
    "/api/2.0/libraries/cluster-status" : {
      "get" : {
        "summary" : "Get status",
        "operationId" : "Libraries.clusterStatus",
        "tags" : [ "ManagedLibraries" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "cluster_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "Unique identifier of the cluster whose status should be retrieved."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/libraries.ClusterLibraryStatuses"
                },
                "example" : {
                  "statuses" : [ {
                    "cluster_id" : "11203-my-cluster",
                    "library_statuses" : [ {
                      "is_library_for_all_clusters" : false,
                      "library" : {
                        "jar" : "dbfs:/mnt/libraries/library.jar"
                      },
                      "messages" : [ ],
                      "status" : "INSTALLING"
                    } ]
                  }, {
                    "cluster_id" : "20131-my-other-cluster",
                    "library_statuses" : [ {
                      "is_library_for_all_clusters" : false,
                      "library" : {
                        "egg" : "dbfs:/mnt/libraries/library.egg"
                      },
                      "messages" : [ "Could not download library" ],
                      "status" : "ERROR"
                    } ]
                  } ]
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Get the status of libraries on a cluster. A status will be available for all libraries installed on this cluster via the API \nor the libraries UI as well as libraries set to be installed on all clusters via the libraries UI. \nThe order of returned libraries will be as follows.\n\n1. Libraries set to be installed on this cluster will be returned first. \n  Within this group, the final order will be order in which the libraries were added to the cluster.\n\n2. Libraries set to be installed on all clusters are returned next. \n  Within this group there is no order guarantee.\n\n3. Libraries that were previously requested on this cluster or on all clusters, but now marked for removal. \n  Within this group there is no order guarantee.\n",
        "x-databricks-crud" : "list"
      }
    },
    "/api/2.0/libraries/install" : {
      "post" : {
        "summary" : "Add a library",
        "operationId" : "Libraries.install",
        "tags" : [ "ManagedLibraries" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/libraries.InstallLibrariesResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/libraries.InstallLibraries"
              },
              "example" : {
                "cluster_id" : "10201-my-cluster",
                "libraries" : [ {
                  "jar" : "dbfs:/mnt/libraries/library.jar"
                }, {
                  "egg" : "dbfs:/mnt/libraries/library.egg"
                }, {
                  "whl" : "dbfs:/mnt/libraries/library.whl"
                }, {
                  "maven" : {
                    "coordinates" : "org.jsoup:jsoup:1.7.2",
                    "exclusions" : [ "slf4j:slf4j" ]
                  }
                }, {
                  "pypi" : {
                    "package" : "simplejson",
                    "repo" : "http://my-pypi-mirror.com"
                  }
                }, {
                  "cran" : {
                    "package" : "ada",
                    "repo" : "http://cran.us.r-project.org"
                  }
                } ]
              }
            }
          },
          "description" : ""
        },
        "description" : "Add libraries to be installed on a cluster. \nThe installation is asynchronous; it happens in the background after the completion of this request. \n\n**Note**: The actual set of libraries to be installed on a cluster is the union of the libraries specified via this method and \nthe libraries set to be installed on all clusters via the libraries UI.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/libraries/uninstall" : {
      "post" : {
        "summary" : "Uninstall libraries",
        "operationId" : "Libraries.uninstall",
        "tags" : [ "ManagedLibraries" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/libraries.UninstallLibrariesResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/libraries.UninstallLibraries"
              },
              "example" : {
                "cluster_id" : "10201-my-cluster",
                "libraries" : [ {
                  "jar" : "dbfs:/mnt/libraries/library.jar"
                }, {
                  "cran" : "ada"
                } ]
              }
            }
          },
          "description" : ""
        },
        "description" : "Set libraries to be uninstalled on a cluster. The libraries won't be uninstalled until the cluster is restarted. \nUninstalling libraries that are not installed on the cluster will have no impact but is not an error.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/mlflow/artifacts/list" : {
      "get" : {
        "summary" : "Get all artifacts",
        "operationId" : "MLflowArtifacts.listArtifacts",
        "tags" : [ "MLflow Artifacts" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "run_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "ID of the run whose artifacts to list. Must be provided."
        }, {
          "in" : "query",
          "name" : "run_uuid",
          "schema" : {
            "type" : "string"
          },
          "description" : "[Deprecated, use run_id instead] ID of the run whose artifacts to list. This field will\nbe removed in a future MLflow version.",
          "deprecated" : true
        }, {
          "in" : "query",
          "name" : "path",
          "schema" : {
            "type" : "string"
          },
          "description" : "Filter artifacts matching this path (a relative path from the root artifact directory)."
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "Token indicating the page of artifact results to fetch"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.ListArtifactsResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "List artifacts for a run. \nTakes an optional `artifact_path` prefix. If it is specified, the response contains only artifacts with the specified prefix.\",\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "files",
          "token" : {
            "request" : "page_token",
            "response" : "next_page_token"
          }
        }
      }
    },
    "/api/2.0/mlflow/comments/create" : {
      "post" : {
        "summary" : "Post a comment",
        "operationId" : "ModelVersionComments.create",
        "tags" : [ "MLflow Model Version Comments" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "comment" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/mlflow.CommentObject"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Comment was made successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.CreateComment"
              }
            }
          },
          "description" : "Details required to create a comment on a model version."
        },
        "description" : "Posts a comment on a model version. A comment can be submitted either by a user or programmatically to display \nrelevant information about the model. For example, test results or deployment errors.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/mlflow/comments/delete" : {
      "delete" : {
        "summary" : "Delete a comment",
        "operationId" : "ModelVersionComments.delete",
        "tags" : [ "MLflow Model Version Comments" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "id",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.activity_id"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "example" : { }
              }
            },
            "description" : "Comment was deleted successfully."
          }
        },
        "description" : "Deletes a comment on a model version.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/mlflow/comments/update" : {
      "post" : {
        "summary" : "Update a comment",
        "operationId" : "ModelVersionComments.update",
        "tags" : [ "MLflow Model Version Comments" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "comment" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/mlflow.CommentObject"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Comment was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.UpdateComment"
              }
            }
          },
          "description" : "Details required to edit a comment on a model version."
        },
        "description" : "Post an edit to a comment on a model version.",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.0/mlflow/databricks/model-versions/transition-stage" : {
      "post" : {
        "summary" : "Transition a stage",
        "operationId" : "MLflowDatabricks.transitionStage",
        "tags" : [ "Databricks versions of MLflow API endpoints" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "model_version" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/mlflow.ModelVersionDatabricks"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Model version's stage was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.TransitionModelVersionStageDatabricks"
              }
            }
          },
          "description" : "Details required to transition a model version's stage."
        },
        "description" : "Transition a model version's stage. \nThis is a Databricks Workspace version of the [MLflow endpoint](https://www.mlflow.org/docs/latest/rest-api.html#transition-modelversion-stage) \nthat also accepts a comment associated with the transition to be recorded.\",\n"
      }
    },
    "/api/2.0/mlflow/databricks/registered-models/get" : {
      "get" : {
        "summary" : "Get model",
        "operationId" : "MLflowDatabricks.get",
        "tags" : [ "Databricks versions of MLflow API endpoints" ],
        "parameters" : [ {
          "ref" : true,
          "extRef" : true,
          "$ref" : "#/components/parameters/mlflow.name"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "registered_model" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/mlflow.RegisteredModelDatabricks"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Model details were returned successfully."
          }
        },
        "description" : "Get the details of a model. \nThis is a Databricks Workspace version of the [MLflow endpoint](https://www.mlflow.org/docs/latest/rest-api.html#get-registeredmodel) \nthat also returns the model's Databricks Workspace ID and the permission level of the requesting user on the model.\n",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/mlflow/experiments/create" : {
      "post" : {
        "summary" : "Create experiment",
        "operationId" : "Experiments.createExperiment",
        "tags" : [ "MLflow Experiments" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.CreateExperimentResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.CreateExperiment"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates an experiment with a name. Returns the ID of the newly created experiment.\nValidates that another experiment with the same name does not already exist and fails \nif another experiment with the same name already exists.\n\nThrows `RESOURCE_ALREADY_EXISTS` if a experiment with the given name exists.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/mlflow/experiments/delete" : {
      "post" : {
        "summary" : "Delete an experiment",
        "operationId" : "Experiments.deleteExperiment",
        "tags" : [ "MLflow Experiments" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.DeleteExperimentResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.DeleteExperiment"
              }
            }
          },
          "description" : ""
        },
        "description" : "Marks an experiment and associated metadata, runs, metrics, params, and tags for deletion.\nIf the experiment uses FileStore, artifacts associated with experiment are also deleted.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/mlflow/experiments/get" : {
      "get" : {
        "summary" : "Get an experiment",
        "operationId" : "Experiments.getExperiment",
        "tags" : [ "MLflow Experiments" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "experiment_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "ID of the associated experiment."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.Experiment"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets metadata for an experiment. This method works on deleted experiments.",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/mlflow/experiments/get-by-name" : {
      "get" : {
        "summary" : "Get metadata",
        "operationId" : "Experiments.getByName",
        "tags" : [ "MLflow Experiments" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "experiment_name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of the associated experiment."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.GetExperimentByNameResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "\"Gets metadata for an experiment.\n\nThis endpoint will return deleted experiments, but prefers the active experiment if an active and deleted experiment \nshare the same name. If multiple deleted\\nexperiments share the same name, the API will return one of them. \n\nThrows `RESOURCE_DOES_NOT_EXIST` if no experiment with the specified name exists.S\n"
      }
    },
    "/api/2.0/mlflow/experiments/list" : {
      "get" : {
        "summary" : "List experiments",
        "operationId" : "Experiments.listExperiments",
        "tags" : [ "MLflow Experiments" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "view_type",
          "schema" : {
            "type" : "string"
          },
          "description" : "Qualifier for type of experiments to be returned.\nIf unspecified, return only active experiments."
        }, {
          "in" : "query",
          "name" : "max_results",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Maximum number of experiments desired.\nIf `max_results` is unspecified, return all experiments.\nIf `max_results` is too large, it'll be automatically capped at 1000.\nCallers of this endpoint are encouraged to pass max_results explicitly and leverage\npage_token to iterate through experiments."
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "Token indicating the page of experiments to fetch"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.ListExperimentsResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets a list of all experiments.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "experiments",
          "token" : {
            "request" : "page_token",
            "response" : "next_page_token"
          }
        }
      }
    },
    "/api/2.0/mlflow/experiments/restore" : {
      "post" : {
        "summary" : "Restores an experiment",
        "operationId" : "Experiments.restoreExperiment",
        "tags" : [ "MLflow Experiments" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.RestoreExperimentResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.RestoreExperiment"
              }
            }
          },
          "description" : ""
        },
        "description" : "\"Restore an experiment marked for deletion. This also restores\\nassociated metadata, runs, metrics, params, and tags. If experiment uses FileStore, underlying\\nartifacts associated with experiment are also restored.\\n\\nThrows `RESOURCE_DOES_NOT_EXIST` if experiment was never created or was permanently deleted.\",\n"
      }
    },
    "/api/2.0/mlflow/experiments/search" : {
      "post" : {
        "summary" : "Search experiments",
        "operationId" : "Experiments.searchExperiments",
        "tags" : [ "MLflow Experiments" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.SearchExperimentsResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.SearchExperiments"
              }
            }
          },
          "description" : ""
        },
        "description" : "Searches for experiments that satisfy specified search criteria.",
        "x-databricks-pagination" : {
          "results" : "experiments",
          "token" : {
            "request" : "page_token",
            "response" : "next_page_token"
          }
        }
      }
    },
    "/api/2.0/mlflow/experiments/set-experiment-tag" : {
      "post" : {
        "summary" : "Set a tag",
        "operationId" : "Experiments.setExperimentTag",
        "tags" : [ "MLflow Experiments" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.SetExperimentTagResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.SetExperimentTag"
              }
            }
          },
          "description" : ""
        },
        "description" : "Sets a tag on an experiment. Experiment tags are metadata that can be updated."
      }
    },
    "/api/2.0/mlflow/experiments/update" : {
      "post" : {
        "summary" : "Update an experiment",
        "operationId" : "Experiments.updateExperiment",
        "tags" : [ "MLflow Experiments" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.UpdateExperimentResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.UpdateExperiment"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates experiment metadata.",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.0/mlflow/metrics/get-history" : {
      "get" : {
        "summary" : "Get history of a given metric within a run",
        "operationId" : "MLflowMetrics.getHistory",
        "tags" : [ "MLflow Metrics" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "run_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "ID of the run from which to fetch metric values. Must be provided."
        }, {
          "in" : "query",
          "name" : "run_uuid",
          "schema" : {
            "type" : "string"
          },
          "description" : "[Deprecated, use run_id instead] ID of the run from which to fetch metric values. This field\nwill be removed in a future MLflow version.",
          "deprecated" : true
        }, {
          "required" : true,
          "in" : "query",
          "name" : "metric_key",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of the metric."
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "Token indicating the page of metric histories to fetch."
        }, {
          "in" : "query",
          "name" : "max_results",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Maximum number of Metric records to return per paginated request.\nDefault is set to 25,000.\nIf set higher than 25,000, a request Exception will be raised."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.GetMetricHistoryResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets a list of all values for the specified metric for a given run.",
        "x-databricks-crud" : "list"
      }
    },
    "/api/2.0/mlflow/model-versions/create" : {
      "post" : {
        "summary" : "Create a model version",
        "operationId" : "ModelVersions.createModelVersion",
        "tags" : [ "MLflow Model Versions" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.CreateModelVersionResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.CreateModelVersionRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a model version.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/mlflow/model-versions/delete" : {
      "delete" : {
        "summary" : "Delete a model version.",
        "operationId" : "ModelVersions.deleteModelVersion",
        "tags" : [ "MLflow Model Versions" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of the registered model"
        }, {
          "required" : true,
          "in" : "query",
          "name" : "version",
          "schema" : {
            "type" : "string"
          },
          "description" : "Model version number"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Deletes a model version. \n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/mlflow/model-versions/delete-tag" : {
      "delete" : {
        "summary" : "Delete a model version tag",
        "operationId" : "ModelVersions.deleteModelVersionTag",
        "tags" : [ "MLflow Model Versions" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of the registered model that the tag was logged under."
        }, {
          "required" : true,
          "in" : "query",
          "name" : "version",
          "schema" : {
            "type" : "string"
          },
          "description" : "Model version number that the tag was logged under."
        }, {
          "required" : true,
          "in" : "query",
          "name" : "key",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of the tag. The name must be an exact match; wild-card deletion is not supported. Maximum size is 250 bytes."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Deletes a model version tag.\n"
      }
    },
    "/api/2.0/mlflow/model-versions/get" : {
      "get" : {
        "summary" : "Get a model version",
        "operationId" : "ModelVersions.getModelVersion",
        "tags" : [ "MLflow Model Versions" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of the registered model"
        }, {
          "required" : true,
          "in" : "query",
          "name" : "version",
          "schema" : {
            "type" : "string"
          },
          "description" : "Model version number"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.GetModelVersionResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Get a model version.\n",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/mlflow/model-versions/get-download-uri" : {
      "get" : {
        "summary" : "Get a model version URI",
        "operationId" : "ModelVersions.getModelVersionDownloadUri",
        "tags" : [ "MLflow Model Versions" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of the registered model"
        }, {
          "required" : true,
          "in" : "query",
          "name" : "version",
          "schema" : {
            "type" : "string"
          },
          "description" : "Model version number"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.GetModelVersionDownloadUriResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets a URI to download the model version.\n"
      }
    },
    "/api/2.0/mlflow/model-versions/search" : {
      "get" : {
        "summary" : "Searches model versions",
        "operationId" : "ModelVersions.searchModelVersions",
        "tags" : [ "MLflow Model Versions" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "filter",
          "schema" : {
            "type" : "string"
          },
          "description" : "String filter condition, like \"name='my-model-name'\". Must be a single boolean condition,\nwith string values wrapped in single quotes."
        }, {
          "in" : "query",
          "name" : "max_results",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Maximum number of models desired. Max threshold is 10K."
        }, {
          "in" : "query",
          "name" : "order_by",
          "schema" : {
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "description" : "List of columns to be ordered by including model name, version, stage with an\noptional \"DESC\" or \"ASC\" annotation, where \"ASC\" is the default.\nTiebreaks are done by latest stage transition timestamp, followed by name ASC, followed by\nversion DESC."
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "Pagination token to go to next page based on previous search query."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.SearchModelVersionsResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Searches for specific model versions based on the supplied __filter__.\n",
        "x-databricks-pagination" : {
          "results" : "model_versions",
          "token" : {
            "request" : "page_token",
            "response" : "next_page_token"
          }
        }
      }
    },
    "/api/2.0/mlflow/model-versions/set-tag" : {
      "post" : {
        "summary" : "Set a version tag",
        "operationId" : "ModelVersions.setModelVersionTag",
        "tags" : [ "MLflow Model Versions" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.SetModelVersionTagResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.SetModelVersionTagRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Sets a model version tag.\n"
      }
    },
    "/api/2.0/mlflow/model-versions/transition-stage" : {
      "post" : {
        "summary" : "Transition a stage",
        "operationId" : "ModelVersions.transitionStage",
        "tags" : [ "MLflow Model Versions" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.TransitionModelVersionStageResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.TransitionModelVersionStage"
              }
            }
          },
          "description" : ""
        },
        "description" : "Transition to the next model stage.\n"
      }
    },
    "/api/2.0/mlflow/model-versions/update" : {
      "patch" : {
        "summary" : "Update model version",
        "operationId" : "ModelVersions.updateModelVersion",
        "tags" : [ "MLflow Model Versions" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.UpdateModelVersionRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the model version.\n",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.0/mlflow/registered-models/create" : {
      "post" : {
        "summary" : "Create a model",
        "operationId" : "RegisteredModels.createRegisteredModel",
        "tags" : [ "MLflow Registered Models" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.CreateRegisteredModelResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.CreateRegisteredModelRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new registered model with the name specified in the request body.\n\nThrows `RESOURCE_ALREADY_EXISTS` if a registered model with the given name exists.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/mlflow/registered-models/delete" : {
      "delete" : {
        "summary" : "Delete a model",
        "operationId" : "RegisteredModels.deleteRegisteredModel",
        "tags" : [ "MLflow Registered Models" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Registered model unique name identifier."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Deletes a registered model.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/mlflow/registered-models/delete-tag" : {
      "delete" : {
        "summary" : "Delete a model tag",
        "operationId" : "RegisteredModels.deleteRegisteredModelTag",
        "tags" : [ "MLflow Registered Models" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of the registered model that the tag was logged under."
        }, {
          "required" : true,
          "in" : "query",
          "name" : "key",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of the tag. The name must be an exact match; wild-card deletion is not supported. Maximum size is 250 bytes."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Deletes the tag for a registered model.\n"
      }
    },
    "/api/2.0/mlflow/registered-models/get" : {
      "get" : {
        "summary" : "Get a model",
        "operationId" : "RegisteredModels.getRegisteredModel",
        "tags" : [ "MLflow Registered Models" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Registered model unique name identifier."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.GetRegisteredModelResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets the registered model that matches the specified ID.\n",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/mlflow/registered-models/get-latest-versions" : {
      "post" : {
        "summary" : "Get the latest version",
        "operationId" : "RegisteredModels.getLatestVersions",
        "tags" : [ "MLflow Registered Models" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.GetLatestVersionsResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.GetLatestVersionsRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Gets the latest version of a registered model.\n",
        "x-databricks-pagination" : {
          "results" : "model_versions"
        }
      }
    },
    "/api/2.0/mlflow/registered-models/list" : {
      "get" : {
        "summary" : "List models",
        "operationId" : "RegisteredModels.listRegisteredModels",
        "tags" : [ "MLflow Registered Models" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "max_results",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Maximum number of registered models desired. Max threshold is 1000."
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "Pagination token to go to the next page based on a previous query."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.ListRegisteredModelsResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Lists all available registered models, up to the limit specified in __max_results__.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "registered_models",
          "token" : {
            "request" : "page_token",
            "response" : "next_page_token"
          }
        }
      }
    },
    "/api/2.0/mlflow/registered-models/rename" : {
      "post" : {
        "summary" : "Rename a model",
        "operationId" : "RegisteredModels.renameRegisteredModel",
        "tags" : [ "MLflow Registered Models" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.RenameRegisteredModelResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.RenameRegisteredModelRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Renames a registered model.\n"
      }
    },
    "/api/2.0/mlflow/registered-models/search" : {
      "get" : {
        "summary" : "Search models",
        "operationId" : "RegisteredModels.searchRegisteredModels",
        "tags" : [ "MLflow Registered Models" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "filter",
          "schema" : {
            "type" : "string"
          },
          "description" : "String filter condition, like \"name LIKE 'my-model-name'\".\nInterpreted in the backend automatically as \"name LIKE '%my-model-name%'\".\nSingle boolean condition, with string values wrapped in single quotes."
        }, {
          "in" : "query",
          "name" : "max_results",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Maximum number of models desired. Default is 100. Max threshold is 1000."
        }, {
          "in" : "query",
          "name" : "order_by",
          "schema" : {
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "description" : "List of columns for ordering search results, which can include model name and last updated\ntimestamp with an optional \"DESC\" or \"ASC\" annotation, where \"ASC\" is the default.\nTiebreaks are done by model name ASC."
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "Pagination token to go to the next page based on a previous search query."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.SearchRegisteredModelsResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Search for registered models based on the specified __filter__.\n",
        "x-databricks-pagination" : {
          "results" : "registered_models",
          "token" : {
            "request" : "page_token",
            "response" : "next_page_token"
          }
        }
      }
    },
    "/api/2.0/mlflow/registered-models/set-tag" : {
      "post" : {
        "summary" : "Set a tag",
        "operationId" : "RegisteredModels.setRegisteredModelTag",
        "tags" : [ "MLflow Registered Models" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.SetRegisteredModelTagResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.SetRegisteredModelTagRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Sets a tag on a registered model.\n"
      }
    },
    "/api/2.0/mlflow/registered-models/update" : {
      "patch" : {
        "summary" : "Update model",
        "operationId" : "RegisteredModels.updateRegisteredModel",
        "tags" : [ "MLflow Registered Models" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.UpdateRegisteredModelRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates a registered model.\n",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.0/mlflow/registry-webhooks/create" : {
      "post" : {
        "summary" : "Create a webhook",
        "operationId" : "RegistryWebhooks.create",
        "tags" : [ "MLflow Registry Webhooks" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "webhook" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/mlflow.RegistryWebhook"
                    }
                  },
                  "type" : "object"
                },
                "examples" : {
                  "HTTPRegistryWebhook" : {
                    "ref" : true,
                    "extRef" : true,
                    "$ref" : "#/components/examples/mlflow.HTTPRegistryWebhookExample"
                  },
                  "JobRegistryWebhook" : {
                    "ref" : true,
                    "extRef" : true,
                    "$ref" : "#/components/examples/mlflow.JobRegistryWebhookExample"
                  }
                }
              }
            },
            "description" : "Registry webhook was created successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.CreateRegistryWebhook"
              }
            }
          },
          "description" : "Details required to create a registry webhook."
        },
        "description" : "**NOTE**: This endpoint is in Public Preview.\n\nCreates a registry webhook.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/mlflow/registry-webhooks/delete" : {
      "delete" : {
        "summary" : "Delete a webhook",
        "operationId" : "RegistryWebhooks.delete",
        "tags" : [ "MLflow Registry Webhooks" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "id",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.webhook_id"
          },
          "description" : "Webhook ID required to delete a registry webhook."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "example" : { }
              }
            },
            "description" : "Registry webhook was deleted successfully."
          }
        },
        "description" : "**NOTE:** This endpoint is in Public Preview.\n\nDeletes a registry webhook.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/mlflow/registry-webhooks/list" : {
      "get" : {
        "summary" : "List registry webhooks",
        "operationId" : "RegistryWebhooks.list",
        "tags" : [ "MLflow Registry Webhooks" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "model_name",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.model_name"
          },
          "description" : "If not specified, all webhooks associated with the specified events are listed, regardless of their associated model."
        }, {
          "in" : "query",
          "name" : "events",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RegistryWebhookEvents"
          },
          "description" : "If `events` is specified, any webhook with one or more of the specified trigger events is included in the output. If `events` is not specified, webhooks of all event types are included in the output."
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "Token indicating the page of artifact results to fetch"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.ListRegistryWebhooks"
                },
                "examples" : {
                  "webhooks" : {
                    "ref" : true,
                    "extRef" : true,
                    "$ref" : "#/components/examples/mlflow.RegistryWebhookExamples"
                  }
                }
              }
            },
            "description" : "Registry webhooks listed successfully."
          }
        },
        "description" : "**NOTE:** This endpoint is in Public Preview.\n\nLists all registry webhooks.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "webhooks",
          "token" : {
            "request" : "page_token",
            "response" : "next_page_token"
          }
        }
      }
    },
    "/api/2.0/mlflow/registry-webhooks/test" : {
      "post" : {
        "summary" : "Test a webhook",
        "operationId" : "RegistryWebhooks.testRegistryWebhook",
        "tags" : [ "MLflow Registry Webhooks" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "webhook" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/mlflow.TestRegistryWebhook"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Registry webhook was tested successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.TestRegistryWebhookRequest"
              }
            }
          },
          "description" : "Details required to test a registry webhook."
        },
        "description" : "**NOTE:** This endpoint is in Public Preview.\n\nTests a registry webhook.\n"
      }
    },
    "/api/2.0/mlflow/registry-webhooks/update" : {
      "patch" : {
        "summary" : "Update a webhook",
        "operationId" : "RegistryWebhooks.update",
        "tags" : [ "MLflow Registry Webhooks" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                },
                "examples" : {
                  "HTTPRegistryWebhook" : {
                    "ref" : true,
                    "extRef" : true,
                    "$ref" : "#/components/examples/mlflow.HTTPRegistryWebhookDisabledExample"
                  },
                  "JobRegistryWebhook" : {
                    "ref" : true,
                    "extRef" : true,
                    "$ref" : "#/components/examples/mlflow.UpdatedJobRegistryWebhookExample"
                  }
                }
              }
            },
            "description" : "Registry webhook was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.UpdateRegistryWebhook"
              },
              "examples" : {
                "HTTPRegistryWebhook" : {
                  "ref" : true,
                  "extRef" : true,
                  "$ref" : "#/components/examples/mlflow.UpdateHTTPRegistryWebhookExample"
                },
                "JobRegistryWebhook" : {
                  "ref" : true,
                  "extRef" : true,
                  "$ref" : "#/components/examples/mlflow.UpdateJobRegistryWebhookExample"
                }
              }
            }
          },
          "description" : "Details required to update a registry webhook. Only the fields that need to be updated should be specified, and both `http_url_spec` and `job_spec` should not be specified in the same request."
        },
        "description" : "**NOTE:** This endpoint is in Public Preview.\n\nUpdates a registry webhook.\n",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.0/mlflow/runs/create" : {
      "post" : {
        "summary" : "Create a run",
        "operationId" : "MLflowRuns.createRun",
        "tags" : [ "MLflow Runs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.CreateRunResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.CreateRun"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new run within an experiment. \nA run is usually a single execution of a machine learning or data ETL pipeline. \nMLflow uses runs to track the `mlflowParam`, `mlflowMetric` and `mlflowRunTag` associated with a single execution.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/mlflow/runs/delete" : {
      "post" : {
        "summary" : "Delete a run",
        "operationId" : "MLflowRuns.deleteRun",
        "tags" : [ "MLflow Runs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.DeleteRunResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.DeleteRun"
              }
            }
          },
          "description" : ""
        },
        "description" : "Marks a run for deletion.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/mlflow/runs/delete-tag" : {
      "post" : {
        "summary" : "Delete a tag",
        "operationId" : "MLflowRuns.deleteTag",
        "tags" : [ "MLflow Runs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.DeleteTagResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.DeleteTag"
              }
            }
          },
          "description" : ""
        },
        "description" : "Deletes a tag on a run. Tags are run metadata that can be updated during a run and after a run completes."
      }
    },
    "/api/2.0/mlflow/runs/get" : {
      "get" : {
        "summary" : "Get a run",
        "operationId" : "MLflowRuns.getRun",
        "tags" : [ "MLflow Runs" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "run_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "ID of the run to fetch. Must be provided."
        }, {
          "in" : "query",
          "name" : "run_uuid",
          "schema" : {
            "type" : "string"
          },
          "description" : "[Deprecated, use run_id instead] ID of the run to fetch. This field will\nbe removed in a future MLflow version.",
          "deprecated" : true
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.GetRunResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "\"Gets the metadata, metrics, params, and tags for a run. \nIn the case where multiple metrics with the same key are logged for a run, return only the value \nwith the latest timestamp.\n\nIf there are multiple values with the latest timestamp, return the maximum of these values.\n",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/mlflow/runs/log-batch" : {
      "post" : {
        "summary" : "Log a batch",
        "operationId" : "MLflowRuns.logBatch",
        "tags" : [ "MLflow Runs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.LogBatchResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.LogBatch"
              }
            }
          },
          "description" : ""
        },
        "description" : "Logs a batch of metrics, params, and tags for a run.\nIf any data failed to be persisted, the server will respond with an error (non-200 status code).\n\nIn case of error (due to internal server error or an invalid request), partial data may be written.\n\nYou can write metrics, params, and tags in interleaving fashion, but within a given entity type are guaranteed to follow\nthe order specified in the request body.\n\nThe overwrite behavior for metrics,  params, and tags is as follows:\n\n* Metrics: metric values are never overwritten. \n  Logging a metric (key, value, timestamp) appends to the set of values for the metric with the provided key.\n\n* Tags: tag values can be overwritten by successive writes to the same tag key. \n  That is, if multiple tag values with the same key are provided in the same API request, \n  the last-provided tag value is written. Logging the same tag (key, value) is permitted. Specifically, logging a tag is idempotent.\n\n* Parameters: once written, param values cannot be changed (attempting to overwrite a param value will result in an error). \n  However, logging the same param (key, value) is permitted. Specifically, logging a param is idempotent.\n\n  Request Limits\n  -------------------------------\n  A single JSON-serialized API request may be up to 1 MB in size and contain:\n\n * No more than 1000 metrics,  params, and tags in total\n * Up to 1000 metrics\\n- Up to 100  params\n * Up to 100 tags\n\n For example, a valid request might contain 900 metrics, 50 params, and 50 tags, but logging 900 metrics, 50 params, \n and 51 tags is invalid.\n\n The following limits also apply to metric, param, and tag keys and values:\n\n * Metric keyes, param keys, and tag keys can be up to 250 characters in length\n * Parameter and tag values can be up to 250 characters in length\n\n"
      }
    },
    "/api/2.0/mlflow/runs/log-metric" : {
      "post" : {
        "summary" : "Log a metric",
        "operationId" : "MLflowRuns.logMetric",
        "tags" : [ "MLflow Runs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.LogMetricResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.LogMetric"
              }
            }
          },
          "description" : ""
        },
        "description" : "Logs a metric for a run. A metric is a key-value pair (string key, float value) with an associated timestamp. \nExamples include the various metrics that represent ML model accuracy. A metric can be logged multiple times.\n"
      }
    },
    "/api/2.0/mlflow/runs/log-model" : {
      "post" : {
        "summary" : "Log a model",
        "operationId" : "MLflowRuns.logModel",
        "tags" : [ "MLflow Runs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.LogModelResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.LogModel"
              }
            }
          },
          "description" : ""
        },
        "description" : "**NOTE:** Experimental: This API may change or be removed in a future release without warning."
      }
    },
    "/api/2.0/mlflow/runs/log-parameter" : {
      "post" : {
        "summary" : "Log a param",
        "operationId" : "MLflowRuns.logParam",
        "tags" : [ "MLflow Runs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.LogParamResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.LogParam"
              }
            }
          },
          "description" : ""
        },
        "description" : "Logs a param used for a run. A param is a key-value pair (string key, string value). \nExamples include hyperparameters used for ML model training and constant dates and values used in an ETL pipeline. \nA param can be logged only once for a run.\n"
      }
    },
    "/api/2.0/mlflow/runs/restore" : {
      "post" : {
        "summary" : "Restore a run",
        "operationId" : "MLflowRuns.restoreRun",
        "tags" : [ "MLflow Runs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.RestoreRunResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.RestoreRun"
              }
            }
          },
          "description" : ""
        },
        "description" : "Restores a deleted run."
      }
    },
    "/api/2.0/mlflow/runs/search" : {
      "post" : {
        "summary" : "Search for runs",
        "operationId" : "MLflowRuns.searchRuns",
        "tags" : [ "MLflow Runs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.SearchRunsResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.SearchRuns"
              }
            }
          },
          "description" : ""
        },
        "description" : "Searches for runs that satisfy expressions. \n\nSearch expressions can use `mlflowMetric` and `mlflowParam` keys.\",\n",
        "x-databricks-pagination" : {
          "results" : "runs",
          "token" : {
            "request" : "page_token",
            "response" : "next_page_token"
          }
        }
      }
    },
    "/api/2.0/mlflow/runs/set-tag" : {
      "post" : {
        "summary" : "Set a tag",
        "operationId" : "MLflowRuns.setTag",
        "tags" : [ "MLflow Runs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.SetTagResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.SetTag"
              }
            }
          },
          "description" : ""
        },
        "description" : "Sets a tag on a run. Tags are run metadata that can be updated during a run and after\na run completes."
      }
    },
    "/api/2.0/mlflow/runs/update" : {
      "post" : {
        "summary" : "Update a run",
        "operationId" : "MLflowRuns.updateRun",
        "tags" : [ "MLflow Runs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/mlflow.UpdateRunResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.UpdateRun"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates run metadata.",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.0/mlflow/transition-requests/approve" : {
      "post" : {
        "summary" : "Approve transition requests",
        "operationId" : "TransitionRequests.approve",
        "tags" : [ "MLflow Transition Requests" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "activity" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/mlflow.Activity"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Model version's stage was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.ApproveTransitionRequest"
              }
            }
          },
          "description" : "Details required to identify and approve a model version stage transition request."
        },
        "description" : "Approves a model version stage transition request."
      }
    },
    "/api/2.0/mlflow/transition-requests/create" : {
      "post" : {
        "summary" : "Make a transition request",
        "operationId" : "TransitionRequests.create",
        "tags" : [ "MLflow Transition Requests" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "request" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/mlflow.TransitionRequest"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Transition request was made successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.CreateTransitionRequest"
              }
            }
          },
          "description" : "Details required to create a model version stage transition request."
        },
        "description" : "Creates a model version stage transition request.",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/mlflow/transition-requests/delete" : {
      "delete" : {
        "summary" : "Delete a ransition request",
        "operationId" : "TransitionRequests.delete",
        "tags" : [ "MLflow Transition Requests" ],
        "parameters" : [ {
          "ref" : true,
          "extRef" : true,
          "$ref" : "#/components/parameters/mlflow.name"
        }, {
          "ref" : true,
          "extRef" : true,
          "$ref" : "#/components/parameters/mlflow.version"
        }, {
          "required" : true,
          "in" : "query",
          "name" : "stage",
          "schema" : {
            "type" : "string"
          },
          "description" : "Target stage of the transition request. Valid values are:\n\n* `None`: The initial stage of a model version.\n\n* `Staging`: Staging or pre-production stage.\n\n* `Production`: Production stage.\n\n* `Archived`: Archived stage."
        }, {
          "required" : true,
          "in" : "query",
          "name" : "creator",
          "schema" : {
            "format" : "email",
            "type" : "string"
          },
          "description" : "Username of the user who created this request. Of the transition requests matching the specified details, only the one transition created by this user will be deleted."
        }, {
          "ref" : true,
          "extRef" : true,
          "$ref" : "#/components/parameters/mlflow.comment"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "example" : { }
              }
            },
            "description" : "Transition request was deleted successfully."
          }
        },
        "description" : "Cancels a model version stage transition request.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/mlflow/transition-requests/list" : {
      "get" : {
        "summary" : "List transition requests",
        "operationId" : "TransitionRequests.list",
        "tags" : [ "MLflow Transition Requests" ],
        "parameters" : [ {
          "ref" : true,
          "extRef" : true,
          "$ref" : "#/components/parameters/mlflow.name"
        }, {
          "ref" : true,
          "extRef" : true,
          "$ref" : "#/components/parameters/mlflow.version"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "requests" : {
                      "description" : "Array of open transition requests.",
                      "type" : "array",
                      "items" : {
                        "extRef" : true,
                        "ref" : true,
                        "$ref" : "#/components/schemas/mlflow.Activity"
                      }
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Fetched all open requests successfully."
          }
        },
        "description" : "Gets a list of all open stage transition requests for the model version.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "requests"
        }
      }
    },
    "/api/2.0/mlflow/transition-requests/reject" : {
      "post" : {
        "summary" : "Reject a transition request",
        "operationId" : "TransitionRequests.reject",
        "tags" : [ "MLflow Transition Requests" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "activity" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/mlflow.Activity"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Transition request was rejected successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/mlflow.RejectTransitionRequest"
              }
            }
          },
          "description" : "Details required to identify and reject a model version stage transition request."
        },
        "description" : "Rejects a model version stage transition request."
      }
    },
    "/api/2.0/permissions/{request_object_type}/{request_object_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "request_object_type",
        "schema" : {
          "type" : "string"
        },
        "description" : "<needs content>"
      }, {
        "required" : true,
        "in" : "path",
        "name" : "request_object_id",
        "schema" : {
          "type" : "string"
        },
        "description" : ""
      } ],
      "get" : {
        "summary" : "Get object permissions",
        "operationId" : "Permissions.get",
        "tags" : [ "Permissions" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/permissions.ObjectPermissions"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets the permission of an object. Objects can inherit permissions from their parent objects or root objects.",
        "x-databricks-crud" : "get"
      },
      "put" : {
        "summary" : "Set permissions",
        "operationId" : "Permissions.set",
        "tags" : [ "Permissions" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/permissions.PermissionsRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Sets permissions on object. Objects can inherit permissions from their parent objects and root objects.",
        "x-databricks-crud" : "update"
      },
      "patch" : {
        "summary" : "Update permission",
        "operationId" : "Permissions.update",
        "tags" : [ "Permissions" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/permissions.PermissionsRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the permissions on an object.",
        "x-databricks-crud" : "patch",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_group\" \"auto\" {\n  display_name = \"Automation\"\n}\n\nresource \"databricks_group\" \"eng\" {\n  display_name = \"Engineering\"\n}\n\nresource \"databricks_service_principal\" \"aws_principal\" {\n  display_name = \"main\"\n}\n\ndata \"databricks_spark_version\" \"latest\" {}\n\ndata \"databricks_node_type\" \"smallest\" {\n  local_disk = true\n}\n\nresource \"databricks_job\" \"this\" {\n  name                = \"Featurization\"\n  max_concurrent_runs = 1\n\n  new_cluster {\n    num_workers   = 300\n    spark_version = data.databricks_spark_version.latest.id\n    node_type_id  = data.databricks_node_type.smallest.id\n  }\n\n  notebook_task {\n    notebook_path = \"/Production/MakeFeatures\"\n  }\n}\n\nresource \"databricks_permissions\" \"job_usage\" {\n  job_id = databricks_job.this.id\n\n  access_control {\n    group_name       = \"users\"\n    permission_level = \"CAN_VIEW\"\n  }\n\n  access_control {\n    group_name       = databricks_group.auto.display_name\n    permission_level = \"CAN_MANAGE_RUN\"\n  }\n\n  access_control {\n    group_name       = databricks_group.eng.display_name\n    permission_level = \"CAN_MANAGE\"\n  }\n\n  access_control {\n    service_principal_name = databricks_service_principal.aws_principal.application_id\n    permission_level       = \"IS_OWNER\"\n  }\n}\n"
        } ]
      }
    },
    "/api/2.0/permissions/{request_object_type}/{request_object_id}/permissionLevels" : {
      "get" : {
        "summary" : "Get permission levels",
        "operationId" : "Permissions.getPermissionLevels",
        "tags" : [ "Permissions" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "request_object_type",
          "schema" : {
            "type" : "string"
          },
          "description" : "<needs content>"
        }, {
          "required" : true,
          "in" : "path",
          "name" : "request_object_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "<needs content>"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/permissions.GetPermissionLevelsResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets the permission levels that a user can have on an object."
      }
    },
    "/api/2.0/pipelines" : {
      "get" : {
        "summary" : "List pipelines",
        "operationId" : "Pipelines.listPipelines",
        "tags" : [ "Delta Live Tables" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "max_results",
          "schema" : {
            "type" : "integer"
          },
          "description" : "The maximum number of entries to return in a single page. The system may\nreturn fewer than max_results events in a response, even if there are\nmore events available. This field is optional. The default value is 25.\nThe maximum value is 100. An error is returned if the value of max_results\nis greater than 100.\n"
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "Page token returned by previous call"
        }, {
          "in" : "query",
          "name" : "order_by",
          "schema" : {
            "example" : [ "name asc" ],
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "description" : "A list of strings specifying the order of results.\nSupported order_by fields are id and name. The default is id asc.\nThis field is optional.\n"
        }, {
          "in" : "query",
          "name" : "filter",
          "schema" : {
            "type" : "string"
          },
          "description" : "Select a subset of results based on the specified criteria.\nThe supported filters are:\n\n* `notebook='<path>'` to select pipelines that reference the provided notebook path.\n* `name LIKE '[pattern]'` to select pipelines with a name that matches pattern.\n  Wildcards are supported, for example: `name LIKE '%shopping%'`\n\nComposite filters are not supported. This field is optional.\n"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/pipelines.ListPipelinesResponse"
                },
                "example" : {
                  "next_page_token" : "eyJ...==",
                  "prev_page_token" : "eyJ..x9",
                  "statuses" : [ {
                    "name" : "DLT quickstart (Python)",
                    "pipeline_id" : "e0f01758-fc61-11eb-9a03-0242ac130003",
                    "state" : "IDLE",
                    "latest_updates" : [ {
                      "creation_time" : "2021-08-13T00:34:21.871Z",
                      "state" : "COMPLETED",
                      "update_id" : "ee9ae73e-fc61-11eb-9a03-0242ac130003"
                    } ],
                    "creator_user_name" : "username"
                  }, {
                    "creator_user_name" : "username",
                    "name" : "My DLT quickstart example",
                    "pipeline_id" : "f4c82f5e-fc61-11eb-9a03-0242ac130003",
                    "state" : "IDLE"
                  } ]
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Lists pipelines defined in the Delta Live Tables system.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "statuses",
          "token" : {
            "request" : "page_token",
            "response" : "next_page_token"
          }
        }
      },
      "post" : {
        "summary" : "Create a pipeline",
        "operationId" : "Pipelines.create",
        "tags" : [ "Delta Live Tables" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/pipelines.CreatePipelineResponse"
                },
                "example" : {
                  "pipeline_id" : "a12cd3e4-0ab1-1abc-1a2b-1a2bcd3e4fg5"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/pipelines.CreatePipeline"
              },
              "example" : {
                "continuous" : false,
                "name" : "Wikipedia pipeline (SQL)",
                "clusters" : [ {
                  "autoscale" : {
                    "max_workers" : 5,
                    "min_workers" : 1,
                    "mode" : "ENHANCED"
                  },
                  "label" : "default"
                } ],
                "libraries" : [ {
                  "notebook" : {
                    "path" : "/Users/username/DLT Notebooks/Delta Live Tables quickstart (SQL)"
                  }
                } ],
                "storage" : "/Users/username/data"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new data processing pipeline based on the requested configuration. If successful, this method returns\nthe ID of the new pipeline.\n",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_notebook\" \"dlt_demo\" {\n  #...\n}\n\nresource \"databricks_pipeline\" \"this\" {\n  name    = \"Pipeline Name\"\n  storage = \"/test/first-pipeline\"\n  configuration = {\n    key1 = \"value1\"\n    key2 = \"value2\"\n  }\n\n  cluster {\n    label       = \"default\"\n    num_workers = 2\n    custom_tags = {\n      cluster_type = \"default\"\n    }\n  }\n\n  cluster {\n    label       = \"maintenance\"\n    num_workers = 1\n    custom_tags = {\n      cluster_type = \"maintenance\"\n    }\n  }\n\n  library {\n    notebook {\n      path = databricks_notebook.dlt_demo.id\n    }\n  }\n\n  continuous = false\n}\n"
        } ]
      }
    },
    "/api/2.0/pipelines/{pipeline_id}" : {
      "get" : {
        "summary" : "Get a pipeline",
        "operationId" : "Pipelines.get",
        "tags" : [ "Delta Live Tables" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "pipeline_id",
          "schema" : {
            "type" : "string"
          },
          "description" : ""
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/pipelines.GetPipelineResponse"
                },
                "example" : {
                  "name" : "Wikipedia pipeline (SQL)",
                  "pipeline_id" : "a12cd3e4-0ab1-1abc-1a2b-1a2bcd3e4fg5",
                  "state" : "IDLE",
                  "latest_updates" : [ {
                    "creation_time" : "2021-08-13T00:37:30.279Z",
                    "state" : "COMPLETED",
                    "update_id" : "8a0b6d02-fbd0-11eb-9a03-0242ac130003"
                  }, {
                    "creation_time" : "2021-08-13T00:35:51.902Z",
                    "state" : "CANCELED",
                    "update_id" : "a72c08ba-fbd0-11eb-9a03-0242ac130003"
                  }, {
                    "creation_time" : "2021-08-13T00:33:38.565Z",
                    "state" : "FAILED",
                    "update_id" : "ac37d924-fbd0-11eb-9a03-0242ac130003"
                  } ],
                  "creator_user_name" : "username",
                  "spec" : {
                    "continuous" : false,
                    "name" : "Wikipedia pipeline (SQL)",
                    "clusters" : [ {
                      "autoscale" : {
                        "max_workers" : 5,
                        "min_workers" : 1,
                        "mode" : "ENHANCED"
                      },
                      "label" : "default"
                    } ],
                    "id" : "a12cd3e4-0ab1-1abc-1a2b-1a2bcd3e4fg5",
                    "libraries" : [ {
                      "notebook" : {
                        "path" : "/Users/username/DLT Notebooks/Delta Live Tables quickstart (SQL)"
                      }
                    } ],
                    "storage" : "/Users/username/data",
                    "target" : "wikipedia_quickstart_data"
                  },
                  "run_as_user_name" : "username",
                  "cluster_id" : "1234-567891-abcde123"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "",
        "x-databricks-crud" : "read",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "pipeline_id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "FAILED" ],
          "message" : [ "cause" ]
        }
      },
      "put" : {
        "summary" : "Edit a pipeline",
        "operationId" : "Pipelines.update",
        "tags" : [ "Delta Live Tables" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "pipeline_id",
          "schema" : {
            "type" : "string"
          },
          "description" : ""
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/pipelines.EditPipelineResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/pipelines.EditPipeline"
              },
              "example" : {
                "continuous" : false,
                "name" : "Wikipedia pipeline (SQL)",
                "clusters" : [ {
                  "autoscale" : {
                    "max_workers" : 5,
                    "min_workers" : 1,
                    "mode" : "ENHANCED"
                  },
                  "label" : "default"
                } ],
                "id" : "a12cd3e4-0ab1-1abc-1a2b-1a2bcd3e4fg5",
                "libraries" : [ {
                  "notebook" : {
                    "path" : "/Users/username/DLT Notebooks/Delta Live Tables quickstart (SQL)"
                  }
                } ],
                "storage" : "/Users/username/data",
                "target" : "wikipedia_quickstart_data"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates a pipeline with the supplied configuration.",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a pipeline",
        "operationId" : "Pipelines.delete",
        "tags" : [ "Delta Live Tables" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "pipeline_id",
          "schema" : {
            "type" : "string"
          },
          "description" : ""
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/pipelines.DeletePipelineResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Deletes a pipeline.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/pipelines/{pipeline_id}/reset" : {
      "post" : {
        "summary" : "Reset a pipeline",
        "operationId" : "Pipelines.reset",
        "tags" : [ "Delta Live Tables" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "pipeline_id",
          "schema" : {
            "type" : "string"
          },
          "description" : ""
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/pipelines.ResetPipelineResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Resets a pipeline.",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "pipeline_id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "FAILED" ],
          "message" : [ "cause" ]
        }
      }
    },
    "/api/2.0/pipelines/{pipeline_id}/stop" : {
      "post" : {
        "summary" : "Stop a pipeline",
        "operationId" : "Pipelines.stop",
        "tags" : [ "Delta Live Tables" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "pipeline_id",
          "schema" : {
            "type" : "string"
          },
          "description" : ""
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/pipelines.StopPipelineResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Stops a pipeline.",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "pipeline_id",
          "field" : [ "state" ],
          "success" : [ "IDLE" ],
          "failure" : [ "FAILED" ],
          "message" : [ "cause" ]
        }
      }
    },
    "/api/2.0/pipelines/{pipeline_id}/updates" : {
      "get" : {
        "summary" : "List pipeline updates",
        "operationId" : "Pipelines.listUpdates",
        "tags" : [ "Delta Live Tables" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "pipeline_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "The pipeline to return updates for."
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "Page token returned by previous call"
        }, {
          "in" : "query",
          "name" : "max_results",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Max number of entries to return in a single page."
        }, {
          "in" : "query",
          "name" : "until_update_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "If present, returns updates until and including this update_id."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/pipelines.ListUpdatesResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "List updates for an active pipeline."
      },
      "post" : {
        "summary" : "Queue a pipeline update",
        "operationId" : "Pipelines.startUpdate",
        "tags" : [ "Delta Live Tables" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "pipeline_id",
          "schema" : {
            "type" : "string"
          },
          "description" : ""
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/pipelines.StartUpdateResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/pipelines.StartUpdate"
              },
              "example" : {
                "full_refresh" : true
              }
            }
          },
          "description" : ""
        },
        "description" : "Starts or queues a pipeline update."
      }
    },
    "/api/2.0/pipelines/{pipeline_id}/updates/{update_id}" : {
      "get" : {
        "summary" : "Get a pipeline update",
        "operationId" : "Pipelines.getUpdate",
        "tags" : [ "Delta Live Tables" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "pipeline_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "The ID of the pipeline."
        }, {
          "required" : true,
          "in" : "path",
          "name" : "update_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "The ID of the update."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/pipelines.GetUpdateResponse"
                },
                "example" : {
                  "update" : {
                    "creation_time" : 1628815050279,
                    "pipeline_id" : "a12cd3e4-0ab1-1abc-1a2b-1a2bcd3e4fg5",
                    "state" : "COMPLETED",
                    "update_id" : "9a84f906-fc51-11eb-9a03-0242ac130003",
                    "config" : {
                      "continuous" : false,
                      "name" : "Wikipedia pipeline (SQL)",
                      "clusters" : [ {
                        "autoscale" : {
                          "max_workers" : 5,
                          "min_workers" : 1,
                          "mode" : "ENHANCED"
                        },
                        "label" : "default"
                      } ],
                      "id" : "a12cd3e4-0ab1-1abc-1a2b-1a2bcd3e4fg5",
                      "libraries" : [ {
                        "notebook" : {
                          "path" : "/Users/username/DLT Notebooks/Delta Live Tables quickstart (SQL)"
                        }
                      } ],
                      "configuration" : {
                        "pipelines.numStreamRetryAttempts" : "5"
                      },
                      "storage" : "/Users/username/data",
                      "target" : "wikipedia_quickstart_data",
                      "development" : false
                    },
                    "full_refresh" : true,
                    "cause" : "API_CALL",
                    "request_id" : "a83d9f7c-d798-4fd5-aa39-301b6e6f4429"
                  }
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets an update from an active pipeline."
      }
    },
    "/api/2.0/policies/clusters/create" : {
      "post" : {
        "summary" : "Create a new policy",
        "operationId" : "ClusterPolicies.create",
        "tags" : [ "Cluster Policies" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusterpolicies.CreatePolicyResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusterpolicies.CreatePolicy"
              },
              "examples" : {
                "Create" : {
                  "value" : {
                    "definition" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
                    "name" : "Test policy"
                  },
                  "x-databricks-cloud" : "gcp"
                },
                "With policy family" : {
                  "value" : {
                    "name" : "Test policy",
                    "policy_family_definition_overrides" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
                    "policy_family_id" : "personal-vm"
                  },
                  "x-databricks-not-cloud" : "gcp"
                },
                "Without policy family" : {
                  "value" : {
                    "definition" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
                    "name" : "Test policy"
                  },
                  "x-databricks-not-cloud" : "gcp"
                }
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new policy with prescribed settings.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "variable \"team\" {\n  description = \"Team that performs the work\"\n}\n\nvariable \"policy_overrides\" {\n  description = \"Cluster policy overrides\"\n}\n\nlocals {\n  default_policy = {\n    \"dbus_per_hour\" : {\n      \"type\" : \"range\",\n      \"maxValue\" : 10\n    },\n    \"autotermination_minutes\" : {\n      \"type\" : \"fixed\",\n      \"value\" : 20,\n      \"hidden\" : true\n    },\n    \"custom_tags.Team\" : {\n      \"type\" : \"fixed\",\n      \"value\" : var.team\n    }\n  }\n}\n\nresource \"databricks_cluster_policy\" \"fair_use\" {\n  name       = \"${var.team} cluster policy\"\n  definition = jsonencode(merge(local.default_policy, var.policy_overrides))\n}\n\nresource \"databricks_permissions\" \"can_use_cluster_policyinstance_profile\" {\n  cluster_policy_id = databricks_cluster_policy.fair_use.id\n  access_control {\n    group_name       = var.team\n    permission_level = \"CAN_USE\"\n  }\n}\n"
        } ]
      }
    },
    "/api/2.0/policies/clusters/delete" : {
      "post" : {
        "summary" : "Delete a cluster policy",
        "operationId" : "ClusterPolicies.delete",
        "tags" : [ "Cluster Policies" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusterpolicies.DeletePolicyResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusterpolicies.DeletePolicy"
              }
            }
          },
          "description" : ""
        },
        "description" : "Delete a policy for a cluster. Clusters governed by this policy can still run, but cannot be edited.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/policies/clusters/edit" : {
      "post" : {
        "summary" : "Update a cluster policy",
        "operationId" : "ClusterPolicies.edit",
        "tags" : [ "Cluster Policies" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusterpolicies.EditPolicyResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/clusterpolicies.EditPolicy"
              },
              "examples" : {
                "Edit" : {
                  "value" : {
                    "definition" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
                    "name" : "Test policy",
                    "policy_id" : "ABCD000000000000"
                  },
                  "x-databricks-cloud" : "gcp"
                },
                "With policy family" : {
                  "value" : {
                    "name" : "Test policy",
                    "policy_family_definition_overrides" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
                    "policy_family_id" : "personal-vm",
                    "policy_id" : "ABCD000000000000"
                  },
                  "x-databricks-not-cloud" : "gcp"
                },
                "Without policy family" : {
                  "value" : {
                    "definition" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
                    "name" : "Test policy",
                    "policy_id" : "ABCD000000000000"
                  },
                  "x-databricks-not-cloud" : "gcp"
                }
              }
            }
          },
          "description" : ""
        },
        "description" : "Update an existing policy for cluster. This operation may make some clusters governed by the previous policy invalid.",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.0/policies/clusters/get" : {
      "get" : {
        "summary" : "Get entity",
        "operationId" : "ClusterPolicies.get",
        "tags" : [ "Cluster Policies" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "policy_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "Canonical unique identifier for the cluster policy."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusterpolicies.Policy"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Get a cluster policy entity. Creation and editing is available to admins only.",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/policies/clusters/list" : {
      "get" : {
        "summary" : "Get a cluster policy",
        "operationId" : "ClusterPolicies.list",
        "tags" : [ "Cluster Policies" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "sort_order",
          "schema" : {
            "default" : "DESC",
            "type" : "string",
            "enum" : [ "DESC", "ASC" ]
          },
          "description" : "The order in which the policies get listed.\n* `DESC` - Sort result list in descending order.\n* `ASC` - Sort result list in ascending order.\n"
        }, {
          "in" : "query",
          "name" : "sort_column",
          "schema" : {
            "type" : "string",
            "enum" : [ "POLICY_CREATION_TIME", "POLICY_NAME" ]
          },
          "description" : "The cluster policy attribute to sort by.\n* `POLICY_CREATION_TIME` - Sort result list by policy creation time.\n* `POLICY_NAME` - Sort result list by policy name.\n"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusterpolicies.ListPoliciesResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Returns a list of policies accessible by the requesting user.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "policies"
        }
      }
    },
    "/api/2.0/policy-families" : {
      "get" : {
        "operationId" : "PolicyFamilies.list",
        "tags" : [ "Policy Families" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "max_results",
          "schema" : {
            "maximum" : 1000,
            "minimum" : 1,
            "format" : "int64",
            "type" : "integer"
          },
          "description" : "The max number of policy families to return."
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "A token that can be used to get the next page of results."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusterpolicies.ListPolicyFamiliesResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "policy_families",
          "token" : {
            "request" : "page_token",
            "response" : "next_page_token"
          }
        }
      },
      "x-databricks-not-cloud" : "gcp"
    },
    "/api/2.0/policy-families/{policy_family_id}" : {
      "get" : {
        "operationId" : "PolicyFamilies.get",
        "tags" : [ "Policy Families" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "policy_family_id",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusterpolicies.policy_family_id"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/clusterpolicies.GetPolicyFamilyResponse"
                }
              }
            },
            "description" : "Policy family was successfully returned."
          }
        },
        "x-databricks-crud" : "read"
      },
      "x-databricks-not-cloud" : "gcp"
    },
    "/api/2.0/preview/accounts/{account_id}/workspaces/{workspace_id}/permissionassignments" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "type" : "string"
        },
        "description" : "The account ID."
      }, {
        "required" : true,
        "in" : "path",
        "name" : "workspace_id",
        "schema" : {
          "format" : "int64",
          "type" : "integer"
        },
        "description" : "The workspace ID for the account."
      } ],
      "get" : {
        "summary" : "Get permission assignments",
        "operationId" : "WorkspaceAssignment.list",
        "tags" : [ "Workspace Assignment" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/permissions.PermissionAssignments"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Get the permission assignments for the specified Databricks Account and Databricks Workspace.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "permission_assignments"
        }
      },
      "post" : {
        "summary" : "Create permission assignments",
        "operationId" : "WorkspaceAssignment.create",
        "tags" : [ "Workspace Assignment" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/permissions.WorkspaceAssignmentsCreated"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/permissions.CreateWorkspaceAssignments"
              }
            }
          },
          "description" : ""
        },
        "description" : "Create new permission assignments for the specified account and workspace.",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/preview/accounts/{account_id}/workspaces/{workspace_id}/permissionassignments/permissions" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "type" : "string"
        },
        "description" : "The account ID."
      }, {
        "required" : true,
        "in" : "path",
        "name" : "workspace_id",
        "schema" : {
          "format" : "int64",
          "type" : "integer"
        },
        "description" : "The workspace ID."
      } ],
      "get" : {
        "summary" : "List workspace permissions",
        "operationId" : "WorkspaceAssignment.get",
        "tags" : [ "Workspace Assignment" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/permissions.WorkspacePermissions"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Get an array of workspace permissions for the specified account and workspace.",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/preview/accounts/{account_id}/workspaces/{workspace_id}/permissionassignments/principals/{principal_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "type" : "string"
        },
        "description" : "The account ID."
      }, {
        "required" : true,
        "in" : "path",
        "name" : "workspace_id",
        "schema" : {
          "format" : "int64",
          "type" : "integer"
        },
        "description" : "The workspace ID."
      }, {
        "required" : true,
        "in" : "path",
        "name" : "principal_id",
        "schema" : {
          "format" : "int64",
          "type" : "integer"
        },
        "description" : "The ID of the service principal."
      } ],
      "put" : {
        "summary" : "Update permissions assignment",
        "operationId" : "WorkspaceAssignment.update",
        "tags" : [ "Workspace Assignment" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/permissions.WorkspaceAssignmentsUpdated"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/permissions.UpdateWorkspaceAssignments"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the workspace permissions assignment for a given account and workspace using the specified service principal."
      },
      "delete" : {
        "summary" : "Delete permissions assignment",
        "operationId" : "WorkspaceAssignment.delete",
        "tags" : [ "Workspace Assignment" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/permissions.DeleteWorkspaceAssignments"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Deletes the workspace permissions assignment for a given account and workspace using the specified service principal."
      }
    },
    "/api/2.0/preview/scim/v2/Groups" : {
      "get" : {
        "summary" : "List group details",
        "operationId" : "Groups.list",
        "tags" : [ "Groups" ],
        "parameters" : [ {
          "examples" : {
            "equals" : {
              "summary" : "Groups whose displayName equals admins.",
              "value" : "displayName eq admins"
            },
            "starts-with" : {
              "summary" : "Groups whose displayName starts with bar.",
              "value" : "displayName sw bar"
            },
            "or" : {
              "summary" : "Group whose displayName contains foo or displayName contains bar.",
              "value" : "displayName co foo or displayName co bar"
            },
            "not-equals" : {
              "summary" : "Groups whose displayName not equals baz.",
              "value" : "displayName ne baz"
            },
            "contains" : {
              "summary" : "Groups whose displayName contains foo.",
              "value" : "displayName co foo"
            },
            "and" : {
              "summary" : "Group whose displayName contains foo and displayName contains bar.",
              "value" : "displayName co foo and displayName co bar"
            }
          },
          "in" : "query",
          "name" : "filter",
          "schema" : {
            "type" : "string"
          },
          "description" : "Query by which the results have to be filtered. Supported operators are equals(`eq`), contains(`co`), starts with(`sw`) and not equals(`ne`). Additionally, simple expressions can be formed using logical operators - `and` and `or`. The [SCIM RFC](https://tools.ietf.org/html/rfc7644#section-3.4.2.2) has more details but we currently only support simple expressions."
        }, {
          "in" : "query",
          "name" : "attributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to return in response."
        }, {
          "in" : "query",
          "name" : "excludedAttributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to exclude in response."
        }, {
          "in" : "query",
          "name" : "startIndex",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Specifies the index of the first result. First item is number 1."
        }, {
          "in" : "query",
          "name" : "count",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Desired number of results per page."
        }, {
          "in" : "query",
          "name" : "sortBy",
          "schema" : {
            "type" : "string"
          },
          "description" : "Attribute to sort the results."
        }, {
          "in" : "query",
          "name" : "sortOrder",
          "schema" : {
            "type" : "string",
            "enum" : [ "ascending", "descending" ]
          },
          "description" : "The order to sort the results."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.ListGroupsResponse"
                }
              }
            },
            "description" : "List groups operation was succesful."
          }
        },
        "description" : "Gets all details of the groups associated with the Databricks Workspace.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "Resources"
        }
      },
      "post" : {
        "summary" : "Create a new group",
        "operationId" : "Groups.create",
        "tags" : [ "Groups" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.Group"
                }
              }
            },
            "description" : "The group creation request succeeded."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.Group"
              }
            }
          },
          "description" : "Properties of the new group."
        },
        "description" : "Creates a group in the Databricks Workspace with a unique name, using the supplied group details.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_group\" \"this\" {\n  display_name               = \"Some Group\"\n  allow_cluster_create       = true\n  allow_instance_pool_create = true\n}\n\nresource \"databricks_user\" \"this\" {\n  user_name = \"someone@example.com\"\n}\n\nresource \"databricks_group_member\" \"vip_member\" {\n  group_id  = databricks_group.this.id\n  member_id = databricks_user.this.id\n}\n"
        } ]
      }
    },
    "/api/2.0/preview/scim/v2/Groups/{id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "id",
        "schema" : {
          "format" : "int64",
          "type" : "string"
        },
        "description" : "Unique ID for a group in the Databricks Workspace."
      } ],
      "get" : {
        "summary" : "Get group details",
        "operationId" : "Groups.get",
        "tags" : [ "Groups" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.Group"
                }
              }
            },
            "description" : "Group information was returned successfully."
          }
        },
        "description" : "Gets the information for a specific group in the Databricks Workspace.",
        "x-databricks-crud" : "read"
      },
      "put" : {
        "summary" : "Replace a group",
        "operationId" : "Groups.update",
        "tags" : [ "Groups" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Group information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.Group"
              }
            }
          },
          "description" : "Operations to be applied on group information."
        },
        "description" : "Updates the details of a group by replacing the entire group entity.",
        "x-databricks-crud" : "update"
      },
      "patch" : {
        "summary" : "Update group details",
        "operationId" : "Groups.patch",
        "tags" : [ "Groups" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Group information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.PartialUpdate"
              }
            }
          },
          "description" : "Operations to be applied on group information."
        },
        "description" : "Partially updates the details of a group."
      },
      "delete" : {
        "summary" : "Delete a group",
        "operationId" : "Groups.delete",
        "tags" : [ "Groups" ],
        "responses" : {
          "204" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Group was deleted successfully."
          }
        },
        "description" : "Deletes a group from the Databricks Workspace.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/preview/scim/v2/Me" : {
      "get" : {
        "summary" : "Get current user info",
        "operationId" : "CurrentUser.me",
        "tags" : [ "CurrentUser" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.User"
                }
              }
            },
            "description" : "User information was returned successfully."
          }
        },
        "description" : "Get details about the current method caller's identity.",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "data \"databricks_current_user\" \"me\" {\n}\n"
        } ]
      }
    },
    "/api/2.0/preview/scim/v2/ServicePrincipals" : {
      "get" : {
        "summary" : "List service principals",
        "operationId" : "ServicePrincipals.list",
        "tags" : [ "Service Principals" ],
        "parameters" : [ {
          "examples" : {
            "equals" : {
              "summary" : "Service principals whose displayName equals foo.",
              "value" : "displayName eq foo"
            },
            "starts-with" : {
              "summary" : "Service principals whose displayName starts with baz.",
              "value" : "displayName sw baz"
            },
            "or" : {
              "summary" : "Service principals whose displayName contains sp or displayName contains foo.",
              "value" : "displayName co sp or displayName co foo"
            },
            "not-equals" : {
              "summary" : "Service principals whose displayName not equals qux.",
              "value" : "displayName ne qux"
            },
            "contains" : {
              "summary" : "Service principals whose displayName contains bar.",
              "value" : "displayName co bar"
            },
            "and" : {
              "summary" : "Service principals whose displayName contains sp and displayName contains foo.",
              "value" : "displayName co sp and displayName co foo"
            }
          },
          "in" : "query",
          "name" : "filter",
          "schema" : {
            "type" : "string"
          },
          "description" : "Query by which the results have to be filtered. Supported operators are equals(`eq`), contains(`co`), starts with(`sw`) and not equals(`ne`). Additionally, simple expressions can be formed using logical operators - `and` and `or`. The [SCIM RFC](https://tools.ietf.org/html/rfc7644#section-3.4.2.2) has more details but we currently only support simple expressions."
        }, {
          "in" : "query",
          "name" : "attributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to return in response."
        }, {
          "in" : "query",
          "name" : "excludedAttributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to exclude in response."
        }, {
          "in" : "query",
          "name" : "startIndex",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Specifies the index of the first result. First item is number 1."
        }, {
          "in" : "query",
          "name" : "count",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Desired number of results per page."
        }, {
          "in" : "query",
          "name" : "sortBy",
          "schema" : {
            "type" : "string"
          },
          "description" : "Attribute to sort the results."
        }, {
          "in" : "query",
          "name" : "sortOrder",
          "schema" : {
            "type" : "string",
            "enum" : [ "ascending", "descending" ]
          },
          "description" : "The order to sort the results."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.ListServicePrincipalResponse"
                }
              }
            },
            "description" : "List service principals operation was succesful."
          }
        },
        "description" : "Gets the set of service principals associated with a Databricks Workspace.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "Resources"
        }
      },
      "post" : {
        "summary" : "Create a service principal",
        "operationId" : "ServicePrincipals.create",
        "tags" : [ "Service Principals" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.ServicePrincipal"
                }
              }
            },
            "description" : "The user creation request succeeded."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.ServicePrincipal"
              }
            }
          },
          "description" : "Properties of the new service principal."
        },
        "description" : "Creates a new service principal in the Databricks Workspace.",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/preview/scim/v2/ServicePrincipals/{id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "id",
        "schema" : {
          "format" : "int64",
          "type" : "string"
        },
        "description" : "Unique ID for a service principal in the Databricks Workspace."
      } ],
      "get" : {
        "summary" : "Get service principal details",
        "operationId" : "ServicePrincipals.get",
        "tags" : [ "Service Principals" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.ServicePrincipal"
                }
              }
            },
            "description" : "Service principal information was returned successfully."
          }
        },
        "description" : "Gets the details for a single service principal define in the Databricks Workspace.",
        "x-databricks-crud" : "read"
      },
      "put" : {
        "summary" : "Replace service principal",
        "operationId" : "ServicePrincipals.update",
        "tags" : [ "Service Principals" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Service principal information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.ServicePrincipal"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the details of a single service principal. \n\nThis action replaces the existing service principal with the same name.\n",
        "x-databricks-crud" : "update"
      },
      "patch" : {
        "summary" : "Update service principal details",
        "operationId" : "ServicePrincipals.patch",
        "tags" : [ "Service Principals" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Service principal information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.PartialUpdate"
              }
            }
          },
          "description" : "Operations to be applied on service principal information."
        },
        "description" : "Partially updates the details of a single service principal in the Databricks Workspace."
      },
      "delete" : {
        "summary" : "Delete a service principal",
        "operationId" : "ServicePrincipals.delete",
        "tags" : [ "Service Principals" ],
        "responses" : {
          "204" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Service principal was deleted successfully."
          }
        },
        "description" : "Delete a single service principal in the Databricks Workspace.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/preview/scim/v2/Users" : {
      "get" : {
        "summary" : "List users",
        "operationId" : "Users.list",
        "tags" : [ "Users" ],
        "parameters" : [ {
          "examples" : {
            "equals" : {
              "summary" : "Users whose displayName equals john.",
              "value" : "displayName eq john"
            },
            "starts-with" : {
              "summary" : "Users whose displayName starts with jo.",
              "value" : "displayName sw jo"
            },
            "or" : {
              "summary" : "Users whose displayName contains john or userName contains doe.",
              "value" : "displayName co john or userName co doe"
            },
            "not-equals" : {
              "summary" : "Users whose displayName not equals john.",
              "value" : "displayName ne john"
            },
            "contains" : {
              "summary" : "Users whose displayName contains doe.",
              "value" : "displayName co doe"
            },
            "and" : {
              "summary" : "Users whose displayName contains john and userName contains doe.",
              "value" : "displayName co john and userName co doe"
            }
          },
          "in" : "query",
          "name" : "filter",
          "schema" : {
            "type" : "string"
          },
          "description" : "Query by which the results have to be filtered. Supported operators are equals(`eq`), contains(`co`), starts with(`sw`) and not equals(`ne`). Additionally, simple expressions can be formed using logical operators - `and` and `or`. The [SCIM RFC](https://tools.ietf.org/html/rfc7644#section-3.4.2.2) has more details but we currently only support simple expressions."
        }, {
          "in" : "query",
          "name" : "attributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to return in response."
        }, {
          "in" : "query",
          "name" : "excludedAttributes",
          "schema" : {
            "type" : "string"
          },
          "description" : "Comma-separated list of attributes to exclude in response."
        }, {
          "in" : "query",
          "name" : "startIndex",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Specifies the index of the first result. First item is number 1."
        }, {
          "in" : "query",
          "name" : "count",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Desired number of results per page."
        }, {
          "in" : "query",
          "name" : "sortBy",
          "schema" : {
            "type" : "string"
          },
          "description" : "Attribute to sort the results. Multi-part paths are supported. For example, `userName`, `name.givenName`, and `emails`."
        }, {
          "in" : "query",
          "name" : "sortOrder",
          "schema" : {
            "type" : "string",
            "enum" : [ "ascending", "descending" ]
          },
          "description" : "The order to sort the results."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.ListUsersResponse"
                }
              }
            },
            "description" : "List users operation was succesful."
          }
        },
        "description" : "Gets details for all the users associated with a Databricks Workspace.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "Resources"
        }
      },
      "post" : {
        "summary" : "Create a new user",
        "operationId" : "Users.create",
        "tags" : [ "Users" ],
        "responses" : {
          "201" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.User"
                }
              }
            },
            "description" : "The user creation request succeeded."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.User"
              }
            }
          },
          "description" : "Properties of the new user."
        },
        "description" : "Creates a new user in the Databricks Workspace. This new user will also be added to the Databricks account.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "data \"databricks_group\" \"admins\" {\n  display_name = \"admins\"\n}\n\nresource \"databricks_user\" \"me\" {\n  user_name = \"me@example.com\"\n}\n\nresource \"databricks_group_member\" \"i-am-admin\" {\n  group_id  = data.databricks_group.admins.id\n  member_id = databricks_user.me.id\n}\n"
        } ]
      }
    },
    "/api/2.0/preview/scim/v2/Users/{id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "id",
        "schema" : {
          "format" : "int64",
          "type" : "string"
        },
        "description" : "Unique ID for a user in the Databricks Workspace."
      } ],
      "get" : {
        "summary" : "Get user details",
        "operationId" : "Users.get",
        "tags" : [ "Users" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/scim.User"
                }
              }
            },
            "description" : "User information was returned successfully."
          }
        },
        "description" : "Gets information for a specific user in Databricks Workspace.",
        "x-databricks-crud" : "read"
      },
      "put" : {
        "summary" : "Replace a user",
        "operationId" : "Users.update",
        "tags" : [ "Users" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "User information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.User"
              }
            }
          },
          "description" : ""
        },
        "description" : "Replaces a user's information with the data supplied in request.",
        "x-databricks-crud" : "update"
      },
      "patch" : {
        "summary" : "Update user details",
        "operationId" : "Users.patch",
        "tags" : [ "Users" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "User information was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/scim.PartialUpdate"
              }
            }
          },
          "description" : "Operations to be applied on user information."
        },
        "description" : "Partially updates a user resource by applying the supplied operations on specific user attributes."
      },
      "delete" : {
        "summary" : "Delete a user",
        "operationId" : "Users.delete",
        "tags" : [ "Users" ],
        "responses" : {
          "204" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "User was deleted successfully."
          }
        },
        "description" : "Deletes a user. Deleting a user from a Databricks Workspace also removes objects associated with the user.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/preview/sql/alerts" : {
      "get" : {
        "summary" : "Get alerts",
        "operationId" : "Alerts.list",
        "tags" : [ "Alerts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "type" : "array",
                  "items" : {
                    "extRef" : true,
                    "ref" : true,
                    "$ref" : "#/components/schemas/sql.Alert"
                  }
                }
              }
            },
            "description" : "List of alerts was successfully fetched."
          }
        },
        "description" : "Gets a list of alerts.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "inline" : true
        }
      },
      "post" : {
        "summary" : "Create an alert",
        "operationId" : "Alerts.create",
        "tags" : [ "Alerts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.Alert"
                }
              }
            },
            "description" : "An alert was successfully created"
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/sql.CreateAlert"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates an alert.\nAn alert is a Databricks SQL object that periodically runs a query, evaluates a condition of its result,\nand notifies users or alert destinations if the condition was met.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/preview/sql/alerts/{alert_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "alert_id",
        "schema" : {
          "type" : "string"
        }
      } ],
      "get" : {
        "summary" : "Get an alert",
        "operationId" : "Alerts.get",
        "tags" : [ "Alerts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.Alert"
                }
              }
            },
            "description" : "Alert was successfully fetched."
          }
        },
        "description" : "Gets an alert.\n",
        "x-databricks-crud" : "read"
      },
      "put" : {
        "summary" : "Update an alert",
        "operationId" : "Alerts.update",
        "tags" : [ "Alerts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Alert was successfully updated."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/sql.EditAlert"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates an alert.\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete an alert",
        "operationId" : "Alerts.delete",
        "tags" : [ "Alerts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Alert was successfully deleted."
          }
        },
        "description" : "Deletes an alert. Deleted alerts are no longer accessible and cannot be restored.\n**Note:** Unlike queries and dashboards, alerts cannot be moved to the trash.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/preview/sql/alerts/{alert_id}/refresh-schedules" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "alert_id",
        "schema" : {
          "type" : "string"
        }
      } ],
      "get" : {
        "summary" : "[DEPRECATED] Get refresh schedules",
        "operationId" : "Alerts.listSchedules",
        "tags" : [ "Alerts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "type" : "array",
                  "items" : {
                    "extRef" : true,
                    "ref" : true,
                    "$ref" : "#/components/schemas/sql.RefreshSchedule"
                  }
                }
              }
            },
            "description" : "List of alert refresh schedules was successfully fetched."
          }
        },
        "description" : "Gets the refresh schedules for the specified alert.\nAlerts can have refresh schedules that specify when to refresh and evaluate the associated query result.\n\n**Note:** Although refresh schedules are returned in a list, only one refresh schedule per alert is currently supported.\nThe structure of refresh schedules is subject to change.\n\n**Note:** This API is deprecated: Use :method:jobs/list to list jobs and filter by the alert.\n"
      },
      "post" : {
        "summary" : "[DEPRECATED] Create a refresh schedule",
        "operationId" : "Alerts.createSchedule",
        "tags" : [ "Alerts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.RefreshSchedule"
                }
              }
            },
            "description" : "Alert refresh schedule was successfully created."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/sql.CreateRefreshSchedule"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new refresh schedule for an alert.\n\n**Note:** The structure of refresh schedules is subject to change.\n\n**Note:** This API is deprecated: Use :method:jobs/create to create a job with the alert.\n"
      }
    },
    "/api/2.0/preview/sql/alerts/{alert_id}/refresh-schedules/{schedule_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "alert_id",
        "schema" : {
          "type" : "string"
        }
      }, {
        "required" : true,
        "in" : "path",
        "name" : "schedule_id",
        "schema" : {
          "type" : "string"
        }
      } ],
      "delete" : {
        "summary" : "[DEPRECATED] Delete a refresh schedule",
        "operationId" : "Alerts.deleteSchedule",
        "tags" : [ "Alerts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Alert refresh schedule was successfully deleted."
          }
        },
        "description" : "Deletes an alert's refresh schedule. The refresh schedule specifies when to refresh and evaluate the associated query result.\n\n**Note:** This API is deprecated: Use :method:jobs/delete to delete a job for the alert.\n"
      }
    },
    "/api/2.0/preview/sql/alerts/{alert_id}/subscriptions" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "alert_id",
        "schema" : {
          "type" : "string"
        }
      } ],
      "get" : {
        "summary" : "[DEPRECATED] Get an alert's subscriptions",
        "operationId" : "Alerts.getSubscriptions",
        "tags" : [ "Alerts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "type" : "array",
                  "items" : {
                    "extRef" : true,
                    "ref" : true,
                    "$ref" : "#/components/schemas/sql.Subscription"
                  }
                }
              }
            },
            "description" : "List of alert subscriptions was successfully fetched."
          }
        },
        "description" : "Get the subscriptions for an alert.\nAn alert subscription represents exactly one recipient being notified whenever the alert is triggered.\nThe alert recipient is specified by either the `user` field or the `destination` field.\nThe `user` field is ignored if `destination` is non-`null`.\n\n**Note:** This API is deprecated: Use :method:jobs/get to get the subscriptions associated with a job for an alert.\n"
      },
      "post" : {
        "summary" : "[DEPRECATED] Subscribe to an alert",
        "operationId" : "Alerts.subscribe",
        "tags" : [ "Alerts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.Subscription"
                }
              }
            },
            "description" : "Successfully subscribed to alert."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/sql.CreateSubscription"
              }
            }
          },
          "description" : "Subscribes a user or destination to an alert. Only `alert_id` is required to subscribe to an alert.\nIf neither `user_id` nor `destination_id` are provided, the user that sent the API request is subscribed to the alert.\nIf both `user_id` and `destination_id` are provided,  `destination_id` overrides the `user_id` and only the alert destination\nis subscribed.',\n"
        },
        "description" : "**Note:** This API is deprecated: Use :method:jobs/update to subscribe to a job for an alert.\n"
      }
    },
    "/api/2.0/preview/sql/alerts/{alert_id}/subscriptions/{subscription_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "alert_id",
        "schema" : {
          "type" : "string"
        }
      }, {
        "required" : true,
        "in" : "path",
        "name" : "subscription_id",
        "schema" : {
          "type" : "string"
        }
      } ],
      "delete" : {
        "summary" : "[DEPRECATED] Unsubscribe to an alert",
        "operationId" : "Alerts.unsubscribe",
        "tags" : [ "Alerts" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Successfully unsubscribed to alert."
          }
        },
        "description" : "Unsubscribes a user or a destination to an alert.\n\n**Note:** This API is deprecated: Use :method:jobs/update to unsubscribe to a job for an alert.\n"
      }
    },
    "/api/2.0/preview/sql/dashboards" : {
      "get" : {
        "summary" : "Get dashboard objects",
        "operationId" : "Dashboards.list",
        "tags" : [ "Dashboards" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "page_size",
          "schema" : {
            "example" : 50,
            "type" : "integer"
          },
          "description" : "Number of dashboards to return per page."
        }, {
          "in" : "query",
          "name" : "page",
          "schema" : {
            "example" : 1,
            "type" : "integer"
          },
          "description" : "Page number to retrieve."
        }, {
          "in" : "query",
          "name" : "order",
          "schema" : {
            "type" : "string",
            "enum" : [ "name", "created_at" ]
          },
          "description" : "Name of dashboard attribute to order by."
        }, {
          "in" : "query",
          "name" : "q",
          "schema" : {
            "example" : "orders by month",
            "type" : "string"
          },
          "description" : "Full text search term."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "count" : {
                      "example" : 1,
                      "description" : "The total number of dashboards.",
                      "type" : "integer"
                    },
                    "page" : {
                      "example" : 1,
                      "description" : "The current page being displayed.",
                      "type" : "integer"
                    },
                    "page_size" : {
                      "example" : 25,
                      "description" : "The number of dashboards per page.",
                      "type" : "integer"
                    },
                    "results" : {
                      "description" : "List of dashboards returned.",
                      "type" : "array",
                      "items" : {
                        "extRef" : true,
                        "ref" : true,
                        "$ref" : "#/components/schemas/sql.Dashboard"
                      }
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Fetch a paginated list of dashboard objects.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "offset" : "page",
          "limit" : "page_size",
          "results" : "results",
          "increment" : 1
        }
      },
      "post" : {
        "summary" : "Create a dashboard object",
        "operationId" : "Dashboards.create",
        "tags" : [ "Dashboards" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.Dashboard"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "properties" : {
                  "is_trashed" : {
                    "example" : false,
                    "description" : "Indicates whether the dashboard is trashed. Trashed dashboards don't appear in list views.",
                    "type" : "boolean"
                  },
                  "parent" : {
                    "default" : "folders/HOME",
                    "example" : "folders/2025532471912059",
                    "description" : "The identifier of the workspace folder containing the dashboard. The default is the user's home folder.",
                    "type" : "string"
                  },
                  "name" : {
                    "example" : "Sales Dashboard",
                    "description" : "The title of this dashboard that appears in list views and at the top of the dashboard page.",
                    "type" : "string"
                  },
                  "tags" : {
                    "type" : "array",
                    "items" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/sql.TagString"
                    }
                  },
                  "dashboard_filters_enabled" : {
                    "default" : false,
                    "example" : false,
                    "description" : "In the web application, query filters that share a name are coupled to a single selection box if this value is true.",
                    "type" : "boolean"
                  },
                  "is_draft" : {
                    "description" : "Draft dashboards only appear in list views for their owners.",
                    "type" : "boolean"
                  },
                  "widgets" : {
                    "description" : "An array of widget objects. A complete description of widget objects can be found in the response to [Retrieve A Dashboard Definition](#operation/sql-analytics-fetch-dashboard). Databricks does not recommend creating new widgets via this API.",
                    "type" : "array",
                    "items" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/sql.Widget"
                    }
                  }
                },
                "type" : "object"
              }
            }
          },
          "description" : "Creates a new dashboard object.\nOnly the `name` parameter is required in the POST request JSON body.\nOther fields can be included when duplicating dashboards with this API.\nDatabricks does not recommend designing dashboards exclusively using this API.',\n"
        },
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/preview/sql/dashboards/trash/{dashboard_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "dashboard_id",
        "schema" : {
          "type" : "string"
        }
      } ],
      "post" : {
        "summary" : "Restore a dashboard",
        "operationId" : "Dashboards.restore",
        "tags" : [ "Dashboards" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "A restored dashboard appears in list views and searches and can be shared.\n"
      }
    },
    "/api/2.0/preview/sql/dashboards/{dashboard_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "dashboard_id",
        "schema" : {
          "type" : "string"
        }
      } ],
      "get" : {
        "summary" : "Retrieve a definition",
        "operationId" : "Dashboards.get",
        "tags" : [ "Dashboards" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.Dashboard"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Returns a JSON representation of a dashboard object, including its visualization and query objects.\n",
        "x-databricks-crud" : "read"
      },
      "delete" : {
        "summary" : "Remove a dashboard",
        "operationId" : "Dashboards.delete",
        "tags" : [ "Dashboards" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Moves a dashboard to the trash. Trashed dashboards do not appear in list views or searches, and cannot be shared.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/preview/sql/data_sources" : {
      "get" : {
        "summary" : "Get a list of SQL warehouses",
        "operationId" : "DataSources.list",
        "tags" : [ "Data Sources" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "type" : "array",
                  "items" : {
                    "extRef" : true,
                    "ref" : true,
                    "$ref" : "#/components/schemas/sql.DataSource"
                  }
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Retrieves a full list of SQL warehouses available in this workspace.\nAll fields that appear in this API response are enumerated for clarity.\nHowever, you need only a SQL warehouse's `id` to create new queries against it.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "inline" : true
        }
      }
    },
    "/api/2.0/preview/sql/permissions/{objectType}/{objectId}" : {
      "get" : {
        "summary" : "Get object ACL",
        "operationId" : "DbsqlPermissions.get",
        "tags" : [ "ACL / Permissions" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "objectType",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.object_type_plural"
          },
          "description" : "The type of object permissions to check."
        }, {
          "required" : true,
          "in" : "path",
          "name" : "objectId",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.object_id"
          },
          "description" : "Object ID. An ACL is returned for the object with this UUID."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "access_control_list" : {
                      "type" : "array",
                      "items" : {
                        "extRef" : true,
                        "ref" : true,
                        "$ref" : "#/components/schemas/sql.AccessControl"
                      }
                    },
                    "object_id" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/sql.object_type"
                    },
                    "object_type" : {
                      "example" : "query/2cca1687-60ff-4886-a445-0230578c864d",
                      "format" : "<object-type>/<uuid>",
                      "description" : "An object's type and UUID, separated by a forward slash (/) character.",
                      "type" : "string"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets a JSON representation of the access control list (ACL) for a specified object.\n"
      },
      "post" : {
        "summary" : "Set object ACL",
        "operationId" : "DbsqlPermissions.set",
        "tags" : [ "ACL / Permissions" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "objectType",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.object_type_plural"
          },
          "description" : "The type of object permission to set."
        }, {
          "required" : true,
          "in" : "path",
          "name" : "objectId",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.object_id"
          },
          "description" : "Object ID. The ACL for the object with this UUID is overwritten by this request's POST content."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "access_control_list" : {
                      "type" : "array",
                      "items" : {
                        "extRef" : true,
                        "ref" : true,
                        "$ref" : "#/components/schemas/sql.AccessControl"
                      }
                    },
                    "object_id" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/sql.object_type"
                    },
                    "object_type" : {
                      "example" : "query/2cca1687-60ff-4886-a445-0230578c864d",
                      "format" : "<object-type>/<uuid>",
                      "description" : "An object's type and UUID, separated by a forward slash (/) character.",
                      "type" : "string"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "properties" : {
                  "access_control_list" : {
                    "type" : "array",
                    "items" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/sql.AccessControl"
                    }
                  }
                },
                "type" : "object"
              }
            }
          },
          "description" : "An ACL list to be applied to the object specified in the URL."
        },
        "description" : "Sets the access control list (ACL) for a specified object. This operation will complete rewrite the ACL.\n"
      }
    },
    "/api/2.0/preview/sql/permissions/{objectType}/{objectId}/transfer" : {
      "post" : {
        "summary" : "Transfer object ownership",
        "operationId" : "DbsqlPermissions.transferOwnership",
        "tags" : [ "ACL / Permissions" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "objectType",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.ownable_object_type"
          },
          "description" : "The type of object on which to change ownership."
        }, {
          "required" : true,
          "in" : "path",
          "name" : "objectId",
          "schema" : {
            "properties" : {
              "new_owner" : {
                "example" : "user@example.com",
                "format" : "email",
                "description" : "Email address for the new owner, who must exist in the workspace.",
                "type" : "string"
              }
            },
            "type" : "object"
          },
          "description" : "The ID of the object on which to change ownership."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.Success"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "properties" : {
                  "new_owner" : {
                    "example" : "user@example.com",
                    "format" : "email",
                    "description" : "Email address for the new owner, who must exist in the workspace.",
                    "type" : "string"
                  }
                },
                "type" : "object"
              }
            }
          },
          "description" : "Email address for the new owner, who must exist in the workspace."
        },
        "description" : "Transfers ownership of a dashboard, query, or alert to an active user. Requires an admin API key.\n"
      },
      "description" : "Transfer ownership of a single object."
    },
    "/api/2.0/preview/sql/queries" : {
      "get" : {
        "summary" : "Get a list of queries",
        "operationId" : "Queries.list",
        "tags" : [ "Queries / Results" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "page_size",
          "schema" : {
            "example" : 50,
            "type" : "integer"
          },
          "description" : "Number of queries to return per page."
        }, {
          "in" : "query",
          "name" : "page",
          "schema" : {
            "example" : 1,
            "type" : "integer"
          },
          "description" : "Page number to retrieve."
        }, {
          "in" : "query",
          "name" : "order",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of query attribute to order by. Default sort order is ascending. Append a dash (`-`) to order descending instead. \n\n- `name`: The name of the query. \n\n- `created_at`: The timestamp the query was created. \n\n- `schedule`: [DEPRECATED] Sorting results by refresh schedule is deprecated. Use :method:jobs/list to list jobs and filter for a query. \n\n- `runtime`: The time it took to run this query. This is blank for parameterized queries. A blank value is treated as the highest value for sorting. \n\n- `executed_at`: The timestamp when the query was last run. \n\n- `created_by`: The user name of the user that created the query. \n "
        }, {
          "in" : "query",
          "name" : "q",
          "schema" : {
            "example" : "orders by month",
            "type" : "string"
          },
          "description" : "Full text search term"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.QueryList"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets a list of queries. Optionally, this list can be filtered by a search term.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "offset" : "page",
          "limit" : "page_size",
          "results" : "results",
          "increment" : 1
        }
      },
      "post" : {
        "summary" : "Create a new query definition",
        "operationId" : "Queries.create",
        "tags" : [ "Queries / Results" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.Query"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/sql.QueryPostContent"
              }
            }
          },
          "description" : "A query object to be created."
        },
        "description" : "Creates a new query definition. Queries created with this endpoint belong to the authenticated user making the request.\n\nThe `data_source_id` field specifies the ID of the SQL warehouse to run this query against. You can use the Data Sources API to see a complete list of available SQL warehouses. Or you can copy the `data_source_id` from an existing query.\n\n**Note**: You cannot add a visualization until you create the query.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/preview/sql/queries/trash/{query_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "query_id",
        "schema" : {
          "example" : "2cca1687-60ff-4886-a445-0230578c864d",
          "format" : "UUID",
          "type" : "string"
        }
      } ],
      "post" : {
        "summary" : "Restore a query",
        "operationId" : "Queries.restore",
        "tags" : [ "Queries / Results" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Restore a query that has been moved to the trash.\nA restored query appears in list views and searches. You can use restored queries for alerts.\n"
      }
    },
    "/api/2.0/preview/sql/queries/{query_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "query_id",
        "schema" : {
          "example" : "2cca1687-60ff-4886-a445-0230578c864d",
          "format" : "UUID",
          "type" : "string"
        }
      } ],
      "get" : {
        "summary" : "Get a query definition.",
        "operationId" : "Queries.get",
        "tags" : [ "Queries / Results" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.Query"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Retrieve a query object definition along with contextual permissions information about the currently authenticated user.",
        "x-databricks-crud" : "read"
      },
      "post" : {
        "summary" : "Change a query definition",
        "operationId" : "Queries.update",
        "tags" : [ "Queries / Results" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.Query"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/sql.QueryEditContent"
              }
            }
          },
          "description" : "The query definition that will replace the current definition for this `query_id`."
        },
        "description" : "Modify this query definition.\n\n**Note**: You cannot undo this operation.\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a query",
        "operationId" : "Queries.delete",
        "tags" : [ "Queries / Results" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Moves a query to the trash.\nTrashed queries immediately disappear from searches and list views, and they cannot be used for alerts.\nThe trash is deleted after 30 days.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/repos" : {
      "get" : {
        "summary" : "Get repos",
        "operationId" : "Repos.list",
        "tags" : [ "Repos" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "path_prefix",
          "schema" : {
            "type" : "string"
          },
          "description" : "Filters repos that have paths starting with the given path prefix."
        }, {
          "in" : "query",
          "name" : "next_page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "Token used to get the next page of results. If not specified, returns the first page of results as well as a next page token if there are more results."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/repos.ListReposResponse"
                }
              }
            },
            "description" : "Repos were successfully returned."
          }
        },
        "description" : "Returns repos that the calling user has Manage permissions on. Results are paginated with each page containing twenty repos.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "repos",
          "token" : {
            "request" : "next_page_token",
            "response" : "next_page_token"
          }
        }
      },
      "post" : {
        "summary" : "Create a repo",
        "operationId" : "Repos.create",
        "tags" : [ "Repos" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/repos.RepoInfo"
                }
              }
            },
            "description" : "The repo was successfully created."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/repos.CreateRepo"
              }
            }
          },
          "description" : "Details required to create and clone a repo object"
        },
        "description" : "Creates a repo in the workspace and links it to the remote Git repo specified. \nNote that repos created programmatically must be linked to a remote Git repo, unlike repos created in the browser.\n",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_repo\" \"this\" {\n  url = \"https://github.com/user/demo.git\"\n}\n"
        } ]
      },
      "description" : "This endpoint manages repos for a workspace."
    },
    "/api/2.0/repos/{repo_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "repo_id",
        "schema" : {
          "format" : "int64",
          "type" : "integer"
        },
        "description" : "The ID for the corresponding repo to access."
      } ],
      "get" : {
        "summary" : "Get a repo",
        "operationId" : "Repos.get",
        "tags" : [ "Repos" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/repos.RepoInfo"
                }
              }
            },
            "description" : "The repo was successfully returned."
          }
        },
        "description" : "Returns the repo with the given repo ID.",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update a repo",
        "operationId" : "Repos.update",
        "tags" : [ "Repos" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The repo was successfully updated."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/repos.UpdateRepo"
              }
            }
          },
          "description" : "Details required to update the repo"
        },
        "description" : "Updates the repo to a different branch or tag, or updates the repo to the latest commit on the same branch.",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a repo",
        "operationId" : "Repos.delete",
        "tags" : [ "Repos" ],
        "responses" : {
          "200" : {
            "description" : "The repo was successfully deleted."
          }
        },
        "description" : "Deletes the specified repo.",
        "x-databricks-crud" : "delete"
      },
      "description" : "This endpoint manages a specific repo."
    },
    "/api/2.0/secrets/acls/delete" : {
      "post" : {
        "summary" : "Delete an ACL",
        "operationId" : "Secrets.deleteAcl",
        "tags" : [ "Secret" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/secrets.DeleteAcl"
              },
              "example" : {
                "principal" : "data-scientists",
                "scope" : "my-secret-scope"
              }
            }
          },
          "description" : ""
        },
        "description" : "Deletes the given ACL on the given scope.\n\nUsers must have the `MANAGE` permission to invoke this API.\nThrows `RESOURCE_DOES_NOT_EXIST` if no such secret scope, principal, or ACL exists.\nThrows `PERMISSION_DENIED` if the user does not have permission to make this API call.\n",
        "x-databricks-shortcut" : true
      }
    },
    "/api/2.0/secrets/acls/get" : {
      "get" : {
        "summary" : "Get secret ACL details",
        "operationId" : "Secrets.getAcl",
        "tags" : [ "Secret" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "scope",
          "schema" : {
            "type" : "string"
          },
          "description" : "The name of the scope to fetch ACL information from."
        }, {
          "required" : true,
          "in" : "query",
          "name" : "principal",
          "schema" : {
            "type" : "string"
          },
          "description" : "The principal to fetch ACL information for."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/secrets.AclItem"
                },
                "example" : {
                  "permission" : "READ",
                  "principal" : "data-scientists"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets the details about the given ACL, such as the group and permission.\nUsers must have the `MANAGE` permission to invoke this API.\n\nThrows `RESOURCE_DOES_NOT_EXIST` if no such secret scope exists.\nThrows `PERMISSION_DENIED` if the user does not have permission to make this API call.\n",
        "x-databricks-shortcut" : true
      }
    },
    "/api/2.0/secrets/acls/list" : {
      "get" : {
        "summary" : "Lists ACLs",
        "operationId" : "Secrets.listAcls",
        "tags" : [ "Secret" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "scope",
          "schema" : {
            "type" : "string"
          },
          "description" : "The name of the scope to fetch ACL information from."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/secrets.ListAclsResponse"
                },
                "example" : {
                  "acls" : [ {
                    "permission" : "MANAGE",
                    "principal" : "admins"
                  }, {
                    "permission" : "READ",
                    "principal" : "data-scientists"
                  } ]
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "List the ACLs for a given secret scope. Users must have the `MANAGE` permission to invoke this API.\n\nThrows `RESOURCE_DOES_NOT_EXIST` if no such secret scope exists.\nThrows `PERMISSION_DENIED` if the user does not have permission to make this API call.\n",
        "x-databricks-shortcut" : true,
        "x-databricks-pagination" : {
          "results" : "items"
        }
      }
    },
    "/api/2.0/secrets/acls/put" : {
      "post" : {
        "summary" : "Create/update an ACL",
        "operationId" : "Secrets.putAcl",
        "tags" : [ "Secret" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/secrets.PutAcl"
              },
              "example" : {
                "permission" : "READ",
                "principal" : "data-scientists",
                "scope" : "my-secret-scope"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates or overwrites the Access Control List (ACL) associated with the given principal (user or group) on the\nspecified scope point.\n\nIn general, a user or group will use the most powerful permission available to them,\nand permissions are ordered as follows:\n\n* `MANAGE` - Allowed to change ACLs, and read and write to this secret scope.\n* `WRITE` - Allowed to read and write to this secret scope.\n* `READ` - Allowed to read this secret scope and list what secrets are available.\n\nNote that in general, secret values can only be read from within a command\\non a cluster (for example, through a notebook).\nThere is no API to read the actual secret value material outside of a cluster.\nHowever, the user's permission will be applied based on who is executing the command, and they must have at least READ permission.\n\nUsers must have the `MANAGE` permission to invoke this API.\n\nThe principal is a user or group name corresponding to an existing Databricks principal to be granted or revoked access.\n\nThrows `RESOURCE_DOES_NOT_EXIST` if no such secret scope exists.\nThrows `RESOURCE_ALREADY_EXISTS` if a permission for the principal already exists.\nThrows `INVALID_PARAMETER_VALUE` if the permission is invalid.\nThrows `PERMISSION_DENIED` if the user does not have permission to make this API call.\n",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_group\" \"ds\" {\n  display_name = \"data-scientists\"\n}\n\nresource \"databricks_secret_scope\" \"app\" {\n  name = \"app-secret-scope\"\n}\n\nresource \"databricks_secret_acl\" \"my_secret_acl\" {\n  principal  = databricks_group.ds.display_name\n  permission = \"READ\"\n  scope      = databricks_secret_scope.app.name\n}\n"
        } ]
      }
    },
    "/api/2.0/secrets/delete" : {
      "post" : {
        "summary" : "Delete a secret",
        "operationId" : "Secrets.deleteSecret",
        "tags" : [ "Secret" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/secrets.DeleteSecretResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/secrets.DeleteSecret"
              },
              "example" : {
                "key" : "my-secret-key",
                "scope" : "my-secret-scope"
              }
            }
          },
          "description" : ""
        },
        "description" : "Deletes the secret stored in this secret scope. \nYou must have `WRITE` or `MANAGE` permission on the secret scope.\n\nThrows `RESOURCE_DOES_NOT_EXIST` if no such secret scope or secret exists.\nThrows `PERMISSION_DENIED` if the user does not have permission to make this API call.\n",
        "x-databricks-crud" : "delete",
        "x-databricks-shortcut" : true
      }
    },
    "/api/2.0/secrets/list" : {
      "get" : {
        "summary" : "List secret keys",
        "operationId" : "Secrets.listSecrets",
        "tags" : [ "Secret" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "scope",
          "schema" : {
            "type" : "string"
          },
          "description" : "The name of the scope to list secrets within."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/secrets.ListSecretsResponse"
                },
                "example" : {
                  "secrets" : [ {
                    "key" : "my-string-key",
                    "last_updated_timestamp" : "1520467595000"
                  }, {
                    "key" : "my-byte-key",
                    "last_updated_timestamp" : "1520467595000"
                  } ]
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Lists the secret keys that are stored at this scope. \nThis is a metadata-only operation; secret data cannot be retrieved using this API. \nUsers need the READ permission to make this call.\n\nThe lastUpdatedTimestamp returned is in milliseconds since epoch.\nThrows `RESOURCE_DOES_NOT_EXIST` if no such secret scope exists.\nThrows `PERMISSION_DENIED` if the user does not have permission to make this API call.\n",
        "x-databricks-crud" : "list",
        "x-databricks-shortcut" : true,
        "x-databricks-pagination" : {
          "results" : "secrets"
        }
      }
    },
    "/api/2.0/secrets/put" : {
      "post" : {
        "summary" : "Add a secret",
        "operationId" : "Secrets.putSecret",
        "tags" : [ "Secret" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/secrets.PutSecret"
              },
              "example" : {
                "key" : "my-string-key",
                "scope" : "my-databricks-scope",
                "string_value" : "foobar"
              }
            }
          },
          "description" : ""
        },
        "description" : "Inserts a secret under the provided scope with the given name. \nIf a secret already exists with the same name, this command overwrites the existing secret's value.\nThe server encrypts the secret using the secret scope's encryption settings before storing it. \n\nYou must have `WRITE` or `MANAGE` permission on the secret scope.\nThe secret key must consist of alphanumeric characters, dashes, underscores, and periods, and cannot exceed 128 characters. \nThe maximum allowed secret value size is 128 KB. The maximum number of secrets in a given scope is 1000.\n\nThe input fields \"string_value\" or \"bytes_value\" specify the type of the secret, which will determine the value returned when \nthe secret value is requested. Exactly one must be specified.\n\nThrows `RESOURCE_DOES_NOT_EXIST` if no such secret scope exists.\nThrows `RESOURCE_LIMIT_EXCEEDED` if maximum number of secrets in scope is exceeded.\nThrows `INVALID_PARAMETER_VALUE` if the key name or value length is invalid.\nThrows `PERMISSION_DENIED` if the user does not have permission to make this API call.\n",
        "x-databricks-crud" : "update",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_secret_scope\" \"app\" {\n  name = \"application-secret-scope\"\n}\n\nresource \"databricks_secret\" \"publishing_api\" {\n  key          = \"publishing_api\"\n  string_value = data.azurerm_key_vault_secret.example.value\n  scope        = databricks_secret_scope.app.id\n}\n\nresource \"databricks_cluster\" \"this\" {\n  # ...\n  spark_conf = {\n    # ...\n    \"fs.azure.account.oauth2.client.secret\" = databricks_secret.publishing_api.config_reference\n  }\n}\n"
        } ]
      }
    },
    "/api/2.0/secrets/scopes/create" : {
      "post" : {
        "summary" : "Create a new secret scope",
        "operationId" : "Secrets.createScope",
        "tags" : [ "Secret" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/secrets.CreateScope"
              }
            }
          },
          "description" : ""
        },
        "description" : "The scope name must consist of alphanumeric characters, dashes, underscores, and periods, \nand may not exceed 128 characters. The maximum number of scopes in a workspace is 100.\n",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_secret_scope\" \"this\" {\n  name = \"terraform-demo-scope\"\n}\n"
        } ]
      }
    },
    "/api/2.0/secrets/scopes/delete" : {
      "post" : {
        "summary" : "Delete a secret scope",
        "operationId" : "Secrets.deleteScope",
        "tags" : [ "Secret" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/secrets.DeleteScope"
              },
              "example" : {
                "scope" : "my-secret-scope"
              }
            }
          },
          "description" : ""
        },
        "description" : "Deletes a secret scope. \n\nThrows `RESOURCE_DOES_NOT_EXIST` if the scope does not exist.\nThrows `PERMISSION_DENIED` if the user does not have permission to make this API call.\n",
        "x-databricks-shortcut" : true
      }
    },
    "/api/2.0/secrets/scopes/list" : {
      "get" : {
        "summary" : "List all scopes",
        "operationId" : "Secrets.listScopes",
        "tags" : [ "Secret" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/secrets.ListScopesResponse"
                },
                "example" : {
                  "scopes" : [ {
                    "backend_type" : "DATABRICKS",
                    "name" : "my-databricks-scope"
                  }, {
                    "backend_type" : "DATABRICKS",
                    "name" : "mount-points"
                  } ]
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Lists all secret scopes available in the workspace. \n\nThrows `PERMISSION_DENIED` if the user does not have permission to make this API call.\n",
        "x-databricks-pagination" : {
          "results" : "scopes"
        }
      }
    },
    "/api/2.0/serving-endpoints" : {
      "get" : {
        "summary" : "Retrieve all serving endpoints",
        "operationId" : "ServingEndpoints.list",
        "tags" : [ "Serving endpoints" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/endpoints.ListEndpointsResponse"
                }
              }
            },
            "description" : "List of serving endpoints was retrieved successfully."
          }
        },
        "x-databricks-crud" : "list"
      },
      "post" : {
        "summary" : "Create a new serving endpoint",
        "operationId" : "ServingEndpoints.create",
        "tags" : [ "Serving endpoints" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/endpoints.ServingEndpointDetailed"
                }
              }
            },
            "description" : "Serving endpoint was created successfully"
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/endpoints.CreateServingEndpoint"
              }
            }
          },
          "description" : ""
        },
        "x-databricks-crud" : "create",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "name",
          "field" : [ "state", "config_update" ],
          "success" : [ "NOT_UPDATING" ],
          "failure" : [ "UPDATE_FAILED" ]
        }
      }
    },
    "/api/2.0/serving-endpoints/{name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "example" : "feed-ads",
          "type" : "string"
        },
        "description" : "The name of the serving endpoint. This field is required."
      } ],
      "get" : {
        "summary" : "Get a single serving endpoint",
        "operationId" : "ServingEndpoints.get",
        "tags" : [ "Serving endpoints" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/endpoints.ServingEndpointDetailed"
                }
              }
            },
            "description" : "Serving endpoint was retrieved successfully."
          }
        },
        "description" : "Retrieves the details for a single serving endpoint.",
        "x-databricks-crud" : "read"
      },
      "delete" : {
        "summary" : "Delete a serving endpoint",
        "operationId" : "ServingEndpoints.delete",
        "tags" : [ "Serving endpoints" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Serving endpoint was deleted successfully."
          }
        },
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/serving-endpoints/{name}/config" : {
      "put" : {
        "summary" : "Update a serving endpoint with a new config.",
        "operationId" : "ServingEndpoints.updateConfig",
        "tags" : [ "Serving endpoints" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "name",
          "schema" : {
            "example" : "feed-ads",
            "type" : "string"
          },
          "description" : "The name of the serving endpoint to update. This field is required."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/endpoints.ServingEndpointDetailed"
                }
              }
            },
            "description" : "Serving endpoint update was issued successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/endpoints.EndpointCoreConfigInput"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates any combination of the serving endpoint's served models, the compute \nconfiguration of those served models, and the endpoint's traffic config.\nAn endpoint that already has an update in progress can not be updated until\nthe current update completes or fails.\n",
        "x-databricks-crud" : "update",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "name",
          "field" : [ "state", "config_update" ],
          "success" : [ "NOT_UPDATING" ],
          "failure" : [ "UPDATE_FAILED" ]
        }
      }
    },
    "/api/2.0/serving-endpoints/{name}/metrics" : {
      "get" : {
        "summary" : "Retrieve the metrics corresponding to a serving endpoint for the current time in Prometheus or OpenMetrics exposition format",
        "operationId" : "ServingEndpoints.exportMetrics",
        "tags" : [ "Serving endpoints" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "name",
          "schema" : {
            "example" : "feed-ads",
            "type" : "string"
          },
          "description" : "The name of the serving endpoint to retrieve metrics for. This field is required."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/openmetrics-text; version=1.0.0" : {
                "schema" : {
                  "example" : "# TYPE mem_usage_percentage gauge\n# HELP mem_usage_percentage Average memory utilization used across all replicas\nmem_usage_percentage{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\"} 2.102 1673463060.000\n# TYPE request_latency_ms histogram\n# HELP request_latency_ms Request latency histogram\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"100\"} 0.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"60000\"} 19.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"2500\"} 19.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"250\"} 11.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"300000\"} 19.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"50\"} 0.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"5\"} 0.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"+inf\"} 19.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"500\"} 13.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"10\"} 0.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"600000\"} 19.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"25\"} 0.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"30000\"} 19.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"1000\"} 19.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"5000\"} 19.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"10000\"} 19.0 1673463060.000\nrequest_latency_ms_count{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\"} 19.0 1673463060.000\nrequest_latency_ms_sum{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\"} 9750.0 1673463060.000\n# TYPE provisioned_concurrent_requests_total gauge\n# HELP provisioned_concurrent_requests_total Total number of provisioned concurrency\nprovisioned_concurrent_requests_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\"} 4.0 1673463060.000\n# TYPE request_count_total gauge\n# HELP request_count_total Total number of requests during the minute\nrequest_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\"} 19.0 1673463060.000\n# TYPE request_5xx_count_total gauge\n# HELP request_5xx_count_total Total number of 5xx errors during the minute\nrequest_5xx_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\"} 0.0 1673463060.000\n# TYPE cpu_usage_percentage gauge\n# HELP cpu_usage_percentage Average CPU utilization used across all replicas\ncpu_usage_percentage{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\"} 0.07 1673463060.000\n# TYPE request_4xx_count_total gauge\n# HELP request_4xx_count_total Total number of 4xx errors during the minute\nrequest_4xx_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\"} 0.0 1673463060.000\n# EOF\n# TYPE mem_usage_percentage gauge\n# HELP mem_usage_percentage Average memory utilization used across all replicas\nmem_usage_percentage{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\"} 2.129 1673463060.000\n# TYPE request_count_total gauge\n# HELP request_count_total Total number of requests during the minute\nrequest_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\"} 18.0 1673463060.000\n# TYPE provisioned_concurrent_requests_total gauge\n# HELP provisioned_concurrent_requests_total Total number of provisioned concurrency\nprovisioned_concurrent_requests_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\"} 4.0 1673463060.000\n# TYPE cpu_usage_percentage gauge\n# HELP cpu_usage_percentage Average CPU utilization used across all replicas\ncpu_usage_percentage{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\"} 0.062 1673463060.000\n# TYPE request_5xx_count_total gauge\n# HELP request_5xx_count_total Total number of 5xx errors during the minute\nrequest_5xx_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\"} 0.0 1673463060.000\n# TYPE request_4xx_count_total gauge\n# HELP request_4xx_count_total Total number of 4xx errors during the minute\nrequest_4xx_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\"} 0.0 1673463060.000\n# TYPE request_latency_ms histogram\n# HELP request_latency_ms Request latency histogram\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"100\"} 0.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"60000\"} 18.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"2500\"} 18.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"250\"} 6.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"300000\"} 18.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"50\"} 0.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"5\"} 0.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"+inf\"} 18.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"500\"} 14.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"10\"} 0.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"600000\"} 18.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"25\"} 0.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"30000\"} 18.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"1000\"} 18.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"5000\"} 18.0 1673463060.000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"10000\"} 18.0 1673463060.000\nrequest_latency_ms_count{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\"} 18.0 1673463060.000\nrequest_latency_ms_sum{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\"} 9500.0 1673463060.000\n# EOF\n",
                  "type" : "string"
                }
              },
              "text/plain; version=0.0.4; charset=utf-8" : {
                "schema" : {
                  "example" : "# HELP request_5xx_count_total Total number of 5xx errors during the minute\n# TYPE request_5xx_count_total gauge\nrequest_5xx_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",} 0.0 1673462820000\n# HELP cpu_usage_percentage Average CPU utilization used across all replicas\n# TYPE cpu_usage_percentage gauge\ncpu_usage_percentage{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",} 0.056 1673462820000\n# HELP provisioned_concurrent_requests_total Total number of provisioned concurrency\n# TYPE provisioned_concurrent_requests_total gauge\nprovisioned_concurrent_requests_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",} 4.0 1673462820000\n# HELP mem_usage_percentage Average memory utilization used across all replicas\n# TYPE mem_usage_percentage gauge\nmem_usage_percentage{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",} 2.102 1673462820000\n# HELP request_count_total Total number of requests during the minute\n# TYPE request_count_total gauge\nrequest_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",} 14.0 1673462820000\n# HELP request_latency_ms Request latency histogram\n# TYPE request_latency_ms histogram\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"100\",} 0.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"60000\",} 14.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"2500\",} 14.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"250\",} 7.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"300000\",} 14.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"50\",} 0.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"5\",} 0.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"+inf\",} 14.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"500\",} 9.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"10\",} 0.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"600000\",} 14.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"25\",} 0.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"30000\",} 14.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"1000\",} 14.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"5000\",} 14.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",le=\"10000\",} 14.0 1673462820000\nrequest_latency_ms_count{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",} 14.0 1673462820000\nrequest_latency_ms_sum{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",} 7750.0 1673462820000\n# HELP request_4xx_count_total Total number of 4xx errors during the minute\n# TYPE request_4xx_count_total gauge\nrequest_4xx_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-1\",} 0.0 1673462820000\n# HELP request_latency_ms Request latency histogram\n# TYPE request_latency_ms histogram\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"100\",} 0.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"60000\",} 22.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"2500\",} 22.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"250\",} 10.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"300000\",} 22.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"50\",} 0.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"5\",} 0.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"+inf\",} 22.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"500\",} 21.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"10\",} 0.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"600000\",} 22.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"25\",} 0.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"30000\",} 22.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"1000\",} 22.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"5000\",} 22.0 1673462820000\nrequest_latency_ms_bucket{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",le=\"10000\",} 22.0 1673462820000\nrequest_latency_ms_count{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",} 22.0 1673462820000\nrequest_latency_ms_sum{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",} 9000.0 1673462820000\n# HELP mem_usage_percentage Average memory utilization used across all replicas\n# TYPE mem_usage_percentage gauge\nmem_usage_percentage{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",} 2.129 1673462820000\n# HELP provisioned_concurrent_requests_total Total number of provisioned concurrency\n# TYPE provisioned_concurrent_requests_total gauge\nprovisioned_concurrent_requests_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",} 4.0 1673462820000\n# HELP request_4xx_count_total Total number of 4xx errors during the minute\n# TYPE request_4xx_count_total gauge\nrequest_4xx_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",} 0.0 1673462820000\n# HELP cpu_usage_percentage Average CPU utilization used across all replicas\n# TYPE cpu_usage_percentage gauge\ncpu_usage_percentage{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",} 0.072 1673462820000\n# HELP request_count_total Total number of requests during the minute\n# TYPE request_count_total gauge\nrequest_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",} 22.0 1673462820000\n# HELP request_5xx_count_total Total number of 5xx errors during the minute\n# TYPE request_5xx_count_total gauge\nrequest_5xx_count_total{workspaceId=\"5236064513646960\",endpointName=\"feed-ads\",servedModelName=\"ads-model-2\",} 0.0 1673462820000\n",
                  "type" : "string"
                }
              }
            },
            "description" : "Metrics for the serving endpoint in Prometheus exposition or OpenMetrics exposition format were fetched successfully."
          }
        },
        "description" : "Retrieves the metrics associated with the provided serving endpoint in either Prometheus or OpenMetrics exposition format.\nPlease note that this API is in preview and may change in the future.\n",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/serving-endpoints/{name}/served-models/{served_model_name}/build-logs" : {
      "get" : {
        "summary" : "Retrieve the logs associated with building the model's environment for a given serving endpoint's served model.",
        "operationId" : "ServingEndpoints.buildLogs",
        "tags" : [ "Serving endpoints" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "name",
          "schema" : {
            "example" : "feed-ads",
            "type" : "string"
          },
          "description" : "The name of the serving endpoint that the served model belongs to. This field is required."
        }, {
          "required" : true,
          "in" : "path",
          "name" : "served_model_name",
          "schema" : {
            "example" : "ads-model-3",
            "type" : "string"
          },
          "description" : "The name of the served model that build logs will be retrieved for. This field is required."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/endpoints.BuildLogsResponse"
                }
              }
            },
            "description" : "Build logs were fetched successfully."
          }
        },
        "description" : "Retrieves the build logs associated with the provided served model.\nPlease note that this API is in preview and may change in the future.\n",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/serving-endpoints/{name}/served-models/{served_model_name}/logs" : {
      "get" : {
        "summary" : "Retrieve the most recent log lines associated with a given serving endpoint's served model",
        "operationId" : "ServingEndpoints.logs",
        "tags" : [ "Serving endpoints" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "name",
          "schema" : {
            "example" : "feed-ads",
            "type" : "string"
          },
          "description" : "The name of the serving endpoint that the served model belongs to. This field is required."
        }, {
          "required" : true,
          "in" : "path",
          "name" : "served_model_name",
          "schema" : {
            "example" : "ads-model-3",
            "type" : "string"
          },
          "description" : "The name of the served model that logs will be retrieved for. This field is required."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/endpoints.ServerLogsResponse"
                }
              }
            },
            "description" : "Logs were fetched successfully."
          }
        },
        "description" : "Retrieves the service logs associated with the provided served model.\nPlease note that this API is in preview and may change in the future.\n",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/sql/config/warehouses" : {
      "get" : {
        "summary" : "Get the workspace configuration",
        "operationId" : "Warehouses.getWorkspaceWarehouseConfig",
        "tags" : [ "SQL Warehouses" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.GetWorkspaceWarehouseConfigResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets the workspace level configuration that is shared by all SQL warehouses in a workspace."
      },
      "put" : {
        "summary" : "Set the workspace configuration",
        "operationId" : "Warehouses.setWorkspaceWarehouseConfig",
        "tags" : [ "SQL Warehouses" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.SetWorkspaceWarehouseConfigResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/sql.SetWorkspaceWarehouseConfigRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Sets the workspace level configuration that is shared by all SQL warehouses in a workspace."
      }
    },
    "/api/2.0/sql/history/queries" : {
      "get" : {
        "summary" : "List Queries",
        "operationId" : "QueryHistory.list",
        "tags" : [ "Query History" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "filter_by",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.QueryFilter"
          },
          "description" : "A filter to limit query history results. This field is optional."
        }, {
          "in" : "query",
          "name" : "max_results",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Limit the number of results returned in one page. The default is 100."
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "A token that can be used to get the next page of results."
        }, {
          "in" : "query",
          "name" : "include_metrics",
          "schema" : {
            "type" : "boolean"
          },
          "description" : "Whether to include metrics about query."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.ListQueriesResponse"
                }
              }
            },
            "description" : "Queries listed successfully."
          }
        },
        "description" : "List the history of queries through SQL warehouses.\n\nYou can filter by user ID, warehouse ID, status, and time range.\n",
        "x-databricks-pagination" : {
          "results" : "res",
          "token" : {
            "request" : "page_token",
            "response" : "next_page_token"
          }
        }
      }
    },
    "/api/2.0/sql/statements/" : {
      "post" : {
        "summary" : "Execute an SQL statement",
        "operationId" : "StatementExecution.executeStatement",
        "tags" : [ "Statement Execution" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "manifest" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/sql.ResultManifest"
                    },
                    "result" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/sql.ResultData"
                    },
                    "statement_id" : {
                      "description" : "Statement ID is returned upon successful submission of a SQL statement, and is a required reference for all\nsubsequent calls.\n",
                      "type" : "string"
                    },
                    "status" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/sql.StatementStatus"
                    }
                  },
                  "type" : "object"
                },
                "examples" : {
                  "statement_response_ext_links_succeeded" : {
                    "summary" : "Large result sets with EXTERNAL_LINKS + ARROW_STREAM",
                    "value" : {
                      "manifest" : {
                        "format" : "ARROW_STREAM",
                        "total_chunk_count" : 1,
                        "chunks" : [ {
                          "chunk_index" : 0,
                          "row_count" : 100,
                          "row_offset" : 0
                        } ],
                        "schema" : {
                          "column_count" : 1,
                          "columns" : [ {
                            "name" : "id",
                            "position" : 0,
                            "type_name" : "LONG",
                            "type_text" : "BIGINT"
                          } ]
                        },
                        "total_byte_count" : 16160,
                        "total_row_count" : 100
                      },
                      "result" : {
                        "external_links" : [ {
                          "external_link" : "https://someplace.s3.us-west-2.amazonaws.com/very/long/path/...",
                          "row_count" : 100,
                          "expiration" : "2023-01-30T22:23:23.140Z",
                          "chunk_index" : 0,
                          "row_offset" : 0
                        } ]
                      },
                      "statement_id" : "01eda0ea-9b4b-15ce-b8bb-a7d4114cb5ed",
                      "status" : {
                        "state" : "SUCCEEDED"
                      }
                    }
                  },
                  "statement_response_inline_succeeded" : {
                    "summary" : "JSON_ARRAY formatted data returned INLINE",
                    "value" : {
                      "manifest" : {
                        "format" : "JSON_ARRAY",
                        "schema" : {
                          "column_count" : 1,
                          "columns" : [ {
                            "name" : "id",
                            "position" : 0,
                            "type_name" : "LONG",
                            "type_text" : "BIGINT"
                          } ]
                        }
                      },
                      "result" : {
                        "chunk_index" : 0,
                        "data_array" : [ [ "0" ], [ "1" ], [ "2" ] ],
                        "row_count" : 3,
                        "row_offset" : 0
                      },
                      "statement_id" : "01eda0e7-e315-1846-84e2-79a963ffad44",
                      "status" : {
                        "state" : "SUCCEEDED"
                      }
                    }
                  },
                  "statement_response_running" : {
                    "summary" : "Call mode: asynchronous. Submission is accepted",
                    "value" : {
                      "statement_id" : "01ed9db9-24c4-1cb6-a320-fb6ebbe7410d",
                      "status" : {
                        "state" : "RUNNING"
                      }
                    }
                  }
                }
              }
            },
            "description" : "StatementResponse will contain `statement_id` and `status`; other fields may be absent or\npresent depending on context. See each field for its description.\n"
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/sql.ExecuteStatementRequest"
              },
              "examples" : {
                "execute_statement_async" : {
                  "summary" : "Call mode: asynchronous",
                  "value" : {
                    "statement" : "SELECT * FROM range(100)",
                    "wait_timeout" : "0s",
                    "warehouse_id" : "abcdef0123456789"
                  }
                },
                "execute_statement_with_defaults" : {
                  "summary" : "Call mode: synchronous. Deafult parameters",
                  "value" : {
                    "statement" : "SELECT * FROM range(3)",
                    "warehouse_id" : "abcdef0123456789"
                  }
                },
                "execute_statement_sync_continue" : {
                  "summary" : "Call mode: synchronous. Continue on timeout",
                  "value" : {
                    "on_wait_timeout" : "CONTINUE",
                    "statement" : "SELECT * FROM range(100)",
                    "wait_timeout" : "30s",
                    "warehouse_id" : "abcdef0123456789"
                  }
                },
                "execute_statement_sync_cancel" : {
                  "summary" : "Call mode: synchronous. Cancel on timeout",
                  "value" : {
                    "on_wait_timeout" : "CANCEL",
                    "statement" : "SELECT * FROM range(100)",
                    "wait_timeout" : "30s",
                    "warehouse_id" : "abcdef0123456789"
                  }
                },
                "execute_statement_sync_ext_links" : {
                  "summary" : "Large result sets with EXTERNAL_LINKS + ARROW_STREAM",
                  "value" : {
                    "disposition" : "EXTERNAL_LINKS",
                    "format" : "ARROW_STREAM",
                    "statement" : "SELECT * FROM range(100)",
                    "warehouse_id" : "abcdef0123456789"
                  }
                }
              }
            }
          },
          "description" : ""
        },
        "description" : "Execute an SQL statement, and if flagged as such, await its result for a specified time.\n"
      }
    },
    "/api/2.0/sql/statements/{statement_id}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/sql.statementId"
      } ],
      "get" : {
        "summary" : "Get status, manifest, and result first chunk",
        "operationId" : "StatementExecution.getStatement",
        "tags" : [ "Statement Execution" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "statement_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "manifest" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/sql.ResultManifest"
                    },
                    "result" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/sql.ResultData"
                    },
                    "statement_id" : {
                      "description" : "Statement ID is returned upon successful submission of a SQL statement, and is a required reference for all\nsubsequent calls.\n",
                      "type" : "string"
                    },
                    "status" : {
                      "extRef" : true,
                      "ref" : true,
                      "$ref" : "#/components/schemas/sql.StatementStatus"
                    }
                  },
                  "type" : "object"
                },
                "examples" : {
                  "statement_response_ext_links_succeeded" : {
                    "summary" : "Large result sets with EXTERNAL_LINKS + ARROW_STREAM",
                    "value" : {
                      "manifest" : {
                        "format" : "ARROW_STREAM",
                        "total_chunk_count" : 1,
                        "chunks" : [ {
                          "chunk_index" : 0,
                          "row_count" : 100,
                          "row_offset" : 0
                        } ],
                        "schema" : {
                          "column_count" : 1,
                          "columns" : [ {
                            "name" : "id",
                            "position" : 0,
                            "type_name" : "LONG",
                            "type_text" : "BIGINT"
                          } ]
                        },
                        "total_byte_count" : 16160,
                        "total_row_count" : 100
                      },
                      "result" : {
                        "external_links" : [ {
                          "external_link" : "https://someplace.s3.us-west-2.amazonaws.com/very/long/path/...",
                          "row_count" : 100,
                          "expiration" : "2023-01-30T22:23:23.140Z",
                          "chunk_index" : 0,
                          "row_offset" : 0
                        } ]
                      },
                      "statement_id" : "01eda0ea-9b4b-15ce-b8bb-a7d4114cb5ed",
                      "status" : {
                        "state" : "SUCCEEDED"
                      }
                    }
                  },
                  "statement_response_inline_succeeded" : {
                    "summary" : "JSON_ARRAY formatted data returned INLINE",
                    "value" : {
                      "manifest" : {
                        "format" : "JSON_ARRAY",
                        "schema" : {
                          "column_count" : 1,
                          "columns" : [ {
                            "name" : "id",
                            "position" : 0,
                            "type_name" : "LONG",
                            "type_text" : "BIGINT"
                          } ]
                        }
                      },
                      "result" : {
                        "chunk_index" : 0,
                        "data_array" : [ [ "0" ], [ "1" ], [ "2" ] ],
                        "row_count" : 3,
                        "row_offset" : 0
                      },
                      "statement_id" : "01eda0e7-e315-1846-84e2-79a963ffad44",
                      "status" : {
                        "state" : "SUCCEEDED"
                      }
                    }
                  },
                  "statement_response_running" : {
                    "summary" : "Call mode: asynchronous. Submission is accepted",
                    "value" : {
                      "statement_id" : "01ed9db9-24c4-1cb6-a320-fb6ebbe7410d",
                      "status" : {
                        "state" : "RUNNING"
                      }
                    }
                  }
                }
              }
            },
            "description" : "StatementResponse will contain `statement_id` and `status`; other fields may be absent or\npresent depending on context. See each field for its description.\n"
          }
        },
        "description" : "Polls for statement status; when status.state=SUCCEEDED will also return result manifest, and the first chunk\nof result data.\n\n**NOTE**\nThis call currently may take up to 5 seconds to get latest status and result.\n"
      }
    },
    "/api/2.0/sql/statements/{statement_id}/cancel" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/sql.statementId"
      } ],
      "post" : {
        "summary" : "Cancel statement execution",
        "operationId" : "StatementExecution.cancelExecution",
        "tags" : [ "Statement Execution" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "statement_id",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "description" : "Cancel response is empty; receiving response indicates successful receipt."
          }
        },
        "description" : "Requests that an executing statement be cancelled. Callers must poll for status to see terminal state.\n"
      }
    },
    "/api/2.0/sql/statements/{statement_id}/result/chunks/{chunk_index}" : {
      "parameters" : [ {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/sql.statementId"
      }, {
        "ref" : true,
        "extRef" : true,
        "$ref" : "#/components/parameters/sql.chunkIndex"
      }, {
        "required" : true,
        "in" : "query",
        "name" : "row_offset",
        "schema" : {
          "format" : "int64",
          "type" : "integer"
        }
      } ],
      "get" : {
        "summary" : "Get result chunk by index",
        "operationId" : "StatementExecution.getStatementResultChunkN",
        "tags" : [ "Statement Execution" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "statement_id",
          "schema" : {
            "type" : "string"
          }
        }, {
          "required" : true,
          "in" : "path",
          "name" : "chunk_index",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.ResultData"
                },
                "examples" : {
                  "get_result_chunk_inline_next" : {
                    "summary" : "Small result sets with INLINE + JSON_ARRAY",
                    "value" : {
                      "next_chunk_internal_link" : "/api/2.0/sql/statements/01eda0ed-9149-1648-bb5d-46d635ab7d74/result/chunks/1?row_offset=139800",
                      "row_count" : 139800,
                      "next_chunk_index" : 1,
                      "data_array" : [ [ "0-54-0.8256375429534709" ], [ "// ... 139799 more rows" ] ],
                      "chunk_index" : 0,
                      "row_offset" : 0
                    }
                  }
                }
              }
            },
            "description" : "Successful return; depending on `disposition` returns chunks of data either inline, or as links."
          }
        },
        "description" : "After statement execution has SUCCEEDED, result data can be fetched by chunks.\n\nThe first chunk (`chunk_index=0`) is typically fetched through `getStatementResult`, and subsequent chunks\nwith this call. The response structure is identical to the nested `result` element described in\ngetStatementResult, and similarly includes `next_chunk_index` and `next_chunk_internal_link` for simple\niteration through the result set.\n"
      }
    },
    "/api/2.0/sql/warehouses" : {
      "get" : {
        "summary" : "List warehouses",
        "operationId" : "Warehouses.list",
        "tags" : [ "SQL Warehouses" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "run_as_user_id",
          "schema" : {
            "type" : "integer"
          },
          "description" : "Service Principal which will be used to fetch the list of endpoints.\nIf not specified, the user from the session header is used."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.ListWarehousesResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Lists all SQL warehouses that a user has manager permissions on.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "warehouses"
        }
      },
      "post" : {
        "summary" : "Create a warehouse",
        "operationId" : "Warehouses.create",
        "tags" : [ "SQL Warehouses" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.CreateWarehouseResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/sql.CreateWarehouseRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new SQL warehouse.",
        "x-databricks-crud" : "create",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "STOPPED", "DELETED" ],
          "message" : [ "health", "summary" ]
        },
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "data \"databricks_current_user\" \"me\" {}\n\nresource \"databricks_sql_endpoint\" \"this\" {\n  name             = \"Endpoint of ${data.databricks_current_user.me.alphanumeric}\"\n  cluster_size     = \"Small\"\n  max_num_clusters = 1\n\n  tags {\n    custom_tags {\n      key   = \"City\"\n      value = \"Amsterdam\"\n    }\n  }\n}\n"
        } ]
      }
    },
    "/api/2.0/sql/warehouses/{id}" : {
      "get" : {
        "summary" : "Get warehouse info",
        "operationId" : "Warehouses.get",
        "tags" : [ "SQL Warehouses" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "id",
          "schema" : {
            "type" : "string"
          },
          "description" : "Required. Id of the SQL warehouse."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.GetWarehouseResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets the information for a single SQL warehouse.",
        "x-databricks-crud" : "read",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "STOPPED", "DELETED" ],
          "message" : [ "health", "summary" ]
        }
      },
      "delete" : {
        "summary" : "Delete a warehouse",
        "operationId" : "Warehouses.delete",
        "tags" : [ "SQL Warehouses" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "id",
          "schema" : {
            "type" : "string"
          },
          "description" : "Required. Id of the SQL warehouse."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.DeleteWarehouseResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Deletes a SQL warehouse.",
        "x-databricks-crud" : "delete",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "id",
          "field" : [ "state" ],
          "success" : [ "DELETED" ],
          "message" : [ "health", "summary" ]
        }
      }
    },
    "/api/2.0/sql/warehouses/{id}/edit" : {
      "post" : {
        "summary" : "Update a warehouse",
        "operationId" : "Warehouses.edit",
        "tags" : [ "SQL Warehouses" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "id",
          "schema" : {
            "type" : "string"
          },
          "description" : "Required. Id of the warehouse to configure."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.EditWarehouseResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/sql.EditWarehouseRequest"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the configuration for a SQL warehouse.",
        "x-databricks-crud" : "update",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "STOPPED", "DELETED" ],
          "message" : [ "health", "summary" ]
        }
      }
    },
    "/api/2.0/sql/warehouses/{id}/start" : {
      "post" : {
        "summary" : "Start a warehouse",
        "operationId" : "Warehouses.start",
        "tags" : [ "SQL Warehouses" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "id",
          "schema" : {
            "type" : "string"
          },
          "description" : "Required. Id of the SQL warehouse."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.StartWarehouseResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Starts a SQL warehouse.",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "id",
          "field" : [ "state" ],
          "success" : [ "RUNNING" ],
          "failure" : [ "STOPPED", "DELETED" ],
          "message" : [ "health", "summary" ]
        }
      }
    },
    "/api/2.0/sql/warehouses/{id}/stop" : {
      "post" : {
        "summary" : "Stop a warehouse",
        "operationId" : "Warehouses.stop",
        "tags" : [ "SQL Warehouses" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "id",
          "schema" : {
            "type" : "string"
          },
          "description" : "Required. Id of the SQL warehouse."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.StopWarehouseResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Stops a SQL warehouse.",
        "x-databricks-wait" : {
          "poll" : "get",
          "bind" : "id",
          "field" : [ "state" ],
          "success" : [ "STOPPED" ],
          "message" : [ "health", "summary" ]
        }
      }
    },
    "/api/2.0/token-management/on-behalf-of/tokens" : {
      "post" : {
        "summary" : "Create on-behalf token",
        "operationId" : "TokenManagement.createOboToken",
        "tags" : [ "Token management" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/tokenmanagement.CreateOboTokenResponse"
                }
              }
            },
            "description" : "An on-behalf token was successfully created for the service principal."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/tokenmanagement.CreateOboTokenRequest"
              }
            }
          },
          "description" : "Configuration details for creating on-behalf tokens."
        },
        "description" : "Creates a token on behalf of a service principal."
      },
      "description" : "This endpoint enables admins to create tokens on behalf of service principals.",
      "x-databricks-cloud" : "aws"
    },
    "/api/2.0/token-management/tokens" : {
      "get" : {
        "summary" : "List all tokens",
        "operationId" : "TokenManagement.list",
        "tags" : [ "Token management" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "created_by_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "User ID of the user that created the token."
        }, {
          "in" : "query",
          "name" : "created_by_username",
          "schema" : {
            "type" : "string"
          },
          "description" : "Username of the user that created the token."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/tokenmanagement.ListTokensResponse"
                }
              }
            },
            "description" : "Tokens were successfully returned."
          }
        },
        "description" : "Lists all tokens associated with the specified workspace or user.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "token_infos"
        }
      }
    },
    "/api/2.0/token-management/tokens/{token_id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "token_id",
        "schema" : {
          "type" : "string"
        },
        "description" : "The ID of the token to get."
      } ],
      "get" : {
        "summary" : "Get token info",
        "operationId" : "TokenManagement.get",
        "tags" : [ "Token management" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/tokenmanagement.TokenInfo"
                }
              }
            },
            "description" : "Token with specified Token ID was successfully returned."
          }
        },
        "description" : "Gets information about a token, specified by its ID.",
        "x-databricks-crud" : "read"
      },
      "delete" : {
        "summary" : "Delete a token",
        "operationId" : "TokenManagement.delete",
        "tags" : [ "Token management" ],
        "responses" : {
          "200" : {
            "description" : "The token was successfully deleted."
          }
        },
        "description" : "Deletes a token, specified by its ID.",
        "x-databricks-crud" : "delete"
      },
      "description" : "This endpoint enables admins to get a specific token."
    },
    "/api/2.0/token/create" : {
      "post" : {
        "summary" : "Create a user token",
        "operationId" : "Tokens.create",
        "tags" : [ "Token" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/tokens.CreateTokenResponse"
                },
                "example" : {
                  "token_info" : {
                    "comment" : "This is an example token",
                    "creation_time" : 1626286601651,
                    "expiry_time" : 1634062601651,
                    "token_id" : "1234567890a12bc3456de789012f34ab56c78d9012e3fabc4de56f7a89b012c3"
                  },
                  "token_value" : "dapi1a2b3c45d67890e1f234567a8bc9012d"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/tokens.CreateTokenRequest"
              },
              "example" : {
                "comment" : "This is an example token",
                "lifetime_seconds" : 7776000
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates and returns a token for a user. If this call is made through token authentication, it creates\na token with the same client ID as the authenticated token. If the user's token quota is exceeded, this call\nreturns an error **QUOTA_EXCEEDED**.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.0/token/delete" : {
      "post" : {
        "summary" : "Revoke token",
        "operationId" : "Tokens.revokeToken",
        "tags" : [ "Token" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/tokens.RevokeTokenResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/tokens.RevokeTokenRequest"
              },
              "example" : {
                "token_id" : "1234567890a12bc3456de789012f34ab56c78d9012e3fabc4de56f7a89b012c3"
              }
            }
          },
          "description" : ""
        },
        "description" : "Revokes an access token.\n\nIf a token with the specified ID is not valid, this call returns an error **RESOURCE_DOES_NOT_EXIST**.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/token/list" : {
      "get" : {
        "summary" : "List tokens",
        "operationId" : "Tokens.listTokens",
        "tags" : [ "Token" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/tokens.ListTokensResponse"
                },
                "example" : {
                  "token_infos" : [ {
                    "comment" : "This is an example token",
                    "creation_time" : 1626286601651,
                    "expiry_time" : 1634062601651,
                    "token_id" : "1234567890a12bc3456de789012f34ab56c78d9012e3fabc4de56f7a89b012c3"
                  }, {
                    "comment" : "This is another example token",
                    "creation_time" : 1626286906596,
                    "expiry_time" : 1634062906596,
                    "token_id" : "2345678901a12bc3456de789012f34ab56c78d9012e3fabc4de56f7a89b012c4"
                  } ]
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Lists all the valid tokens for a user-workspace pair.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "token_infos"
        }
      }
    },
    "/api/2.0/workspace-conf" : {
      "get" : {
        "summary" : "Check configuration status",
        "operationId" : "WorkspaceConf.getStatus",
        "tags" : [ "Workspace Conf" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "keys",
          "schema" : {
            "type" : "string"
          }
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/workspaceconf.WorkspaceConf"
                }
              }
            },
            "description" : "Status was returned successfully."
          }
        },
        "description" : "Gets the configuration status for a workspace.",
        "x-databricks-crud" : "read",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_workspace_conf\" \"this\" {\n  custom_config = {\n    \"enableIpAccessLists\" : true\n  }\n}\n"
        } ]
      },
      "patch" : {
        "summary" : "Enable/disable features",
        "operationId" : "WorkspaceConf.setStatus",
        "tags" : [ "Workspace Conf" ],
        "responses" : {
          "204" : {
            "description" : "Enabling or disabling feature was successful."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/workspaceconf.WorkspaceConf"
              }
            }
          },
          "description" : ""
        },
        "description" : "Sets the configuration status for a workspace, including enabling or disabling it.",
        "x-databricks-crud" : "update"
      },
      "description" : "Enables or disables a specified feature for a workspace."
    },
    "/api/2.0/workspace/delete" : {
      "post" : {
        "summary" : "Delete a workspace object",
        "operationId" : "Workspace.delete",
        "tags" : [ "Workspace" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/workspace.DeleteResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/workspace.Delete"
              },
              "example" : {
                "path" : "/Users/me@example.com/MyFolder",
                "recursive" : true
              }
            }
          },
          "description" : ""
        },
        "description" : "Deletes an object or a directory (and optionally recursively deletes all objects in the directory).\n* If `path` does not exist, this call returns an error `RESOURCE_DOES_NOT_EXIST`.\n* If `path` is a non-empty directory and `recursive` is set to `false`, this call returns an error `DIRECTORY_NOT_EMPTY`.\n\nObject deletion cannot be undone and deleting a directory recursively is not atomic.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.0/workspace/export" : {
      "get" : {
        "summary" : "Export a notebook",
        "operationId" : "Workspace.export",
        "tags" : [ "Workspace" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "path",
          "schema" : {
            "type" : "string"
          },
          "description" : "The absolute path of the notebook or directory. Exporting directory is only support for `DBC` format."
        }, {
          "in" : "query",
          "name" : "format",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/workspace.ExportFormat"
          },
          "description" : "This specifies the format of the exported file. By default, this is `SOURCE`.\nHowever it may be one of: `SOURCE`, `HTML`, `JUPYTER`, `DBC`.\n\nThe value is case sensitive.\n"
        }, {
          "in" : "query",
          "name" : "direct_download",
          "schema" : {
            "type" : "boolean"
          },
          "description" : "Flag to enable direct download. If it is `true`, the response will be the exported file itself.\nOtherwise, the response contains content as base64 encoded string.\n"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/workspace.ExportResponse"
                },
                "example" : {
                  "content" : "Ly8gRGF0YWJyaWNrcyBub3RlYm9vayBzb3VyY2UKMSsx"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Exports a notebook or the contents of an entire directory.\n\nIf `path` does not exist, this call returns an error `RESOURCE_DOES_NOT_EXIST`.\n\nOne can only export a directory in `DBC` format.\nIf the exported data would exceed size limit, this call returns `MAX_NOTEBOOK_SIZE_EXCEEDED`.\nCurrently, this API does not support exporting a library.\n"
      }
    },
    "/api/2.0/workspace/get-status" : {
      "get" : {
        "summary" : "Get status",
        "operationId" : "Workspace.getStatus",
        "tags" : [ "Workspace" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "path",
          "schema" : {
            "type" : "string"
          },
          "description" : "The absolute path of the notebook or directory."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/workspace.ObjectInfo"
                },
                "example" : {
                  "language" : "SCALA",
                  "object_id" : 789,
                  "object_type" : "NOTEBOOK",
                  "path" : "/Users/user@example.com/project/ScalaExampleNotebook"
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Gets the status of an object or a directory.\nIf `path` does not exist, this call returns an error `RESOURCE_DOES_NOT_EXIST`.\n",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.0/workspace/import" : {
      "post" : {
        "summary" : "Import a notebook",
        "operationId" : "Workspace.import",
        "tags" : [ "Workspace" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/workspace.ImportResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/workspace.Import"
              },
              "example" : {
                "format" : "SOURCE",
                "path" : "/Users/user@example.com/project/ScalaExampleNotebook",
                "language" : "SCALA",
                "content" : "MSsx",
                "overwrite" : true
              }
            }
          },
          "description" : ""
        },
        "description" : "Imports a notebook or the contents of an entire directory.\nIf `path` already exists and `overwrite` is set to `false`, this call returns an error `RESOURCE_ALREADY_EXISTS`.\nOne can only use `DBC` format to import a directory.\n",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_notebook\" \"notebook\" {\n  content_base64 = base64encode(<<-EOT\n    # created from ${abspath(path.module)}\n    display(spark.range(10))\n    EOT\n  )\n  path     = \"/Shared/Demo\"\n  language = \"PYTHON\"\n}\n"
        } ]
      }
    },
    "/api/2.0/workspace/list" : {
      "get" : {
        "summary" : "List contents",
        "operationId" : "Workspace.list",
        "tags" : [ "Workspace" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "path",
          "schema" : {
            "type" : "string"
          },
          "description" : "The absolute path of the notebook or directory."
        }, {
          "in" : "query",
          "name" : "notebooks_modified_after",
          "schema" : {
            "type" : "integer"
          },
          "description" : "<content needed>"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/workspace.ListResponse"
                },
                "example" : {
                  "objects" : [ {
                    "object_id" : 123,
                    "object_type" : "DIRECTORY",
                    "path" : "/Users/user@example.com/project"
                  }, {
                    "language" : "PYTHON",
                    "object_id" : 456,
                    "object_type" : "NOTEBOOK",
                    "path" : "/Users/user@example.com/PythonExampleNotebook"
                  } ]
                }
              }
            },
            "description" : ""
          }
        },
        "description" : "Lists the contents of a directory, or the object if it is not a directory.If\nthe input path does not exist, this call returns an error `RESOURCE_DOES_NOT_EXIST`.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "objects"
        }
      }
    },
    "/api/2.0/workspace/mkdirs" : {
      "post" : {
        "summary" : "Create a directory",
        "operationId" : "Workspace.mkdirs",
        "tags" : [ "Workspace" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/workspace.MkdirsResponse"
                }
              }
            },
            "description" : ""
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/workspace.Mkdirs"
              },
              "example" : {
                "path" : "/Users/user@example.com/project"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates the specified directory (and necessary parent directories if they do not exist). \nIf there is an object (not a directory) at any prefix of the input path, this call returns \nan error `RESOURCE_ALREADY_EXISTS`.\n\nNote that if this operation fails it may have succeeded in creating some of the necessary\\nparrent directories.\n"
      }
    },
    "/api/2.1/jobs/create" : {
      "post" : {
        "summary" : "Create a new job",
        "operationId" : "Jobs.create",
        "tags" : [ "Jobs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "job_id" : {
                      "example" : 11223344,
                      "format" : "int64",
                      "description" : "The canonical identifier for the newly created job.",
                      "type" : "integer"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Job was created successfully"
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/jobs.CreateJob"
              }
            }
          },
          "description" : ""
        },
        "description" : "Create a new job.",
        "x-databricks-crud" : "create",
        "x-codeSamples" : [ {
          "lang" : "Terraform",
          "label" : "Terraform",
          "source" : "resource \"databricks_job\" \"this\" {\n  name = \"Job with multiple tasks\"\n\n  job_cluster {\n    job_cluster_key = \"j\"\n    new_cluster {\n      num_workers   = 2\n      spark_version = data.databricks_spark_version.latest.id\n      node_type_id  = data.databricks_node_type.smallest.id\n    }\n  }\n\n  task {\n    task_key = \"a\"\n\n    new_cluster {\n      num_workers   = 1\n      spark_version = data.databricks_spark_version.latest.id\n      node_type_id  = data.databricks_node_type.smallest.id\n    }\n\n    notebook_task {\n      notebook_path = databricks_notebook.this.path\n    }\n  }\n\n  task {\n    task_key = \"b\"\n    //this task will only run after task a\n    depends_on {\n      task_key = \"a\"\n    }\n\n    existing_cluster_id = databricks_cluster.shared.id\n\n    spark_jar_task {\n      main_class_name = \"com.acme.data.Main\"\n    }\n  }\n\n  task {\n    task_key = \"c\"\n\n    job_cluster_key = \"j\"\n\n    notebook_task {\n      notebook_path = databricks_notebook.this.path\n    }\n  }\n  //this task starts a Delta Live Tables pipline update\n  task {\n    task_key = \"d\"\n\n    pipeline_task {\n      pipeline_id = databricks_pipeline.this.id\n    }\n  }\n}\n"
        } ]
      }
    },
    "/api/2.1/jobs/delete" : {
      "post" : {
        "summary" : "Delete a job",
        "operationId" : "Jobs.delete",
        "tags" : [ "Jobs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Job was deleted successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/jobs.DeleteJob"
              }
            }
          },
          "description" : ""
        },
        "description" : "Deletes a job.",
        "x-databricks-crud" : "delete",
        "x-databricks-shortcut" : true
      }
    },
    "/api/2.1/jobs/get" : {
      "get" : {
        "summary" : "Get a single job",
        "operationId" : "Jobs.get",
        "tags" : [ "Jobs" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "job_id",
          "schema" : {
            "example" : 11223344,
            "format" : "int64",
            "type" : "integer"
          },
          "description" : "The canonical identifier of the job to retrieve information about. This field is required."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/jobs.Job"
                }
              }
            },
            "description" : "Job was retrieved successfully."
          }
        },
        "description" : "Retrieves the details for a single job.",
        "x-databricks-crud" : "read",
        "x-databricks-shortcut" : true
      }
    },
    "/api/2.1/jobs/list" : {
      "get" : {
        "summary" : "List all jobs",
        "operationId" : "Jobs.list",
        "tags" : [ "Jobs" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "limit",
          "schema" : {
            "default" : 20,
            "example" : 25,
            "maximum" : 25,
            "minimum" : 1,
            "type" : "integer"
          },
          "description" : "The number of jobs to return. This value must be greater than 0 and less or equal to 25. The default value is 20."
        }, {
          "in" : "query",
          "name" : "offset",
          "schema" : {
            "default" : 0,
            "example" : 0,
            "type" : "integer"
          },
          "description" : "The offset of the first job to return, relative to the most recently created job."
        }, {
          "in" : "query",
          "name" : "name",
          "schema" : {
            "example" : "A%20multitask%20job",
            "type" : "string"
          },
          "description" : "A filter on the list based on the exact (case insensitive) job name."
        }, {
          "in" : "query",
          "name" : "expand_tasks",
          "schema" : {
            "default" : false,
            "example" : false,
            "type" : "boolean"
          },
          "description" : "Whether to include task and cluster details in the response."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/jobs.ListJobsResponse"
                }
              }
            },
            "description" : "List of jobs was retrieved successfully."
          }
        },
        "description" : "Retrieves a list of jobs.",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "offset" : "offset",
          "limit" : "limit",
          "results" : "jobs",
          "increment" : 25
        }
      }
    },
    "/api/2.1/jobs/reset" : {
      "post" : {
        "summary" : "Overwrites all settings for a job",
        "operationId" : "Jobs.reset",
        "tags" : [ "Jobs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Job was overwritten successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/jobs.ResetJob"
              }
            }
          },
          "description" : ""
        },
        "description" : "Overwrites all the settings for a specific job. Use the Update endpoint to update job settings partially.",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.1/jobs/run-now" : {
      "post" : {
        "summary" : "Trigger a new job run",
        "operationId" : "Jobs.runNow",
        "tags" : [ "Jobs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/jobs.RunNowResponse"
                }
              }
            },
            "description" : "Run was started successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/jobs.RunNow"
              }
            }
          },
          "description" : ""
        },
        "description" : "Run a job and return the `run_id` of the triggered run.",
        "x-databricks-shortcut" : true,
        "x-databricks-wait" : {
          "poll" : "getRun",
          "bind" : "run_id",
          "field" : [ "state", "life_cycle_state" ],
          "success" : [ "TERMINATED", "SKIPPED" ],
          "failure" : [ "INTERNAL_ERROR" ],
          "message" : [ "state", "state_message" ]
        }
      }
    },
    "/api/2.1/jobs/runs/cancel" : {
      "post" : {
        "summary" : "Cancel a job run",
        "operationId" : "Jobs.cancelRun",
        "tags" : [ "Jobs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Run was cancelled successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/jobs.CancelRun"
              }
            }
          },
          "description" : ""
        },
        "description" : "Cancels a job run. The run is canceled asynchronously, so it may still be running when\nthis request completes.\n",
        "x-databricks-shortcut" : true,
        "x-databricks-wait" : {
          "poll" : "getRun",
          "bind" : "run_id",
          "field" : [ "state", "life_cycle_state" ],
          "success" : [ "TERMINATED", "SKIPPED" ],
          "failure" : [ "INTERNAL_ERROR" ],
          "message" : [ "state", "state_message" ]
        }
      }
    },
    "/api/2.1/jobs/runs/cancel-all" : {
      "post" : {
        "summary" : "Cancel all runs of a job",
        "operationId" : "Jobs.cancelAllRuns",
        "tags" : [ "Jobs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "All runs were cancelled successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/jobs.CancelAllRuns"
              }
            }
          },
          "description" : ""
        },
        "description" : "Cancels all active runs of a job. The runs are canceled asynchronously, so it doesn't\nprevent new runs from being started.\n",
        "x-databricks-shortcut" : true
      }
    },
    "/api/2.1/jobs/runs/delete" : {
      "post" : {
        "summary" : "Delete a job run",
        "operationId" : "Jobs.deleteRun",
        "tags" : [ "Jobs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Run was deleted successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/jobs.DeleteRun"
              }
            }
          },
          "description" : ""
        },
        "description" : "Deletes a non-active run. Returns an error if the run is active.",
        "x-databricks-shortcut" : true
      }
    },
    "/api/2.1/jobs/runs/export" : {
      "get" : {
        "summary" : "Export and retrieve a job run",
        "operationId" : "Jobs.exportRun",
        "tags" : [ "Jobs" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "run_id",
          "schema" : {
            "example" : 455644833,
            "format" : "int64",
            "type" : "integer"
          },
          "description" : "The canonical identifier for the run. This field is required."
        }, {
          "in" : "query",
          "name" : "views_to_export",
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.ViewsToExport"
          },
          "description" : "Which views to export (CODE, DASHBOARDS, or ALL). Defaults to CODE."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/jobs.ExportRunOutput"
                }
              }
            },
            "description" : "Run was exported successfully."
          }
        },
        "description" : "Export and retrieve the job run task."
      }
    },
    "/api/2.1/jobs/runs/get" : {
      "get" : {
        "summary" : "Get a single job run",
        "operationId" : "Jobs.getRun",
        "tags" : [ "Jobs" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "run_id",
          "schema" : {
            "example" : 455644833,
            "format" : "int64",
            "type" : "integer"
          },
          "description" : "The canonical identifier of the run for which to retrieve the metadata.\nThis field is required.\n"
        }, {
          "in" : "query",
          "name" : "include_history",
          "schema" : {
            "example" : true,
            "type" : "boolean"
          },
          "description" : "Whether to include the repair history in the response."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/jobs.Run"
                }
              }
            },
            "description" : "Run was retrieved successfully"
          }
        },
        "description" : "Retrieve the metadata of a run.",
        "x-databricks-shortcut" : true,
        "x-databricks-wait" : {
          "poll" : "getRun",
          "bind" : "run_id",
          "field" : [ "state", "life_cycle_state" ],
          "success" : [ "TERMINATED", "SKIPPED" ],
          "failure" : [ "INTERNAL_ERROR" ],
          "message" : [ "state", "state_message" ]
        }
      }
    },
    "/api/2.1/jobs/runs/get-output" : {
      "get" : {
        "summary" : "Get the output for a single run",
        "operationId" : "Jobs.getRunOutput",
        "tags" : [ "Jobs" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "run_id",
          "schema" : {
            "example" : 455644833,
            "format" : "int64",
            "type" : "integer"
          },
          "description" : "The canonical identifier for the run. This field is required."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/jobs.RunOutput"
                }
              }
            },
            "description" : "Run output was retrieved successfully."
          }
        },
        "description" : "Retrieve the output and metadata of a single task run. When a notebook task returns\na value through the `dbutils.notebook.exit()` call, you can use this endpoint to retrieve\nthat value. Databricks restricts this API to returning the first 5 MB of the output.\nTo return a larger result, you can store job results in a cloud storage service.\n\nThis endpoint validates that the __run_id__ parameter is valid and returns an HTTP status\ncode 400 if the __run_id__ parameter is invalid. Runs are automatically removed after\n60 days. If you to want to reference them beyond 60 days, you must save old run results\nbefore they expire.\n",
        "x-databricks-shortcut" : true
      }
    },
    "/api/2.1/jobs/runs/list" : {
      "get" : {
        "summary" : "List runs for a job",
        "operationId" : "Jobs.listRuns",
        "tags" : [ "Jobs" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "active_only",
          "schema" : {
            "default" : false,
            "example" : false,
            "type" : "boolean"
          },
          "description" : "If active_only is `true`, only active runs are included in the results; otherwise,\nlists both active and completed runs. An active run is a run in the `PENDING`,\n`RUNNING`, or `TERMINATING`. This field cannot be `true` when completed_only is `true`.\n"
        }, {
          "in" : "query",
          "name" : "completed_only",
          "schema" : {
            "default" : false,
            "example" : false,
            "type" : "boolean"
          },
          "description" : "If completed_only is `true`, only completed runs are included in the results;\notherwise, lists both active and completed runs. This field cannot be `true` when\nactive_only is `true`.\n"
        }, {
          "in" : "query",
          "name" : "job_id",
          "schema" : {
            "example" : 11223344,
            "format" : "int64",
            "type" : "integer"
          },
          "description" : "The job for which to list runs. If omitted, the Jobs service lists runs from all jobs."
        }, {
          "in" : "query",
          "name" : "offset",
          "schema" : {
            "default" : 0,
            "example" : 0,
            "format" : "int32",
            "type" : "integer"
          },
          "description" : "The offset of the first run to return, relative to the most recent run."
        }, {
          "in" : "query",
          "name" : "limit",
          "schema" : {
            "default" : 25,
            "example" : 25,
            "maximum" : 25,
            "minimum" : 1,
            "format" : "int32",
            "type" : "integer"
          },
          "description" : "The number of runs to return. This value must be greater than 0 and less than 25.\nThe default value is 25. If a request specifies a limit of 0, the service instead\nuses the maximum limit.\n"
        }, {
          "in" : "query",
          "name" : "run_type",
          "schema" : {
            "example" : "JOB_RUN",
            "description" : "This describes an enum",
            "type" : "string",
            "enum" : [ "JOB_RUN", "WORKFLOW_RUN", "SUBMIT_RUN" ],
            "x-databricks-enum-descriptions" : {
              "JOB_RUN" : "Normal job run. A run created with :method:jobs/runNow.",
              "WORKFLOW_RUN" : "Workflow run. A run created with [dbutils.notebook.run](/dev-tools/databricks-utils.html#dbutils-workflow).",
              "SUBMIT_RUN" : "Submit run. A run created with :method:jobs/submit."
            }
          },
          "description" : "The type of runs to return. For a description of run types, see :method:jobs/getRun."
        }, {
          "in" : "query",
          "name" : "expand_tasks",
          "schema" : {
            "default" : false,
            "example" : false,
            "type" : "boolean"
          },
          "description" : "Whether to include task and cluster details in the response."
        }, {
          "in" : "query",
          "name" : "start_time_from",
          "schema" : {
            "example" : 1642521600000,
            "type" : "integer"
          },
          "description" : "Show runs that started _at or after_ this value. The value must be a UTC timestamp\nin milliseconds. Can be combined with _start_time_to_ to filter by a time range.\n"
        }, {
          "in" : "query",
          "name" : "start_time_to",
          "schema" : {
            "example" : 1642608000000,
            "type" : "integer"
          },
          "description" : "Show runs that started _at or before_ this value. The value must be a UTC timestamp\nin milliseconds. Can be combined with _start_time_from_ to filter by a time range.\n"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/jobs.ListRunsResponse"
                }
              }
            },
            "description" : "List of runs was retrieved successfully."
          }
        },
        "description" : "List runs in descending order by start time.",
        "x-databricks-pagination" : {
          "offset" : "offset",
          "limit" : "limit",
          "results" : "runs",
          "increment" : 25
        }
      }
    },
    "/api/2.1/jobs/runs/repair" : {
      "post" : {
        "summary" : "Repair a job run",
        "operationId" : "Jobs.repairRun",
        "tags" : [ "Jobs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : {
                    "repair_id" : {
                      "example" : 734650698524280,
                      "format" : "int64",
                      "description" : "The ID of the repair.",
                      "type" : "integer"
                    }
                  },
                  "type" : "object"
                }
              }
            },
            "description" : "Run repair was initiated."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/jobs.RepairRun"
              }
            }
          },
          "description" : ""
        },
        "description" : "Re-run one or more tasks. Tasks are re-run as part of the original job run.\nThey use the current job and task settings, and can be viewed in the history for the\noriginal job run.\n",
        "x-databricks-wait" : {
          "poll" : "getRun",
          "binding" : {
            "run_id" : {
              "request" : "run_id"
            }
          },
          "field" : [ "state", "life_cycle_state" ],
          "success" : [ "TERMINATED", "SKIPPED" ],
          "failure" : [ "INTERNAL_ERROR" ],
          "message" : [ "state", "state_message" ]
        }
      }
    },
    "/api/2.1/jobs/runs/submit" : {
      "post" : {
        "summary" : "Create and trigger a one-time run",
        "operationId" : "Jobs.submit",
        "tags" : [ "Jobs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/jobs.SubmitRunResponse"
                }
              }
            },
            "description" : "Run was created and started successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/jobs.SubmitRun"
              }
            }
          },
          "description" : ""
        },
        "description" : "Submit a one-time run. This endpoint allows you to submit a workload directly without\ncreating a job. Runs submitted using this endpoint dont display in the UI. Use the\n`jobs/runs/get` API to check the run state after the job is submitted.\n",
        "x-databricks-wait" : {
          "poll" : "getRun",
          "bind" : "run_id",
          "field" : [ "state", "life_cycle_state" ],
          "success" : [ "TERMINATED", "SKIPPED" ],
          "failure" : [ "INTERNAL_ERROR" ],
          "message" : [ "state", "state_message" ]
        }
      }
    },
    "/api/2.1/jobs/update" : {
      "post" : {
        "summary" : "Partially updates a job",
        "operationId" : "Jobs.update",
        "tags" : [ "Jobs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "Job was updated successfully."
          }
        },
        "requestBody" : {
          "required" : true,
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/jobs.UpdateJob"
              }
            }
          },
          "description" : ""
        },
        "description" : "Add, update, or remove specific settings of an existing job. Use the ResetJob to overwrite all job settings.",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.1/unity-catalog/catalogs" : {
      "get" : {
        "summary" : "List catalogs",
        "operationId" : "Catalogs.list",
        "tags" : [ "Catalogs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListCatalogsResponse"
                }
              }
            },
            "description" : "The catalog list was successfully retrieved."
          }
        },
        "description" : "Gets an array of catalogs in the metastore.\nIf the caller is the metastore admin, all catalogs will be retrieved.\nOtherwise, only catalogs owned by the caller (or for which the caller has the **USE_CATALOG** privilege) will be retrieved.\nThere is no guarantee of a specific ordering of the elements in the array.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "catalogs"
        }
      },
      "post" : {
        "summary" : "Create a catalog",
        "operationId" : "Catalogs.create",
        "tags" : [ "Catalogs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.CatalogInfo"
                }
              }
            },
            "description" : "The new catalog was successfully created."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateCatalog"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new catalog instance in the parent metastore if the caller is a metastore admin or has the **CREATE_CATALOG** privilege.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.1/unity-catalog/catalogs/{name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "The name of the catalog."
      } ],
      "get" : {
        "summary" : "Get a catalog",
        "operationId" : "Catalogs.get",
        "tags" : [ "Catalogs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.CatalogInfo"
                }
              }
            },
            "description" : "The catalog was successfully retrieved."
          }
        },
        "description" : "Gets the specified catalog in a metastore.\nThe caller must be a metastore admin, the owner of the catalog, or a user that has the **USE_CATALOG** privilege set for their account.\n",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update a catalog",
        "operationId" : "Catalogs.update",
        "tags" : [ "Catalogs" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.CatalogInfo"
                }
              }
            },
            "description" : "The catalog was successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateCatalog"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the catalog that matches the supplied name.\nThe caller must be either the owner of the catalog, or a metastore admin (when changing the owner field of the catalog).\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a catalog",
        "operationId" : "Catalogs.delete",
        "tags" : [ "Catalogs" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "force",
          "schema" : {
            "type" : "boolean"
          },
          "description" : "Force deletion even if the catalog is not empty."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The catalog was successfully deleted."
          }
        },
        "description" : "Deletes the catalog that matches the supplied name. The caller must be a metastore admin or the owner of the catalog.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.1/unity-catalog/constraints" : {
      "post" : {
        "summary" : "Create a table constraint",
        "operationId" : "TableConstraints.create",
        "tags" : [ "Table Constraints" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.TableConstraint"
                }
              }
            },
            "description" : "The new table constraint was successfully created."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateTableConstraint"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new table constraint.\n\nFor the table constraint creation to succeed, the user must satisfy both of these conditions:\n- the user must have the **USE_CATALOG** privilege on the table's parent catalog,\n  the **USE_SCHEMA** privilege on the table's parent schema, and be the owner of the table.\n- if the new constraint is a __ForeignKeyConstraint__,\n  the user must have the **USE_CATALOG** privilege on the referenced parent table's catalog,\n  the **USE_SCHEMA** privilege on the referenced parent table's schema,\n  and be the owner of the referenced parent table.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.1/unity-catalog/constraints/{full_name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "full_name",
        "schema" : {
          "type" : "string"
        },
        "description" : "Full name of the table referenced by the constraint."
      } ],
      "delete" : {
        "summary" : "Delete a table constraint",
        "operationId" : "TableConstraints.delete",
        "tags" : [ "Table Constraints" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "constraint_name",
          "schema" : {
            "type" : "string"
          },
          "description" : "The name of the constraint to delete."
        }, {
          "required" : true,
          "in" : "query",
          "name" : "cascade",
          "schema" : {
            "default" : "false",
            "type" : "boolean"
          },
          "description" : "If true, try deleting all child constraints of the current constraint.\\n\nIf false, reject this operation if the current constraint has any child constraints.\n"
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The table constraint was successfully deleted."
          }
        },
        "description" : "Deletes a table constraint.\n\nFor the table constraint deletion to succeed, the user must satisfy both of these conditions:\n- the user must have the **USE_CATALOG** privilege on the table's parent catalog,\n  the **USE_SCHEMA** privilege on the table's parent schema, and be the owner of the table.\n- if __cascade__ argument is **true**, the user must have the following permissions on all of the child tables:\n  the **USE_CATALOG** privilege on the table's catalog,\n  the **USE_SCHEMA** privilege on the table's schema,\n  and be the owner of the table.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.1/unity-catalog/current-metastore-assignment" : {
      "get" : {
        "summary" : "Get metastore assignment for workspace",
        "operationId" : "Metastores.current",
        "tags" : [ "Metastores" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.MetastoreAssignment"
                }
              }
            },
            "description" : "The metastore assignment was successfully retrieved."
          }
        },
        "description" : "Gets the metastore assignment for the workspace being accessed.\n",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.1/unity-catalog/effective-permissions/{securable_type}/{full_name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "securable_type",
        "schema" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/unitycatalog.SecurableType"
        },
        "description" : "Type of securable."
      }, {
        "required" : true,
        "in" : "path",
        "name" : "full_name",
        "schema" : {
          "x-databricks-name" : true,
          "type" : "string"
        },
        "description" : "Full name of securable."
      } ],
      "get" : {
        "summary" : "Get effective permissions",
        "operationId" : "Grants.getEffective",
        "tags" : [ "Grants" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "principal",
          "schema" : {
            "type" : "string"
          },
          "description" : "If provided, only the effective permissions for the specified principal (user or group) are returned."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.EffectivePermissionsList"
                }
              }
            },
            "description" : "The effective permissions list for securable was successfully retrieved."
          }
        },
        "description" : "Gets the effective permissions for a securable.",
        "x-databricks-crud" : "read"
      }
    },
    "/api/2.1/unity-catalog/external-locations" : {
      "get" : {
        "summary" : "List external locations",
        "operationId" : "ExternalLocations.list",
        "tags" : [ "External Locations" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListExternalLocationsResponse"
                }
              }
            },
            "description" : "The external location list was successfully retrieved."
          }
        },
        "description" : "Gets an array of external locations (__ExternalLocationInfo__ objects) from the metastore.\nThe caller must be a metastore admin, the owner of the external location, or a user that has some privilege on the external location.\nThere is no guarantee of a specific ordering of the elements in the array.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "external_locations"
        }
      },
      "post" : {
        "summary" : "Create an external location",
        "operationId" : "ExternalLocations.create",
        "tags" : [ "External Locations" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ExternalLocationInfo"
                }
              }
            },
            "description" : "The new external location was successfully created."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateExternalLocation"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new external location entry in the metastore.\nThe caller must be a metastore admin or have the **CREATE_EXTERNAL_LOCATION** privilege on both the metastore and the associated storage credential.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.1/unity-catalog/external-locations/{name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "Name of the external location."
      } ],
      "get" : {
        "summary" : "Get an external location",
        "operationId" : "ExternalLocations.get",
        "tags" : [ "External Locations" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ExternalLocationInfo"
                }
              }
            },
            "description" : "The external location was successfully retrieved."
          }
        },
        "description" : "Gets an external location from the metastore.\nThe caller must be either a metastore admin, the owner of the external location, or a user that has some privilege on the external location.\n",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update an external location",
        "operationId" : "ExternalLocations.update",
        "tags" : [ "External Locations" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ExternalLocationInfo"
                }
              }
            },
            "description" : "The external location was successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateExternalLocation"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates an external location in the metastore. The caller must be the owner of the external location, or be a metastore admin.\nIn the second case, the admin can only update the name of the external location.\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete an external location",
        "operationId" : "ExternalLocations.delete",
        "tags" : [ "External Locations" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "force",
          "schema" : {
            "type" : "boolean"
          },
          "description" : "Force deletion even if there are dependent external tables or mounts."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The external location was successfully deleted."
          }
        },
        "description" : "Deletes the specified external location from the metastore. The caller must be the owner of the external location.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.1/unity-catalog/functions" : {
      "get" : {
        "summary" : "List functions",
        "operationId" : "Functions.list",
        "tags" : [ "Functions" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "catalog_name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of parent catalog for functions of interest."
        }, {
          "required" : true,
          "in" : "query",
          "name" : "schema_name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Parent schema of functions."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListFunctionsResponse"
                }
              }
            },
            "description" : "The function list was successfully retrieved."
          }
        },
        "description" : "List functions within the specified parent catalog and schema.\nIf the user is a metastore admin, all functions are returned in the output list.\nOtherwise, the user must have the **USE_CATALOG** privilege on the catalog and the **USE_SCHEMA** privilege on the schema, and the output list contains only functions for which either the user has the **EXECUTE** privilege or the user is the owner.\nThere is no guarantee of a specific ordering of the elements in the array.\n",
        "x-databricks-crud" : "list"
      },
      "post" : {
        "summary" : "Create a function",
        "operationId" : "Functions.create",
        "tags" : [ "Functions" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.FunctionInfo"
                }
              }
            },
            "description" : "The new function was successfully created."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateFunction"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new function\n\nThe user must have the following permissions in order for the function to be created:\n- **USE_CATALOG** on the function's parent catalog\n- **USE_SCHEMA** and **CREATE_FUNCTION** on the function's parent schema\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.1/unity-catalog/functions/{name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "The fully-qualified name of the function (of the form __catalog_name__.__schema_name__.__function__name__)."
      } ],
      "get" : {
        "summary" : "Get a function",
        "operationId" : "Functions.get",
        "tags" : [ "Functions" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.FunctionInfo"
                }
              }
            },
            "description" : "The function was successfully retrieved."
          }
        },
        "description" : "Gets a function from within a parent catalog and schema.\nFor the fetch to succeed, the user must satisfy one of the following requirements:\n- Is a metastore admin\n- Is an owner of the function's parent catalog\n- Have the **USE_CATALOG** privilege on the function's parent catalog and be the owner of the function\n- Have the **USE_CATALOG** privilege on the function's parent catalog, the **USE_SCHEMA** privilege on the function's parent schema, and the **EXECUTE** privilege on the function itself\n",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update a function",
        "operationId" : "Functions.update",
        "tags" : [ "Functions" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.FunctionInfo"
                }
              }
            },
            "description" : "The function was successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateFunction"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the function that matches the supplied name.\nOnly the owner of the function can be updated. If the user is not a metastore admin, the user must be a member of the group that is the new function owner.\n- Is a metastore admin\n- Is the owner of the function's parent catalog\n- Is the owner of the function's parent schema and has the **USE_CATALOG** privilege on its parent catalog\n- Is the owner of the function itself and has the **USE_CATALOG** privilege on its parent catalog as well as the **USE_SCHEMA** privilege on the function's parent schema.\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a function",
        "operationId" : "Functions.delete",
        "tags" : [ "Functions" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "force",
          "schema" : {
            "type" : "boolean"
          },
          "description" : "Force deletion even if the function is notempty."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The function was successfully deleted."
          }
        },
        "description" : "Deletes the function that matches the supplied name.\nFor the deletion to succeed, the user must satisfy one of the following conditions:\n- Is the owner of the function's parent catalog\n- Is the owner of the function's parent schema and have the **USE_CATALOG** privilege on its parent catalog\n- Is the owner of the function itself and have both the **USE_CATALOG** privilege on its parent catalog and the **USE_SCHEMA** privilege on its parent schema\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.1/unity-catalog/metastore_summary" : {
      "get" : {
        "summary" : "Get a metastore summary",
        "operationId" : "Metastores.summary",
        "tags" : [ "Metastores" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.GetMetastoreSummaryResponse"
                }
              }
            },
            "description" : "The metastore summary was successfully retrieved."
          }
        },
        "description" : "Gets information about a metastore.\nThis summary includes the storage credential, the cloud vendor, the cloud region, and the global metastore ID.\n"
      }
    },
    "/api/2.1/unity-catalog/metastores" : {
      "get" : {
        "summary" : "List metastores",
        "operationId" : "Metastores.list",
        "tags" : [ "Metastores" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListMetastoresResponse"
                }
              }
            },
            "description" : "The metastore list was successfully retrieved."
          }
        },
        "description" : "Gets an array of the available metastores (as __MetastoreInfo__ objects). The caller must be an admin to retrieve this info.\nThere is no guarantee of a specific ordering of the elements in the array.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "metastores"
        }
      },
      "post" : {
        "summary" : "Create a metastore",
        "operationId" : "Metastores.create",
        "tags" : [ "Metastores" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.MetastoreInfo"
                }
              }
            },
            "description" : "The new metastore was successfully created."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateMetastore"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new metastore based on a provided name and storage root path.",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.1/unity-catalog/metastores/{id}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "id",
        "schema" : {
          "type" : "string"
        },
        "description" : "Unique ID of the metastore."
      } ],
      "get" : {
        "summary" : "Get a metastore",
        "operationId" : "Metastores.get",
        "tags" : [ "Metastores" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.MetastoreInfo"
                }
              }
            },
            "description" : "The metastore was successfully retrieved."
          }
        },
        "description" : "Gets a metastore that matches the supplied ID. The caller must be a metastore admin to retrieve this info.",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update a metastore",
        "operationId" : "Metastores.update",
        "tags" : [ "Metastores" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.MetastoreInfo"
                }
              }
            },
            "description" : "The metastore was successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateMetastore"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates information for a specific metastore. The caller must be a metastore admin.",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a metastore",
        "operationId" : "Metastores.delete",
        "tags" : [ "Metastores" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "force",
          "schema" : {
            "type" : "boolean"
          },
          "description" : "Force deletion even if the metastore is not empty. Default is false."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The metastore was successfully deleted."
          }
        },
        "description" : "Deletes a metastore. The caller must be a metastore admin.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.1/unity-catalog/permissions/{securable_type}/{full_name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "securable_type",
        "schema" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/unitycatalog.SecurableType"
        },
        "description" : "Type of securable."
      }, {
        "required" : true,
        "in" : "path",
        "name" : "full_name",
        "schema" : {
          "x-databricks-name" : true,
          "type" : "string"
        },
        "description" : "Full name of securable."
      } ],
      "get" : {
        "summary" : "Get permissions",
        "operationId" : "Grants.get",
        "tags" : [ "Grants" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "principal",
          "schema" : {
            "type" : "string"
          },
          "description" : "If provided, only the permissions for the specified principal (user or group) are returned."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.PermissionsList"
                }
              }
            },
            "description" : "The permissions list for securable was successfully retrieved."
          }
        },
        "description" : "Gets the permissions for a securable.",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update permissions",
        "operationId" : "Grants.update",
        "tags" : [ "Grants" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.PermissionsList"
                }
              }
            },
            "description" : "The permissions list for securable was successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdatePermissions"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the permissions for a securable.",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.1/unity-catalog/providers" : {
      "get" : {
        "summary" : "List providers",
        "operationId" : "Providers.list",
        "tags" : [ "Delta Sharing: Providers" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "data_provider_global_metastore_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "If not provided, all providers will be returned.\nIf no providers exist with this ID, no results will be returned."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListProvidersResponse"
                }
              }
            },
            "description" : "The provider list was successfully retrieved."
          }
        },
        "description" : "Gets an array of available authentication providers.\nThe caller must either be a metastore admin or the owner of the providers.\nProviders not owned by the caller are not included in the response.\nThere is no guarantee of a specific ordering of the elements in the array.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "providers"
        }
      },
      "post" : {
        "summary" : "Create an auth provider",
        "operationId" : "Providers.create",
        "tags" : [ "Delta Sharing: Providers" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ProviderInfo"
                }
              }
            },
            "description" : "The new provider was successfully created."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateProvider"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new authentication provider minimally based on a name and authentication type.\nThe caller must be an admin on the metastore.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.1/unity-catalog/providers/{name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "Name of the provider."
      } ],
      "get" : {
        "summary" : "Get a provider",
        "operationId" : "Providers.get",
        "tags" : [ "Delta Sharing: Providers" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ProviderInfo"
                }
              }
            },
            "description" : "The provider was successfully retrieved."
          }
        },
        "description" : "Gets a specific authentication provider.\nThe caller must supply the name of the provider, and must either be a metastore admin or the owner of the provider.\n",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update a provider",
        "operationId" : "Providers.update",
        "tags" : [ "Delta Sharing: Providers" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ProviderInfo"
                }
              }
            },
            "description" : "The provider was successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateProvider"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the information for an authentication provider, if the caller is a metastore admin or is the owner of the provider.\nIf the update changes the provider name, the caller must be both a metastore admin and the owner of the provider.\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a provider",
        "operationId" : "Providers.delete",
        "tags" : [ "Delta Sharing: Providers" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The provider was successfully deleted."
          }
        },
        "description" : "Deletes an authentication provider, if the caller is a metastore admin or is the owner of the provider.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.1/unity-catalog/providers/{name}/shares" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "Name of the provider in which to list shares."
      } ],
      "get" : {
        "summary" : "List shares by Provider",
        "operationId" : "Providers.listShares",
        "tags" : [ "Delta Sharing: Providers" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListProviderSharesResponse"
                }
              }
            },
            "description" : "The provider shares list was successfully retrieved."
          }
        },
        "description" : "Gets an array of a specified provider's shares within the metastore where:\n\n  * the caller is a metastore admin, or\n  * the caller is the owner.\n"
      }
    },
    "/api/2.1/unity-catalog/public/data_sharing_activation/{activation_url}" : {
      "get" : {
        "summary" : "Get an access token",
        "operationId" : "RecipientActivation.retrieveToken",
        "tags" : [ "Delta Sharing: Recipient Activation" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "activation_url",
          "schema" : {
            "type" : "string"
          },
          "description" : "The one time activation url.\nIt also accepts activation token."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.RetrieveTokenResponse"
                }
              }
            },
            "description" : "The access token was successfully retrieved."
          }
        },
        "description" : "Retrieve access token with an activation url.\nThis is a public API without any authentication."
      }
    },
    "/api/2.1/unity-catalog/public/data_sharing_activation_info/{activation_url}" : {
      "get" : {
        "summary" : "Get a share activation URL",
        "operationId" : "RecipientActivation.getActivationUrlInfo",
        "tags" : [ "Delta Sharing: Recipient Activation" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "activation_url",
          "schema" : {
            "type" : "string"
          },
          "description" : "The one time activation url.\nIt also accepts activation token."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.GetActivationUrlInfoResponse"
                }
              }
            },
            "description" : "The activation URL fetch was successful."
          }
        },
        "description" : "Gets an activation URL for a share."
      }
    },
    "/api/2.1/unity-catalog/recipients" : {
      "get" : {
        "summary" : "List share recipients",
        "operationId" : "Recipients.list",
        "tags" : [ "Delta Sharing: Recipients" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "data_recipient_global_metastore_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "If not provided, all recipients will be returned.\nIf no recipients exist with this ID, no results will be returned."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListRecipientsResponse"
                }
              }
            },
            "description" : "The list of recipients was successfully retrieved."
          }
        },
        "description" : "Gets an array of all share recipients within the current metastore where:\n\n  * the caller is a metastore admin, or\n  * the caller is the owner.\nThere is no guarantee of a specific ordering of the elements in the array.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "recipients"
        }
      },
      "post" : {
        "summary" : "Create a share recipient",
        "operationId" : "Recipients.create",
        "tags" : [ "Delta Sharing: Recipients" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.RecipientInfo"
                }
              }
            },
            "description" : "The new recipient was successfully created."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateRecipient"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new recipient with the delta sharing authentication type in the metastore.\nThe caller must be a metastore admin or has the **CREATE_RECIPIENT** privilege on the metastore.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.1/unity-catalog/recipients/{name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "Name of the recipient."
      } ],
      "get" : {
        "summary" : "Get a share recipient",
        "operationId" : "Recipients.get",
        "tags" : [ "Delta Sharing: Recipients" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.RecipientInfo"
                }
              }
            },
            "description" : "The recipient was successfully retrieved."
          }
        },
        "description" : "Gets a share recipient from the metastore if:\n\n  * the caller is the owner of the share recipient, or:\n  * is a metastore admin\n",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update a share recipient",
        "operationId" : "Recipients.update",
        "tags" : [ "Delta Sharing: Recipients" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The recipient was successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateRecipient"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates an existing recipient in the metastore. The caller must be a metastore admin or the owner of the recipient.\nIf the recipient name will be updated, the user must be both a metastore admin and the owner of the recipient.\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a share recipient",
        "operationId" : "Recipients.delete",
        "tags" : [ "Delta Sharing: Recipients" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The recipient was successfully deleted."
          }
        },
        "description" : "Deletes the specified recipient from the metastore. The caller must be the owner of the recipient.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.1/unity-catalog/recipients/{name}/rotate-token" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "The name of the recipient."
      } ],
      "post" : {
        "summary" : "Rotate a token",
        "operationId" : "Recipients.rotateToken",
        "tags" : [ "Delta Sharing: Recipients" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.RecipientInfo"
                }
              }
            },
            "description" : "The recipient token was successfully rotated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.RotateRecipientToken"
              }
            }
          },
          "description" : ""
        },
        "description" : "Refreshes the specified recipient's delta sharing authentication token with the provided token info.\nThe caller must be the owner of the recipient.\n"
      }
    },
    "/api/2.1/unity-catalog/recipients/{name}/share-permissions" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "The name of the Recipient."
      } ],
      "get" : {
        "summary" : "Get recipient share permissions",
        "operationId" : "Recipients.sharePermissions",
        "tags" : [ "Delta Sharing: Recipients" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.GetRecipientSharePermissionsResponse"
                }
              }
            },
            "description" : "The recipient share permissions list was successfully retrieved."
          }
        },
        "description" : "Gets the share permissions for the specified Recipient. The caller must be a metastore admin or the owner of the Recipient."
      }
    },
    "/api/2.1/unity-catalog/schemas" : {
      "get" : {
        "summary" : "List schemas",
        "operationId" : "Schemas.list",
        "tags" : [ "Schemas" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "catalog_name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Parent catalog for schemas of interest."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListSchemasResponse"
                }
              }
            },
            "description" : "The schemas list was successfully retrieved."
          }
        },
        "description" : "Gets an array of schemas for a catalog in the metastore.\nIf the caller is the metastore admin or the owner of the parent catalog, all schemas for the catalog will be retrieved.\nOtherwise, only schemas owned by the caller (or for which the caller has the **USE_SCHEMA** privilege) will be retrieved.\nThere is no guarantee of a specific ordering of the elements in the array.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "schemas"
        }
      },
      "post" : {
        "summary" : "Create a schema",
        "operationId" : "Schemas.create",
        "tags" : [ "Schemas" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.SchemaInfo"
                }
              }
            },
            "description" : "The new schema was successfully created."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateSchema"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new schema for catalog in the Metatastore.\nThe caller must be a metastore admin, or have the **CREATE_SCHEMA** privilege in the parent catalog.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.1/unity-catalog/schemas/{full_name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "full_name",
        "schema" : {
          "type" : "string"
        },
        "description" : "Full name of the schema."
      } ],
      "get" : {
        "summary" : "Get a schema",
        "operationId" : "Schemas.get",
        "tags" : [ "Schemas" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.SchemaInfo"
                }
              }
            },
            "description" : "The schema was successfully retrieved."
          }
        },
        "description" : "Gets the specified schema within the metastore.\nThe caller must be a metastore admin, the owner of the schema, or a user that has the **USE_SCHEMA** privilege on the schema.\n",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update a schema",
        "operationId" : "Schemas.update",
        "tags" : [ "Schemas" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.SchemaInfo"
                }
              }
            },
            "description" : "The schema was successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateSchema"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates a schema for a catalog. The caller must be the owner of the schema or a metastore admin.\nIf the caller is a metastore admin, only the __owner__ field can be changed in the update.\nIf the __name__ field must be updated, the caller must be a metastore admin or have the **CREATE_SCHEMA** privilege on the parent catalog.\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a schema",
        "operationId" : "Schemas.delete",
        "tags" : [ "Schemas" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The schema was successfully deleted."
          }
        },
        "description" : "Deletes the specified schema from the parent catalog.\nThe caller must be the owner of the schema or an owner of the parent catalog.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.1/unity-catalog/shares" : {
      "get" : {
        "summary" : "List shares",
        "operationId" : "Shares.list",
        "tags" : [ "Delta Sharing: Shares" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListSharesResponse"
                }
              }
            },
            "description" : "The shares list was successfully retrieved."
          }
        },
        "description" : "Gets an array of data object shares from the metastore. The caller must be a metastore admin or the owner of the share.\nThere is no guarantee of a specific ordering of the elements in the array.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "shares"
        }
      },
      "post" : {
        "summary" : "Create a share",
        "operationId" : "Shares.create",
        "tags" : [ "Delta Sharing: Shares" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ShareInfo"
                }
              }
            },
            "description" : "The new share was successfully created."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateShare"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new share for data objects. Data objects can be added at this time or after creation with **update**.\nThe caller must be a metastore admin or have the **CREATE_SHARE** privilege on the metastore.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.1/unity-catalog/shares/{name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "The name of the share."
      } ],
      "get" : {
        "summary" : "Get a share",
        "operationId" : "Shares.get",
        "tags" : [ "Delta Sharing: Shares" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "include_shared_data",
          "schema" : {
            "type" : "boolean"
          },
          "description" : "Query for data to include in the share."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ShareInfo"
                }
              }
            },
            "description" : "The share was successfully retrieved."
          }
        },
        "description" : "Gets a data object share from the metastore. The caller must be a metastore admin or the owner of the share.",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update a share",
        "operationId" : "Shares.update",
        "tags" : [ "Delta Sharing: Shares" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ShareInfo"
                }
              }
            },
            "description" : "The share was successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateShare"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the share with the changes and data objects in the request.\nThe caller must be the owner of the share or a metastore admin.\n\nWhen the caller is a metastore admin, only the __owner__ field can be updated.\n\nIn the case that the share name is changed, **updateShare** requires that the caller is both the share owner and\na metastore admin.\n\nFor each table that is added through this method, the share owner must also have **SELECT** privilege on the table.\nThis privilege must be maintained indefinitely for recipients to be able to access the table.\nTypically, you should use a group as the share owner.\n\nTable removals through **update** do not require additional privileges.\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a share",
        "operationId" : "Shares.delete",
        "tags" : [ "Delta Sharing: Shares" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The share was successfully deleted."
          }
        },
        "description" : "Deletes a data object share from the metastore. The caller must be an owner of the share.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.1/unity-catalog/shares/{name}/permissions" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "The name of the share."
      } ],
      "get" : {
        "summary" : "Get permissions",
        "operationId" : "Shares.sharePermissions",
        "tags" : [ "Delta Sharing: Shares" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.PermissionsList"
                }
              }
            },
            "description" : "The share permissions list was successfully retrieved."
          }
        },
        "description" : "Gets the permissions for a data share from the metastore.\nThe caller must be a metastore admin or the owner of the share.\n",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update permissions",
        "operationId" : "Shares.updatePermissions",
        "tags" : [ "Delta Sharing: Shares" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The share permissions were successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateSharePermissions"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates the permissions for a data share in the metastore.\nThe caller must be a metastore admin or an owner of the share.\n\nFor new recipient grants, the user must also be the owner of the recipients.\nrecipient revocations do not require additional privileges.\n",
        "x-databricks-crud" : "update"
      }
    },
    "/api/2.1/unity-catalog/storage-credentials" : {
      "get" : {
        "summary" : "List credentials",
        "operationId" : "StorageCredentials.list",
        "tags" : [ "Storage Credentials" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListStorageCredentialsResponse"
                }
              }
            },
            "description" : "The storage credentials list was successfully retrieved."
          }
        },
        "description" : "Gets an array of storage credentials (as __StorageCredentialInfo__ objects).\nThe array is limited to only those storage credentials the caller has permission to access.\nIf the caller is a metastore admin, all storage credentials will be retrieved.\nThere is no guarantee of a specific ordering of the elements in the array.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "storage_credentials"
        }
      },
      "post" : {
        "summary" : "Create a storage credential",
        "operationId" : "StorageCredentials.create",
        "tags" : [ "Storage Credentials" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.StorageCredentialInfo"
                }
              }
            },
            "description" : "The new storage credential was successfully created."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateStorageCredential"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new storage credential.\nThe request object is specific to the cloud:\n\n  * **AwsIamRole** for AWS credentials\n  * **AzureServicePrincipal** for Azure credentials\n  * **GcpServiceAcountKey** for GCP credentials.\n\nThe caller must be a metastore admin and have the **CREATE_STORAGE_CREDENTIAL** privilege on the metastore.\n",
        "x-databricks-crud" : "create"
      }
    },
    "/api/2.1/unity-catalog/storage-credentials/{name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "Name of the storage credential."
      } ],
      "get" : {
        "summary" : "Get a credential",
        "operationId" : "StorageCredentials.get",
        "tags" : [ "Storage Credentials" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.StorageCredentialInfo"
                }
              }
            },
            "description" : "The storage credential was successfully retrieved."
          }
        },
        "description" : "Gets a storage credential from the metastore.\nThe caller must be a metastore admin, the owner of the storage credential, or have some permission on the storage credential.\n",
        "x-databricks-crud" : "read"
      },
      "patch" : {
        "summary" : "Update a credential",
        "operationId" : "StorageCredentials.update",
        "tags" : [ "Storage Credentials" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.StorageCredentialInfo"
                }
              }
            },
            "description" : "The storage credential was successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateStorageCredential"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates a storage credential on the metastore.\nThe caller must be the owner of the storage credential or a metastore admin. If the caller is a metastore admin, only the __owner__ credential can be changed.\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete a credential",
        "operationId" : "StorageCredentials.delete",
        "tags" : [ "Storage Credentials" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "force",
          "schema" : {
            "type" : "boolean"
          },
          "description" : "Force deletion even if there are dependent external locations or external tables."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The storage credential was successfully deleted."
          }
        },
        "description" : "Deletes a storage credential from the metastore. The caller must be an owner of the storage credential.",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.1/unity-catalog/table-summaries" : {
      "get" : {
        "summary" : "List table summaries",
        "operationId" : "Tables.listSummaries",
        "tags" : [ "Tables" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "catalog_name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of parent catalog for tables of interest."
        }, {
          "in" : "query",
          "name" : "schema_name_pattern",
          "schema" : {
            "type" : "string"
          },
          "description" : "A sql LIKE pattern (% and _) for schema names.\nAll schemas will be returned if not set or empty."
        }, {
          "in" : "query",
          "name" : "table_name_pattern",
          "schema" : {
            "type" : "string"
          },
          "description" : "A sql LIKE pattern (% and _) for table names.\nAll tables will be returned if not set or empty."
        }, {
          "in" : "query",
          "name" : "max_results",
          "schema" : {
            "format" : "int32",
            "type" : "integer"
          },
          "description" : "Maximum number of tables to return (page length). Defaults to 10000."
        }, {
          "in" : "query",
          "name" : "page_token",
          "schema" : {
            "type" : "string"
          },
          "description" : "Opaque token to send for the next page of results (pagination)."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListTableSummariesResponse"
                }
              }
            },
            "description" : "The table summaries list was successfully retrieved."
          }
        },
        "description" : "Gets an array of summaries for tables for a schema and catalog within the metastore. The table summaries returned are either:\n\n* summaries for all tables (within the current metastore and parent catalog and schema), when the user is a metastore admin, or:\n* summaries for all tables and schemas (within the current metastore and parent catalog)\n  for which the user has ownership or the **SELECT** privilege on the table and ownership or **USE_SCHEMA** privilege on the schema,\n  provided that the user also has ownership or the **USE_CATALOG** privilege on the parent catalog. \n\nThere is no guarantee of a specific ordering of the elements in the array.\n",
        "x-databricks-crud" : "list"
      }
    },
    "/api/2.1/unity-catalog/tables" : {
      "get" : {
        "summary" : "List tables",
        "operationId" : "Tables.list",
        "tags" : [ "Tables" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "catalog_name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Name of parent catalog for tables of interest."
        }, {
          "required" : true,
          "in" : "query",
          "name" : "schema_name",
          "schema" : {
            "type" : "string"
          },
          "description" : "Parent schema of tables."
        }, {
          "in" : "query",
          "name" : "include_delta_metadata",
          "schema" : {
            "type" : "boolean"
          },
          "description" : "Whether delta metadata should be included in the response."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ListTablesResponse"
                }
              }
            },
            "description" : "The tables list was successfully retrieved."
          }
        },
        "description" : "Gets an array of all tables for the current metastore under the parent catalog and schema.\nThe caller must be a metastore admin or an owner of (or have the **SELECT** privilege on) the table.\nFor the latter case, the caller must also be the owner or have the **USE_CATALOG** privilege on the parent catalog and the **USE_SCHEMA** privilege on the parent schema.\nThere is no guarantee of a specific ordering of the elements in the array.\n",
        "x-databricks-crud" : "list",
        "x-databricks-pagination" : {
          "results" : "tables"
        }
      }
    },
    "/api/2.1/unity-catalog/tables/{full_name}" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "full_name",
        "schema" : {
          "type" : "string"
        },
        "description" : "Full name of the table."
      } ],
      "get" : {
        "summary" : "Get a table",
        "operationId" : "Tables.get",
        "tags" : [ "Tables" ],
        "parameters" : [ {
          "in" : "query",
          "name" : "include_delta_metadata",
          "schema" : {
            "type" : "boolean"
          },
          "description" : "Whether delta metadata should be included in the response."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.TableInfo"
                }
              }
            },
            "description" : "The table was successfully retrieved."
          }
        },
        "description" : "Gets a table from the metastore for a specific catalog and schema.\nThe caller must be a metastore admin, be the owner of the table and have the **USE_CATALOG** privilege on the parent catalog and the **USE_SCHEMA** privilege on the parent schema,\nor be the owner of the table and have the **SELECT** privilege on it as well.\n",
        "x-databricks-crud" : "read"
      },
      "delete" : {
        "summary" : "Delete a table",
        "operationId" : "Tables.delete",
        "tags" : [ "Tables" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The table was successfully deleted."
          }
        },
        "description" : "Deletes a table from the specified parent catalog and schema.\nThe caller must be the owner of the parent catalog, have the **USE_CATALOG** privilege on the parent catalog and be the owner of the parent schema,\nor be the owner of the table and have the **USE_CATALOG** privilege on the parent catalog and the **USE_SCHEMA** privilege on the parent schema.\n",
        "x-databricks-crud" : "delete"
      }
    },
    "/api/2.1/unity-catalog/validate-storage-credentials" : {
      "post" : {
        "summary" : "Validate a storage credential",
        "operationId" : "StorageCredentials.validate",
        "tags" : [ "Storage Credentials" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/unitycatalog.ValidateStorageCredentialResponse"
                }
              }
            },
            "description" : "The storage credential validation operation completed successfully."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.ValidateStorageCredential"
              }
            }
          },
          "description" : ""
        },
        "description" : "Validates a storage credential.\nAt least one of __external_location_name__ and __url__ need to be provided. If only one of them is\nprovided, it will be used for validation. And if both are provided, the __url__ will be used for\nvalidation, and __external_location_name__ will be ignored when checking overlapping urls.\n\nEither the __storage_credential_name__ or the cloud-specific credential must be provided.\n\nThe caller must be a metastore admin or the storage credential owner or\nhave the **CREATE_EXTERNAL_LOCATION** privilege on the metastore and the storage credential.\n",
        "x-databricks-crud" : "validate"
      }
    },
    "/api/2.1/unity-catalog/workspaces/{workspace_id}/metastore" : {
      "parameters" : [ {
        "required" : true,
        "in" : "path",
        "name" : "workspace_id",
        "schema" : {
          "format" : "int64",
          "type" : "integer"
        },
        "description" : "A workspace ID."
      } ],
      "put" : {
        "summary" : "Create an assignment",
        "operationId" : "Metastores.assign",
        "tags" : [ "Metastores" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The metastore was successfully assigned."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.CreateMetastoreAssignment"
              }
            }
          },
          "description" : ""
        },
        "description" : "Creates a new metastore assignment.\nIf an assignment for the same __workspace_id__ exists, it will be overwritten by the new __metastore_id__ and\n__default_catalog_name__. The caller must be an account admin.\n",
        "x-databricks-crud" : "create"
      },
      "patch" : {
        "summary" : "Update an assignment",
        "operationId" : "Metastores.updateAssignment",
        "tags" : [ "Metastores" ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The metastore assignment was successfully updated."
          }
        },
        "requestBody" : {
          "content" : {
            "application/json" : {
              "schema" : {
                "extRef" : true,
                "ref" : true,
                "$ref" : "#/components/schemas/unitycatalog.UpdateMetastoreAssignment"
              }
            }
          },
          "description" : ""
        },
        "description" : "Updates a metastore assignment. This operation can be used to update __metastore_id__ or __default_catalog_name__\nfor a specified Workspace, if the Workspace is already assigned a metastore.\nThe caller must be an account admin to update __metastore_id__; otherwise, the caller can be a Workspace admin.\n",
        "x-databricks-crud" : "update"
      },
      "delete" : {
        "summary" : "Delete an assignment",
        "operationId" : "Metastores.unassign",
        "tags" : [ "Metastores" ],
        "parameters" : [ {
          "required" : true,
          "in" : "query",
          "name" : "metastore_id",
          "schema" : {
            "type" : "string"
          },
          "description" : "Query for the ID of the metastore to delete."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "properties" : { },
                  "type" : "object"
                }
              }
            },
            "description" : "The metastore assignment was successfully deleted."
          }
        },
        "description" : "Deletes a metastore assignment. The caller must be an account administrator.",
        "x-databricks-crud" : "delete"
      }
    },
    "/serving-endpoints/{name}/invocations" : {
      "post" : {
        "summary" : "Query a serving endpoint with provided model input.",
        "operationId" : "ServingEndpoints.query",
        "tags" : [ "Serving endpoints" ],
        "parameters" : [ {
          "required" : true,
          "in" : "path",
          "name" : "name",
          "schema" : {
            "example" : "feed-ads",
            "type" : "string"
          },
          "description" : "The name of the serving endpoint. This field is required."
        } ],
        "responses" : {
          "200" : {
            "content" : {
              "application/json" : {
                "schema" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/endpoints.QueryEndpointResponse"
                }
              }
            },
            "description" : "Serving endpoint was queried successfully and returned predictions."
          }
        }
      }
    }
  },
  "components" : {
    "parameters" : {
      "billing.account_id" : {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Databricks account ID of any type. For non-E2 account types, get your account ID from the [Accounts Console](https://docs.databricks.com/administration-guide/account-settings/usage.html)."
      },
      "deployment.GcpAccessToken" : {
        "in" : "header",
        "name" : "X-Databricks-GCP-SA-Access-Token",
        "schema" : {
          "type" : "string"
        },
        "description" : "The Google Cloud access token of the caller. For details about this access token, see [Authentication using Open ID Connect (OIDC) tokens](https://docs.gcp.databricks.com/dev-tools/api/latest/authentication-oidc.html).",
        "x-databricks-cloud" : "gcp"
      },
      "deployment.account_id" : {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Databricks account ID of any type. For non-E2 account types, get your account ID from the [Accounts Console](https://docs.databricks.com/administration-guide/account-settings/usage.html)."
      },
      "mlflow.comment" : {
        "in" : "query",
        "name" : "comment",
        "schema" : {
          "type" : "string"
        },
        "description" : "User-provided comment on the action."
      },
      "mlflow.name" : {
        "required" : true,
        "in" : "query",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "Name of the model."
      },
      "mlflow.version" : {
        "required" : true,
        "in" : "query",
        "name" : "version",
        "schema" : {
          "type" : "string"
        },
        "description" : "Version of the model."
      },
      "sql.chunkIndex" : {
        "required" : true,
        "in" : "path",
        "name" : "chunk_index",
        "schema" : {
          "format" : "int32",
          "type" : "integer"
        }
      },
      "sql.statementId" : {
        "required" : true,
        "in" : "path",
        "name" : "statement_id",
        "schema" : {
          "description" : "Statement ID is returned upon successful submission of a SQL statement, and is a required reference for all\nsubsequent calls.\n",
          "type" : "string"
        }
      },
      "unitycatalog.account_id" : {
        "required" : true,
        "in" : "path",
        "name" : "account_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Databricks account ID of any type. For non-E2 account types, get your account ID from the [Accounts Console](https://docs.databricks.com/administration-guide/account-settings/usage.html)."
      },
      "unitycatalog.metastore_id" : {
        "required" : true,
        "in" : "path",
        "name" : "metastore_id",
        "schema" : {
          "format" : "uuid",
          "type" : "string"
        },
        "description" : "Databricks Unity Catalog metastore ID"
      },
      "unitycatalog.storage_credential_name" : {
        "required" : true,
        "in" : "path",
        "name" : "name",
        "schema" : {
          "type" : "string"
        },
        "description" : "Name of the storage credential."
      }
    },
    "responses" : {
      "billing.BadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.Error"
            }
          }
        },
        "description" : "The request is malformed."
      },
      "billing.Conflict" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.Error"
            }
          }
        },
        "description" : "The request conflicts with the current state of the target resource."
      },
      "billing.Forbidden" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.Error"
            }
          }
        },
        "description" : "The request is forbidden from being fulfilled."
      },
      "billing.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.Error"
            }
          }
        },
        "description" : "The request is not handled correctly due to a server error."
      },
      "billing.NotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.Error"
            }
          }
        },
        "description" : "The requested resource does not exist."
      },
      "billing.ServiceUnavailable" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.Error"
            }
          }
        },
        "description" : "The service is unavailable."
      },
      "billing.Unauthenticated" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.Error"
            }
          }
        },
        "description" : "The request is unauthenticated. The user's credentials are missing or incorrect."
      },
      "clusterpolicies.BadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusterpolicies.Error"
            },
            "examples" : {
              "bad_request" : {
                "value" : {
                  "error_code" : "INVALID_PARAMETER_VALUE",
                  "message" : "Missing required field: policy_id"
                }
              }
            }
          }
        },
        "description" : "The request is malformed."
      },
      "clusterpolicies.ClusterPolicyNotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusterpolicies.Error"
            },
            "examples" : {
              "cluster_policy_not_found" : {
                "value" : {
                  "error_code" : "RESOURCE_DOES_NOT_EXIST",
                  "message" : "Can't find a cluster policy with id: ABCD000000000000"
                }
              }
            }
          }
        },
        "description" : "Cluster policy was not found."
      },
      "clusterpolicies.Forbidden" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusterpolicies.Error"
            },
            "examples" : {
              "bad_request" : {
                "value" : {
                  "error_code" : "PERMISSION_DENIED",
                  "message" : "Unauthorized Access."
                }
              }
            }
          }
        },
        "description" : "The user does not have access to the requested resource."
      },
      "clusterpolicies.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusterpolicies.Error"
            },
            "examples" : {
              "internal_error" : {
                "value" : {
                  "error_code" : "INTERNAL_ERROR",
                  "message" : "There was an error performing the operation. Please try again or open a support ticket."
                }
              }
            }
          }
        },
        "description" : "The request is not handled correctly because of a server error."
      },
      "clusterpolicies.PolicyFamilyNotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusterpolicies.Error"
            },
            "examples" : {
              "policy_family_not_found" : {
                "value" : {
                  "error_code" : "RESOURCE_DOES_NOT_EXIST",
                  "message" : "Cannot find a cluster policy family with id bad-id"
                }
              }
            }
          }
        },
        "description" : "Policy family was not found.",
        "x-databricks-not-cloud" : "gcp"
      },
      "deployment.BadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/deployment.Error"
            }
          }
        },
        "description" : "The request is malformed."
      },
      "deployment.Conflict" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/deployment.Error"
            }
          }
        },
        "description" : "The request conflicts with the current state of the target resource."
      },
      "deployment.Forbidden" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/deployment.Error"
            }
          }
        },
        "description" : "The request is forbidden from being fulfilled."
      },
      "deployment.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/deployment.Error"
            }
          }
        },
        "description" : "The request is not handled correctly due to a server error."
      },
      "deployment.NotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/deployment.Error"
            }
          }
        },
        "description" : "The requested resource does not exist."
      },
      "deployment.ServiceUnavailable" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/deployment.Error"
            }
          }
        },
        "description" : "The service is unavailable."
      },
      "deployment.Unauthenticated" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/deployment.Error"
            }
          }
        },
        "description" : "The request is unauthenticated. The user's credentials are missing or incorrect."
      },
      "endpoints.BadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.Error"
            },
            "example" : {
              "error_code" : "INVALID_PARAMETER_VALUE",
              "message" : "Invalid value for parameter endpoint name."
            }
          }
        },
        "description" : "The request was malformed. See JSON response for error details."
      },
      "endpoints.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.Error"
            }
          }
        },
        "description" : "The request was not handled correctly due to a server error."
      },
      "endpoints.NotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.Error"
            },
            "examples" : {
              "resource_does_not_exist" : {
                "value" : {
                  "error_code" : "RESOURCE_DOES_NOT_EXIST",
                  "message" : "No endpoint with name 'feed-ads' found."
                }
              }
            }
          }
        },
        "description" : "The requested resource does not exist."
      },
      "endpoints.Unauthorized" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.Error"
            },
            "example" : {
              "error_code" : "PERMISSION_DENIED",
              "message" : "Unauthorized access."
            }
          }
        },
        "description" : "The request was unauthorized."
      },
      "endpoints.UpdateConflict" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.Error"
            },
            "examples" : {
              "resource_conflict" : {
                "value" : {
                  "error_code" : "RESOURCE_CONFLICT",
                  "message" : "Endpoint served models are currently being updated. Please try again after the current update is no longer in progress."
                }
              }
            }
          }
        },
        "description" : "There is an ongoing update to the endpoint. Only one update at a time is allowed.\nPlease wait for the current update to finish.\n"
      },
      "gitcredentials.BadRequestPost" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/gitcredentials.Error"
            },
            "examples" : {
              "credential_already_exists" : {
                "value" : {
                  "error_code" : "CREDENTIAL_ALREADY_EXISTS",
                  "message" : "Only one Git credential is supported. To update your Git credential, use the PATCH endpoint."
                }
              },
              "git_username_missing" : {
                "value" : {
                  "error_code" : "GIT_USERNAME_MISSING",
                  "message" : "Git username is a required field for all Git providers."
                }
              },
              "personal_access_token_missing" : {
                "value" : {
                  "error_code" : "TOKEN_MISSING",
                  "message" : "Personal access token is a required field for all Git providers."
                }
              }
            }
          }
        },
        "description" : "Request is invalid"
      },
      "gitcredentials.CredentialNotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/gitcredentials.Error"
            },
            "examples" : {
              "credential_not_found" : {
                "value" : {
                  "error_code" : "CREDENTIAL_NOT_FOUND",
                  "message" : "Credential with the specified ID not found."
                }
              }
            }
          }
        },
        "description" : "Credential with the specified ID does not exist."
      },
      "gitcredentials.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/gitcredentials.Error"
            },
            "examples" : {
              "internal_error" : {
                "value" : {
                  "error_code" : "INTERNAL_ERROR",
                  "message" : "There was an error performing the operation. Please try again or open a support ticket."
                }
              }
            }
          }
        },
        "description" : "The request was not handled correctly due to a server error."
      },
      "gitcredentials.InvalidGitProvider" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/gitcredentials.Error"
            },
            "examples" : {
              "bad_request" : {
                "value" : {
                  "error_code" : "INVALID_GIT_PROVIDER"
                }
              }
            }
          }
        },
        "description" : "The specified Git provider is invalid."
      },
      "globalinitscripts.BadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/globalinitscripts.Error"
            },
            "examples" : {
              "Bad request - resource already exists" : {
                "value" : {
                  "error_code" : "RESOURCE_ALREADY_EXISTS",
                  "message" : "There already exists a global init script with name: My Example Script"
                }
              },
              "Bad request - invalid position param" : {
                "value" : {
                  "error_code" : "INVALID_PARAMETER_VALUE",
                  "message" : "`position` must be a non-negative integer"
                }
              },
              "Bad request - invalid name param" : {
                "value" : {
                  "error_code" : "INVALID_PARAMETER_VALUE",
                  "message" : "`name` can only contain spaces, alphanumeric or '-', '_', '.' characters"
                }
              },
              "Bad request - empty name param" : {
                "value" : {
                  "error_code" : "INVALID_PARAMETER_VALUE",
                  "message" : "`name` cannot be empty"
                }
              },
              "Bad request - empty script param" : {
                "value" : {
                  "error_code" : "INVALID_PARAMETER_VALUE",
                  "message" : "`script` cannot be empty"
                }
              },
              "Bad request - invalid script param" : {
                "value" : {
                  "error_code" : "MALFORMED_REQUEST",
                  "message" : "Could not parse request object: Illegal character '<char>' (code 0x<hex-value>) in Base64 content."
                }
              }
            }
          }
        },
        "description" : "The request was malformed. See JSON response for error details."
      },
      "globalinitscripts.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/globalinitscripts.Error"
            }
          }
        },
        "description" : "The request was not handled correctly due to a server error"
      },
      "globalinitscripts.NotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/globalinitscripts.Error"
            },
            "examples" : {
              "resource_does_not_exist" : {
                "value" : {
                  "error_code" : "RESOURCE_DOES_NOT_EXIST",
                  "message" : "The global unit script with ID 173160F64251D16E does not exist."
                }
              }
            }
          }
        },
        "description" : "The requested resource does not exist"
      },
      "globalinitscripts.Unauthorized" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/globalinitscripts.Error"
            },
            "example" : {
              "error_code" : "PERMISSION_DENIED",
              "message" : "Unauthorized access."
            }
          }
        },
        "description" : "The request was unauthorized."
      },
      "ipaccesslists.BadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/ipaccesslists.Error"
            },
            "examples" : {
              "bad_request" : {
                "value" : {
                  "error_code" : "BAD_REQUEST",
                  "message" : "Invalid IP address: <ip-address>"
                }
              },
              "invalid_state" : {
                "value" : {
                  "error_code" : "INVALID_STATE",
                  "message" : "Your current IP is not allowed to access the workspace under the current configuration."
                }
              },
              "quota_exceeded" : {
                "value" : {
                  "error_code" : "QUOTA_EXCEEDED",
                  "message" : "IP access list quota exceeded (<quota> IP/CIDR values)."
                }
              },
              "resource_already_exists" : {
                "value" : {
                  "error_code" : "RESOURCE_ALREADY_EXISTS",
                  "message" : "IP access list with type (`ALLOW`|`BLOCK`) and label (<list-label>) already exists."
                }
              }
            }
          }
        },
        "description" : "The request is malformed."
      },
      "ipaccesslists.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/ipaccesslists.Error"
            }
          }
        },
        "description" : "The request is not handled correctly due to a server error."
      },
      "ipaccesslists.NotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/ipaccesslists.Error"
            },
            "examples" : {
              "bad_request" : {
                "value" : {
                  "error_code" : "FEATURE_DISABLED",
                  "message" : "IP access list is not available in the pricing tier of this workspace."
                }
              },
              "invalid_state" : {
                "value" : {
                  "error_code" : "RESOURCE_DOES_NOT_EXIST",
                  "message" : "Cannot find an IP access list with ID: <uuid>"
                }
              }
            }
          }
        },
        "description" : "The requested resource does not exist."
      },
      "ipaccesslists.Unauthorized" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/ipaccesslists.Error"
            },
            "example" : {
              "error_code" : "PERMISSION_DENIED",
              "message" : "IP access list can be managed by admins only."
            }
          }
        },
        "description" : "The request is unauthorized."
      },
      "jobs.BadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.Error"
            },
            "example" : {
              "error_code" : "INVALID_PARAMETER_VALUE",
              "message" : "Invalid value for parameter job_id"
            }
          }
        },
        "description" : "The request was malformed. See JSON response for error details."
      },
      "jobs.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.Error"
            }
          }
        },
        "description" : "The request was not handled correctly due to a server error."
      },
      "jobs.NotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.Error"
            },
            "examples" : {
              "resource_does_not_exist" : {
                "value" : {
                  "error_code" : "ENDPOINT_NOT_FOUND",
                  "message" : "No API endpoint found"
                }
              }
            }
          }
        },
        "description" : "The requested resource does not exist."
      },
      "jobs.Unauthorized" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.Error"
            },
            "example" : {
              "error_code" : "PERMISSION_DENIED",
              "message" : "Unauthorized access."
            }
          }
        },
        "description" : "The request was unauthorized."
      },
      "mlflow.BadRequest-InvalidParameterValue" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Error"
            },
            "example" : {
              "error_code" : "INVALID_PARAMETER_VALUE",
              "message" : "Got an invalid version number '0'. Model version numbers are integers, starting from 1."
            }
          }
        },
        "description" : "The request is malformed."
      },
      "mlflow.BadRequest-ResourceAlreadyExists" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Error"
            },
            "example" : {
              "error_code" : "RESOURCE_ALREADY_EXISTS",
              "message" : "Transition request already exists"
            }
          }
        },
        "description" : "The request is malformed."
      },
      "mlflow.Conflict" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Error"
            },
            "example" : {
              "error_code" : "INVALID_STATE_TRANSITION",
              "message" : "This operation cannot be performed. Model 'search-ads-model' version 1 is pending registration."
            }
          }
        },
        "description" : "The request conflicts with the current state of the resource."
      },
      "mlflow.Forbidden" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Error"
            },
            "example" : {
              "error_code" : "PERMISSION_DENIED",
              "message" : "Request failed access control checks."
            }
          }
        },
        "description" : "The request is forbidden."
      },
      "mlflow.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Error"
            },
            "example" : {
              "error_code" : "INTERNAL_ERROR",
              "message" : "Something went wrong"
            }
          }
        },
        "description" : "The request is not handled correctly due to a server error."
      },
      "mlflow.NotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Error"
            },
            "example" : {
              "error_code" : "RESOURCE_DOES_NOT_EXIST",
              "message" : "RegisteredModel 'search-ads-model' does not exist. It might have been deleted."
            }
          }
        },
        "description" : "The requested resource does not exist."
      },
      "repos.BadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/repos.Error"
            },
            "examples" : {
              "invalid_repo_path" : {
                "value" : {
                  "error_code" : "BAD_REQUEST",
                  "message" : "Invalid repo path specified."
                }
              }
            }
          }
        },
        "description" : "The request is invalid."
      },
      "repos.DirectoryNotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/repos.Error"
            },
            "examples" : {
              "directory_not_found" : {
                "value" : {
                  "error_code" : "RESOURCE_DOES_NOT_EXIST",
                  "message" : "RESOURCE_DOES_NOT_EXIST: Parent directory /Repos/testfolder does not exist."
                }
              }
            }
          }
        },
        "description" : "The specified directory does not exist."
      },
      "repos.Forbidden" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/repos.Error"
            },
            "examples" : {
              "bad_request" : {
                "value" : {
                  "error_code" : "PERMISSION_DENIED",
                  "message" : "PERMISSION_DENIED: jsmith@example.com does not have Manage permissions on /Repos/Production/testrepo. Contact the owner or an administrator for access."
                }
              }
            }
          }
        },
        "description" : "The user does not have access to the requested resource."
      },
      "repos.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/repos.Error"
            },
            "examples" : {
              "internal_error" : {
                "value" : {
                  "error_code" : "INTERNAL_ERROR",
                  "message" : "There was an error performing the operation. Try again or open a support ticket."
                }
              }
            }
          }
        },
        "description" : "The request is not handled correctly due to a server error."
      },
      "repos.RepoNotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/repos.Error"
            },
            "examples" : {
              "repo_not_found" : {
                "value" : {
                  "error_code" : "RESOURCE_DOES_NOT_EXIST",
                  "message" : "Repo could not be found."
                }
              }
            }
          }
        },
        "description" : "The specified repo does not exist."
      },
      "repos.ReposNotEnabled" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/repos.Error"
            },
            "examples" : {
              "bad_request" : {
                "value" : {
                  "error_code" : "FEATURE_DISABLED",
                  "message" : "FEATURE_DISABLED: Repos is not enabled."
                }
              }
            }
          }
        },
        "description" : "Repos is not enabled for the workspace."
      },
      "repos.UnknownRef" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/repos.Error"
            },
            "examples" : {
              "unknown_ref" : {
                "value" : {
                  "error_code" : "GIT_UNKNOWN_REF",
                  "message" : "Remote branch does not exist."
                }
              }
            }
          }
        },
        "description" : "The specified ref is invalid."
      },
      "scim.BadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ScimError"
            }
          }
        },
        "description" : "The request is malformed."
      },
      "scim.Conflict" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ScimError"
            }
          }
        },
        "description" : "The request conflicts with the current state of the target resource."
      },
      "scim.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ScimError"
            }
          }
        },
        "description" : "The request is not handled correctly due to a server error."
      },
      "scim.NotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ScimError"
            }
          }
        },
        "description" : "The requested resource does not exist."
      },
      "scim.Unauthenticated" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.Error"
            }
          }
        },
        "description" : "The request is unauthenticated. The user's credentials are missing or incorrect."
      },
      "scim.Unauthorized" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ScimError"
            }
          }
        },
        "description" : "Operation is not permitted based on the supplied authorization."
      },
      "sql.AccessControlListResponse" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "properties" : {
                "access_control_list" : {
                  "type" : "array",
                  "items" : {
                    "extRef" : true,
                    "ref" : true,
                    "$ref" : "#/components/schemas/sql.AccessControl"
                  }
                },
                "object_id" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.object_type"
                },
                "object_type" : {
                  "example" : "query/2cca1687-60ff-4886-a445-0230578c864d",
                  "format" : "<object-type>/<uuid>",
                  "description" : "An object's type and UUID, separated by a forward slash (/) character.",
                  "type" : "string"
                }
              },
              "type" : "object"
            }
          }
        },
        "description" : "A JSON representation of the access control list (ACL) for an object."
      },
      "sql.BadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "properties" : {
                "message" : {
                  "example" : "The browser (or proxy) sent a request that this server could not understand.",
                  "description" : "Human-readable error message that describes the cause of the error.",
                  "type" : "string"
                }
              },
              "type" : "object"
            }
          }
        },
        "description" : "Bad request"
      },
      "sql.CancelStatementResponse" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "properties" : { },
              "type" : "object"
            }
          }
        },
        "description" : ""
      },
      "sql.ErrorResponse" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.ServiceError"
            }
          }
        },
        "description" : ""
      },
      "sql.Forbidden" : {
        "content" : {
          "text/plain; charset=utf-8" : {
            "schema" : {
              "format" : "html",
              "description" : "An HTML page that describes the cause of the error.",
              "type" : "string"
            }
          }
        },
        "description" : "Forbidden request."
      },
      "sql.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.Error"
            }
          }
        },
        "description" : "The request failed due to a server error."
      },
      "sql.NotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "properties" : {
                "message" : {
                  "example" : "The requested URL was not found on the server. If you entered the URL manually please check your spelling and try again.",
                  "description" : "Human-readable error message that describes the cause of the error.",
                  "type" : "string"
                }
              },
              "type" : "object"
            }
          }
        },
        "description" : "Path not found."
      },
      "sql.ObjectTransferBadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "properties" : {
                "message" : {
                  "description" : "Human-readable error message that describes the cause of the error.",
                  "type" : "string",
                  "enum" : [ "You must specify a new_owner by email address", "The browser (or proxy) sent a request that this server could not understand." ]
                }
              },
              "type" : "object"
            }
          }
        },
        "description" : "Bad request."
      },
      "sql.ServerError" : {
        "content" : {
          "text/plain; charset=utf-8" : {
            "schema" : {
              "format" : "html",
              "description" : "An HTML error page that indicates a general server error but does not mention the cause of the error.",
              "type" : "string"
            }
          }
        },
        "description" : "Server error."
      },
      "sql.StatementResponse" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "properties" : {
                "manifest" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.ResultManifest"
                },
                "result" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.ResultData"
                },
                "statement_id" : {
                  "description" : "Statement ID is returned upon successful submission of a SQL statement, and is a required reference for all\nsubsequent calls.\n",
                  "type" : "string"
                },
                "status" : {
                  "extRef" : true,
                  "ref" : true,
                  "$ref" : "#/components/schemas/sql.StatementStatus"
                }
              },
              "type" : "object"
            },
            "examples" : {
              "statement_response_ext_links_succeeded" : {
                "summary" : "Large result sets with EXTERNAL_LINKS + ARROW_STREAM",
                "value" : {
                  "manifest" : {
                    "format" : "ARROW_STREAM",
                    "total_chunk_count" : 1,
                    "chunks" : [ {
                      "chunk_index" : 0,
                      "row_count" : 100,
                      "row_offset" : 0
                    } ],
                    "schema" : {
                      "column_count" : 1,
                      "columns" : [ {
                        "name" : "id",
                        "position" : 0,
                        "type_name" : "LONG",
                        "type_text" : "BIGINT"
                      } ]
                    },
                    "total_byte_count" : 16160,
                    "total_row_count" : 100
                  },
                  "result" : {
                    "external_links" : [ {
                      "external_link" : "https://someplace.s3.us-west-2.amazonaws.com/very/long/path/...",
                      "row_count" : 100,
                      "expiration" : "2023-01-30T22:23:23.140Z",
                      "chunk_index" : 0,
                      "row_offset" : 0
                    } ]
                  },
                  "statement_id" : "01eda0ea-9b4b-15ce-b8bb-a7d4114cb5ed",
                  "status" : {
                    "state" : "SUCCEEDED"
                  }
                }
              },
              "statement_response_inline_succeeded" : {
                "summary" : "JSON_ARRAY formatted data returned INLINE",
                "value" : {
                  "manifest" : {
                    "format" : "JSON_ARRAY",
                    "schema" : {
                      "column_count" : 1,
                      "columns" : [ {
                        "name" : "id",
                        "position" : 0,
                        "type_name" : "LONG",
                        "type_text" : "BIGINT"
                      } ]
                    }
                  },
                  "result" : {
                    "chunk_index" : 0,
                    "data_array" : [ [ "0" ], [ "1" ], [ "2" ] ],
                    "row_count" : 3,
                    "row_offset" : 0
                  },
                  "statement_id" : "01eda0e7-e315-1846-84e2-79a963ffad44",
                  "status" : {
                    "state" : "SUCCEEDED"
                  }
                }
              },
              "statement_response_running" : {
                "summary" : "Call mode: asynchronous. Submission is accepted",
                "value" : {
                  "statement_id" : "01ed9db9-24c4-1cb6-a320-fb6ebbe7410d",
                  "status" : {
                    "state" : "RUNNING"
                  }
                }
              }
            }
          }
        },
        "description" : ""
      },
      "sql.Success" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.Success"
            }
          }
        },
        "description" : "Generic success message"
      },
      "sql.TemporarilyUnavailable" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.Error"
            },
            "example" : {
              "error_code" : "TEMPORARILY_UNAVAILABLE",
              "message" : "The service is temporarily unavailable. Try again later."
            }
          }
        },
        "description" : "Temporarily unavailable due to a server error."
      },
      "tokenmanagement.FeatureDisabled" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/tokenmanagement.Error"
            },
            "examples" : {
              "Feature disabled" : {
                "value" : {
                  "error_code" : "FEATURE_DISABLED",
                  "message" : "<feature-name> is not enabled for this feature tier."
                }
              },
              "feature_disabled" : {
                "value" : {
                  "error_code" : "FEATURE_DISABLED",
                  "message" : "On-behalf-of token creation for service principals is not enabled for this workspace."
                },
                "x-databricks-cloud" : "aws"
              }
            }
          }
        },
        "description" : "The requested feature is not available."
      },
      "tokenmanagement.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/tokenmanagement.Error"
            }
          }
        },
        "description" : "The request is not handled correctly due to a server error."
      },
      "tokenmanagement.NotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/tokenmanagement.Error"
            },
            "examples" : {
              "Feature disabled" : {
                "value" : {
                  "error_code" : "FEATURE_DISABLED",
                  "message" : "<feature-name> is not enabled for this feature tier."
                }
              },
              "Resource does not exist" : {
                "value" : {
                  "error_code" : "RESOURCE_DOES_NOT_EXIST",
                  "message" : "Token with ID <token-id> does not exist."
                }
              }
            }
          }
        },
        "description" : "The requested resource does not exist."
      },
      "tokenmanagement.OboBadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/tokenmanagement.Error"
            },
            "examples" : {
              "invalid_parameter_value_not_found" : {
                "value" : {
                  "error_code" : "INVALID_PARAMETER_VALUE",
                  "message" : "<application-id> does not exist."
                }
              },
              "invalid_parameter_value_not_sp" : {
                "value" : {
                  "error_code" : "INVALID_PARAMETER_VALUE",
                  "message" : "User <id> is not a service principal."
                }
              },
              "invalid_parameter_value_token_lifetime" : {
                "value" : {
                  "error_code" : "INVALID_PARAMETER_VALUE",
                  "message" : "Token lifetime must be less than or equal to the workspace max of <max-lifetime>."
                }
              }
            }
          }
        },
        "description" : "The request is malformed.",
        "x-databricks-cloud" : "aws"
      },
      "tokenmanagement.Unauthorized" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/tokenmanagement.Error"
            },
            "example" : {
              "error_code" : "PERMISSION_DENIED",
              "message" : "Only Admins can access token management APIs."
            }
          }
        },
        "description" : "The request is unauthorized."
      },
      "unitycatalog.BadRequest" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.Error"
            }
          }
        },
        "description" : "The request is malformed."
      },
      "unitycatalog.Conflict" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.Error"
            }
          }
        },
        "description" : "The request conflicts with the current state of the target resource."
      },
      "unitycatalog.Forbidden" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.Error"
            }
          }
        },
        "description" : "The request is forbidden from being fulfilled."
      },
      "unitycatalog.InternalError" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.Error"
            }
          }
        },
        "description" : "The request is not handled correctly due to a server error."
      },
      "unitycatalog.NotFound" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.Error"
            }
          }
        },
        "description" : "The requested resource does not exist."
      },
      "unitycatalog.ServiceUnavailable" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.Error"
            }
          }
        },
        "description" : "The service is unavailable."
      },
      "unitycatalog.Unauthenticated" : {
        "content" : {
          "application/json" : {
            "schema" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.Error"
            }
          }
        },
        "description" : "The request is unauthenticated. The user's credentials are missing or incorrect."
      }
    },
    "schemas" : {
      "billing.Budget" : {
        "required" : [ "name", "start_date", "filter", "target_amount", "period" ],
        "properties" : {
          "name" : {
            "description" : "Human-readable name of the budget.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "start_date" : {
            "format" : "date",
            "description" : "Start date of the budget period calculation.",
            "type" : "string"
          },
          "filter" : {
            "example" : "workspaceId=123 OR (sku='STANDARD_ALL_PURPOSE_COMPUTE' AND tag.'my tag'='my value')",
            "description" : " SQL-like filter expression with workspaceId, SKU and tag. Usage in your account that matches this expression will be counted in this budget.\n\nSupported properties on left-hand side of comparison:\n * `workspaceId` - the ID of the workspace\n * `sku` - SKU of the cluster, e.g. `STANDARD_ALL_PURPOSE_COMPUTE` \n * `tag.tagName`, `tag.'tag name'` - tag of the cluster \n\nSupported comparison operators:\n * `=` - equal \n * `!=` - not equal \n\nSupported logical operators: `AND`, `OR`.\n\nExamples:\n * `workspaceId=123 OR (sku='STANDARD_ALL_PURPOSE_COMPUTE' AND tag.'my tag'='my value')`\n * `workspaceId!=456`\n * `sku='STANDARD_ALL_PURPOSE_COMPUTE' OR sku='PREMIUM_ALL_PURPOSE_COMPUTE'`\n * `tag.name1='value1' AND tag.name2='value2'`\n ",
            "type" : "string"
          },
          "end_date" : {
            "format" : "date",
            "description" : "Optional end date of the budget.",
            "type" : "string"
          },
          "target_amount" : {
            "example" : "1234.56",
            "description" : "Target amount of the budget per period in USD.",
            "type" : "string"
          },
          "alerts" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.BudgetAlert"
            }
          },
          "period" : {
            "example" : "1 month",
            "description" : " Period length in years, months, weeks and/or days.\n Examples: `1 month`, `30 days`, `1 year, 2 months, 1 week, 2 days`\n ",
            "type" : "string"
          }
        },
        "description" : "Budget configuration to be created.",
        "type" : "object"
      },
      "billing.BudgetAlert" : {
        "properties" : {
          "email_notifications" : {
            "description" : "List of email addresses to be notified when budget percentage is exceeded in the given period.",
            "type" : "array",
            "items" : {
              "example" : "foo@bar.com",
              "type" : "string"
            }
          },
          "min_percentage" : {
            "maximum" : 100000,
            "minimum" : 1,
            "description" : "Percentage of the target amount used in the currect period that will trigger a notification.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "billing.BudgetList" : {
        "properties" : {
          "budgets" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.BudgetWithStatus"
            }
          }
        },
        "description" : "List of budgets.",
        "type" : "object"
      },
      "billing.BudgetWithStatus" : {
        "properties" : {
          "name" : {
            "description" : "Human-readable name of the budget.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "creation_time" : {
            "format" : "date-time",
            "type" : "string"
          },
          "start_date" : {
            "format" : "date",
            "description" : "Start date of the budget period calculation.",
            "type" : "string"
          },
          "filter" : {
            "example" : "workspaceId=123 OR (sku='STANDARD_ALL_PURPOSE_COMPUTE' AND tag.'my tag'='my value')",
            "description" : " SQL-like filter expression with workspaceId, SKU and tag. Usage in your account that matches this expression will be counted in this budget.\n\nSupported properties on left-hand side of comparison:\n * `workspaceId` - the ID of the workspace\n * `sku` - SKU of the cluster, e.g. `STANDARD_ALL_PURPOSE_COMPUTE` \n * `tag.tagName`, `tag.'tag name'` - tag of the cluster \n\nSupported comparison operators:\n * `=` - equal \n * `!=` - not equal \n\nSupported logical operators: `AND`, `OR`.\n\nExamples:\n * `workspaceId=123 OR (sku='STANDARD_ALL_PURPOSE_COMPUTE' AND tag.'my tag'='my value')`\n * `workspaceId!=456`\n * `sku='STANDARD_ALL_PURPOSE_COMPUTE' OR sku='PREMIUM_ALL_PURPOSE_COMPUTE'`\n * `tag.name1='value1' AND tag.name2='value2'`\n ",
            "type" : "string"
          },
          "budget_id" : {
            "format" : "uuid",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "status_daily" : {
            "description" : "Amount used in the budget for each day (noncumulative).",
            "type" : "array",
            "items" : {
              "properties" : {
                "amount" : {
                  "example" : "123.45",
                  "description" : "Amount used in this day in USD.",
                  "type" : "string"
                },
                "date" : {
                  "format" : "date",
                  "type" : "string"
                }
              },
              "type" : "object"
            }
          },
          "end_date" : {
            "format" : "date",
            "description" : "Optional end date of the budget.",
            "type" : "string"
          },
          "target_amount" : {
            "example" : "1234.56",
            "description" : "Target amount of the budget per period in USD.",
            "type" : "string"
          },
          "alerts" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.BudgetAlert"
            }
          },
          "update_time" : {
            "format" : "date-time",
            "type" : "string"
          },
          "period" : {
            "example" : "1 month",
            "description" : " Period length in years, months, weeks and/or days.\n Examples: `1 month`, `30 days`, `1 year, 2 months, 1 week, 2 days`\n ",
            "type" : "string"
          }
        },
        "description" : "Budget configuration with daily status.",
        "type" : "object"
      },
      "billing.CreateLogDeliveryConfigurationParams" : {
        "required" : [ "log_type", "output_format", "credentials_id", "storage_configuration_id" ],
        "properties" : {
          "workspace_ids_filter" : {
            "description" : "Optional filter that specifies workspace IDs to deliver logs for. By default the workspace filter is empty and log delivery applies at the account level, delivering workspace-level logs for all workspaces in your account, plus account level logs. You can optionally set this field to an array of workspace IDs (each one is an `int64`) to which log delivery should apply, in which case only workspace-level logs relating to the specified workspaces are delivered.\nIf you plan to use different log delivery configurations for different workspaces, set this field explicitly. Be aware that delivery configurations mentioning specific workspaces won't apply to new workspaces created in the future, and delivery won't include account level logs.\nFor some types of Databricks deployments there is only one workspace per account ID, so this field is unnecessary.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.WorkspaceId"
            }
          },
          "delivery_path_prefix" : {
            "description" : "The optional delivery path prefix within Amazon S3 storage. Defaults to empty, which means that logs are delivered to the root of the bucket. This must be a valid S3 object key. This must not start or end with a slash character.",
            "type" : "string"
          },
          "credentials_id" : {
            "format" : "uuid",
            "description" : "The ID for a method:credentials/create that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page. See [Configure billable usage delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).",
            "type" : "string"
          },
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.LogDeliveryConfigStatus"
          },
          "storage_configuration_id" : {
            "format" : "uuid",
            "description" : "\"The ID for a method:storage/create  that represents the S3 bucket with bucket policy as described in the main billable usage documentation page. See [Configure billable usage delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).\"",
            "type" : "string"
          },
          "output_format" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.OutputFormat"
          },
          "log_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.LogType"
          },
          "delivery_start_time" : {
            "description" : "This field applies only if `log_type` is `BILLABLE_USAGE`. This is the optional start month and year for delivery, specified in `YYYY-MM` format. Defaults to current year and month.  `BILLABLE_USAGE` logs are not available for usage before March 2019 (`2019-03`).",
            "type" : "string"
          },
          "config_name" : {
            "description" : "The optional human-readable name of the log delivery configuration. Defaults to empty.",
            "x-databricks-name" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "billing.DeliveryStatus" : {
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "CREATED", "SUCCEEDED", "USER_FAILURE", "SYSTEM_FAILURE", "NOT_FOUND" ],
        "x-databricks-enum-descriptions" : {
          "SYSTEM_FAILURE" : "The latest attempt of log delivery failed because of an Databricks internal error. Contact support if it doesn't go away soon.",
          "CREATED" : "There were no log delivery attempts since the config was created.",
          "SUCCEEDED" : "The latest attempt of log delivery has succeeded completely.",
          "USER_FAILURE" : "The latest attempt of log delivery failed because of misconfiguration of customer provided permissions on role or storage.",
          "NOT_FOUND" : "The log delivery status as the configuration has been disabled since the release of this feature or there are no workspaces in the account."
        }
      },
      "billing.LogDeliveryConfigStatus" : {
        "description" : "Status of log delivery configuration. Set to `ENABLED` (enabled) or `DISABLED` (disabled). Defaults to `ENABLED`. You can [enable or disable the configuration](#operation/patch-log-delivery-config-status) later. Deletion of a configuration is not supported, so disable a log delivery configuration that is no longer needed.",
        "type" : "string",
        "enum" : [ "ENABLED", "DISABLED" ]
      },
      "billing.LogDeliveryConfiguration" : {
        "properties" : {
          "workspace_ids_filter" : {
            "description" : "Optional filter that specifies workspace IDs to deliver logs for. By default the workspace filter is empty and log delivery applies at the account level, delivering workspace-level logs for all workspaces in your account, plus account level logs. You can optionally set this field to an array of workspace IDs (each one is an `int64`) to which log delivery should apply, in which case only workspace-level logs relating to the specified workspaces are delivered.\nIf you plan to use different log delivery configurations for different workspaces, set this field explicitly. Be aware that delivery configurations mentioning specific workspaces won't apply to new workspaces created in the future, and delivery won't include account level logs.\nFor some types of Databricks deployments there is only one workspace per account ID, so this field is unnecessary.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.WorkspaceId"
            }
          },
          "creation_time" : {
            "format" : "int64",
            "description" : "Time in epoch milliseconds when the log delivery configuration was created.",
            "type" : "integer"
          },
          "config_id" : {
            "format" : "uuid",
            "description" : "Databricks log delivery configuration ID.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "delivery_path_prefix" : {
            "description" : "The optional delivery path prefix within Amazon S3 storage. Defaults to empty, which means that logs are delivered to the root of the bucket. This must be a valid S3 object key. This must not start or end with a slash character.",
            "type" : "string"
          },
          "log_delivery_status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.LogDeliveryStatus"
          },
          "account_id" : {
            "format" : "uuid",
            "description" : "The Databricks account ID that hosts the log delivery configuration.",
            "type" : "string"
          },
          "credentials_id" : {
            "format" : "uuid",
            "description" : "The ID for a method:credentials/create that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page. See [Configure billable usage delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).",
            "type" : "string"
          },
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.LogDeliveryConfigStatus"
          },
          "storage_configuration_id" : {
            "format" : "uuid",
            "description" : "\"The ID for a method:storage/create  that represents the S3 bucket with bucket policy as described in the main billable usage documentation page. See [Configure billable usage delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).\"",
            "type" : "string"
          },
          "output_format" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.OutputFormat"
          },
          "log_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.LogType"
          },
          "delivery_start_time" : {
            "description" : "This field applies only if `log_type` is `BILLABLE_USAGE`. This is the optional start month and year for delivery, specified in `YYYY-MM` format. Defaults to current year and month.  `BILLABLE_USAGE` logs are not available for usage before March 2019 (`2019-03`).",
            "type" : "string"
          },
          "config_name" : {
            "description" : "The optional human-readable name of the log delivery configuration. Defaults to empty.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "update_time" : {
            "format" : "int64",
            "description" : "Time in epoch milliseconds when the log delivery configuration was updated.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "billing.LogDeliveryStatus" : {
        "properties" : {
          "last_attempt_time" : {
            "format" : "date-time",
            "description" : "The UTC time for the latest log delivery attempt.",
            "type" : "string"
          },
          "last_successful_attempt_time" : {
            "format" : "date-time",
            "description" : "The UTC time for the latest successful log delivery.",
            "type" : "string"
          },
          "message" : {
            "description" : "Informative message about the latest log delivery attempt. If the log delivery fails with USER_FAILURE, error details will be provided for fixing misconfigurations in cloud permissions.",
            "type" : "string"
          },
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.DeliveryStatus"
          }
        },
        "description" : "Databricks log delivery status.",
        "type" : "object"
      },
      "billing.LogType" : {
        "description" : "Log delivery type. Supported values are:\n\n* `BILLABLE_USAGE`  Configure [billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html). For the CSV schema, see the [View billable usage](https://docs.databricks.com/administration-guide/account-settings/usage.html).\n\n* `AUDIT_LOGS`  Configure [audit log delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html). For the JSON schema, see [Configure audit logging](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html)",
        "type" : "string",
        "enum" : [ "BILLABLE_USAGE", "AUDIT_LOGS" ]
      },
      "billing.OutputFormat" : {
        "description" : "The file type of log delivery.\n\n*  If `log_type` is `BILLABLE_USAGE`, this value must be `CSV`. Only the CSV (comma-separated values) format is supported. For the schema, see the [View billable usage](https://docs.databricks.com/administration-guide/account-settings/usage.html)\n* If `log_type` is `AUDIT_LOGS`, this value must be `JSON`. Only the JSON (JavaScript Object Notation) format is supported. For the schema, see the [Configuring audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).",
        "type" : "string",
        "enum" : [ "CSV", "JSON" ]
      },
      "billing.UpdateLogDeliveryConfigurationStatusRequest" : {
        "required" : [ "status" ],
        "properties" : {
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.LogDeliveryConfigStatus"
          }
        },
        "type" : "object"
      },
      "billing.UsageDownloadMonth" : {
        "description" : "Format specification for month in the format `YYYY-MM`. This is used to specify billable usage `start_month` and `end_month` properties. **Note**: Billable usage logs are unavailable before March 2019 (`2019-03`).",
        "type" : "string"
      },
      "billing.WorkspaceId" : {
        "format" : "int64",
        "type" : "integer"
      },
      "billing.WrappedBudget" : {
        "required" : [ "budget" ],
        "properties" : {
          "budget" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.Budget"
          }
        },
        "type" : "object"
      },
      "billing.WrappedBudgetWithStatus" : {
        "required" : [ "budget" ],
        "properties" : {
          "budget" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.BudgetWithStatus"
          }
        },
        "type" : "object"
      },
      "billing.WrappedCreateLogDeliveryConfiguration" : {
        "properties" : {
          "log_delivery_configuration" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.CreateLogDeliveryConfigurationParams"
          }
        },
        "type" : "object"
      },
      "billing.WrappedLogDeliveryConfiguration" : {
        "properties" : {
          "log_delivery_configuration" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/billing.LogDeliveryConfiguration"
          }
        },
        "type" : "object"
      },
      "billing.WrappedLogDeliveryConfigurations" : {
        "properties" : {
          "log_delivery_configurations" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/billing.LogDeliveryConfiguration"
            }
          }
        },
        "type" : "object"
      },
      "clusterpolicies.CreatePolicy" : {
        "required" : [ "name" ],
        "properties" : {
          "max_clusters_per_user" : {
            "minimum" : 1,
            "format" : "int64",
            "description" : "Max number of clusters per user that can be active using this policy. If not present, there is no max limit.",
            "type" : "integer",
            "x-databricks-preview" : "PRIVATE"
          },
          "name" : {
            "description" : "Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100\ncharacters.",
            "type" : "string"
          },
          "policy_family_definition_overrides" : {
            "example" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
            "format" : "JSON",
            "description" : "Policy definition JSON document expressed in Databricks Policy Definition Language.\nThe JSON document must be passed as a string and cannot be embedded in the requests.\n\nYou can use this to customize the policy definition inherited from the policy family.\nPolicy rules specified here are merged into the inherited policy definition.\n",
            "type" : "string",
            "x-databricks-not-cloud" : "gcp"
          },
          "description" : {
            "description" : "Additional human-readable description of the cluster policy.",
            "type" : "string",
            "x-databricks-not-cloud" : "gcp"
          },
          "policy_family_id" : {
            "description" : "ID of the policy family. The cluster policy's policy definition inherits the policy\nfamily's policy definition.\n\nCannot be used with `definition`. Use `policy_family_definition_overrides` instead to\ncustomize the policy definition.\n",
            "type" : "string",
            "x-databricks-not-cloud" : "gcp"
          },
          "definition" : {
            "example" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
            "format" : "JSON",
            "description" : "Policy definition document expressed in Databricks Cluster Policy Definition Language.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusterpolicies.CreatePolicyResponse" : {
        "properties" : {
          "policy_id" : {
            "description" : "Canonical unique identifier for the cluster policy.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusterpolicies.DeletePolicy" : {
        "required" : [ "policy_id" ],
        "properties" : {
          "policy_id" : {
            "description" : "The ID of the policy to delete.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusterpolicies.DeletePolicyResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "clusterpolicies.EditPolicy" : {
        "required" : [ "policy_id", "name" ],
        "properties" : {
          "max_clusters_per_user" : {
            "minimum" : 1,
            "format" : "int64",
            "description" : "Max number of clusters per user that can be active using this policy. If not present, there is no max limit.",
            "type" : "integer",
            "x-databricks-preview" : "PRIVATE"
          },
          "name" : {
            "description" : "Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100\ncharacters.",
            "type" : "string"
          },
          "policy_id" : {
            "description" : "The ID of the policy to update.",
            "type" : "string"
          },
          "policy_family_definition_overrides" : {
            "example" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
            "format" : "JSON",
            "description" : "Policy definition JSON document expressed in Databricks Policy Definition Language.\nThe JSON document must be passed as a string and cannot be embedded in the requests.\n\nYou can use this to customize the policy definition inherited from the policy family.\nPolicy rules specified here are merged into the inherited policy definition.\n",
            "type" : "string",
            "x-databricks-not-cloud" : "gcp"
          },
          "description" : {
            "description" : "Additional human-readable description of the cluster policy.",
            "type" : "string",
            "x-databricks-not-cloud" : "gcp"
          },
          "policy_family_id" : {
            "description" : "ID of the policy family. The cluster policy's policy definition inherits the policy\nfamily's policy definition.\n\nCannot be used with `definition`. Use `policy_family_definition_overrides` instead to\ncustomize the policy definition.\n",
            "type" : "string",
            "x-databricks-not-cloud" : "gcp"
          },
          "definition" : {
            "example" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
            "format" : "JSON",
            "description" : "Policy definition document expressed in Databricks Cluster Policy Definition Language.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusterpolicies.EditPolicyResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "clusterpolicies.GetPolicyFamilyResponse" : {
        "extRef" : true,
        "ref" : true,
        "x-databricks-not-cloud" : "gcp",
        "$ref" : "#/components/schemas/clusterpolicies.PolicyFamily"
      },
      "clusterpolicies.ListPoliciesResponse" : {
        "properties" : {
          "policies" : {
            "description" : "List of policies.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusterpolicies.Policy"
            }
          }
        },
        "type" : "object"
      },
      "clusterpolicies.ListPolicyFamiliesResponse" : {
        "required" : [ "policy_families" ],
        "properties" : {
          "next_page_token" : {
            "description" : "A token that can be used to get the next page of results. If not present, there are no more results to show.",
            "type" : "string"
          },
          "policy_families" : {
            "description" : "List of policy families.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusterpolicies.PolicyFamily"
            }
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "clusterpolicies.Policy" : {
        "properties" : {
          "max_clusters_per_user" : {
            "minimum" : 1,
            "format" : "int64",
            "description" : "Max number of clusters per user that can be active using this policy. If not present, there is no max limit.",
            "type" : "integer",
            "x-databricks-preview" : "PRIVATE"
          },
          "name" : {
            "description" : "Cluster Policy name requested by the user. This has to be unique. Length must be between 1 and 100\ncharacters.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "policy_id" : {
            "description" : "Canonical unique identifier for the Cluster Policy.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "is_default" : {
            "description" : "If true, policy is a default policy created and managed by Databricks. Default policies cannot be deleted, and their policy families cannot be changed.",
            "type" : "boolean",
            "x-databricks-not-cloud" : "gcp"
          },
          "policy_family_definition_overrides" : {
            "example" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
            "format" : "JSON",
            "description" : "Policy definition JSON document expressed in Databricks Policy Definition Language.\nThe JSON document must be passed as a string and cannot be embedded in the requests.\n\nYou can use this to customize the policy definition inherited from the policy family.\nPolicy rules specified here are merged into the inherited policy definition.\n",
            "type" : "string",
            "x-databricks-not-cloud" : "gcp"
          },
          "description" : {
            "description" : "Additional human-readable description of the cluster policy.",
            "type" : "string",
            "x-databricks-not-cloud" : "gcp"
          },
          "policy_family_id" : {
            "description" : "ID of the policy family.",
            "type" : "string",
            "x-databricks-not-cloud" : "gcp"
          },
          "creator_user_name" : {
            "description" : "Creator user name.\nThe field won't be included in the response if the user has already been deleted.",
            "x-databricks-computed" : true,
            "type" : "string"
          },
          "definition" : {
            "example" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
            "format" : "JSON",
            "description" : "Policy definition document expressed in Databricks Cluster Policy Definition Language.",
            "type" : "string"
          },
          "created_at_timestamp" : {
            "format" : "int64",
            "description" : "Creation time. The timestamp (in millisecond) when this Cluster Policy was created.",
            "x-databricks-computed" : true,
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "clusterpolicies.PolicyFamily" : {
        "required" : [ "policy_family_id", "name", "description", "definition" ],
        "properties" : {
          "definition" : {
            "example" : "{ \"custom_tags.test_tag\": { \"type\": \"fixed\", \"value\": \"test_value\" } }\n",
            "format" : "JSON",
            "description" : "Policy definition document expressed in Databricks Cluster Policy Definition Language.",
            "type" : "string"
          },
          "description" : {
            "description" : "Human-readable description of the purpose of the policy family.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of the policy family.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "policy_family_id" : {
            "description" : "ID of the policy family.",
            "type" : "string",
            "x-databricks-not-cloud" : "gcp"
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "clusterpolicies.policy_family_id" : {
        "description" : "ID of the policy family.",
        "type" : "string",
        "x-databricks-not-cloud" : "gcp"
      },
      "clusters.AddInstanceProfile" : {
        "required" : [ "instance_profile_arn" ],
        "properties" : {
          "iam_role_arn" : {
            "example" : "arn:aws:iam::<account-id>:role/<name>",
            "description" : "The AWS IAM role ARN of the role associated with the instance profile.\nThis field is required if your role name and instance profile name do\nnot match and you want to use the instance profile with\n[Databricks SQL Serverless](https://docs.databricks.com/sql/admin/serverless.html).\n\nOtherwise, this field is optional.\n",
            "type" : "string"
          },
          "instance_profile_arn" : {
            "example" : "arn:aws:iam::<account-id>:instance-profile/<name>",
            "description" : "The AWS ARN of the instance profile to register with Databricks. This field is required.",
            "type" : "string"
          },
          "is_meta_instance_profile" : {
            "description" : "By default, Databricks validates that it has sufficient permissions to launch\ninstances with the instance profile. This validation uses AWS dry-run mode for\nthe RunInstances API. If validation fails with an error message that does not\nindicate an IAM related permission issue, (e.g. `Your requested instance type\nis not supported in your requested availability zone`), you can pass this flag\nto skip the validation and forcibly add the instance profile.\n",
            "type" : "boolean"
          },
          "skip_validation" : {
            "description" : "By default, Databricks validates that it has sufficient permissions to launch\ninstances with the instance profile. This validation uses AWS dry-run mode for\nthe RunInstances API. If validation fails with an error message that does not\nindicate an IAM related permission issue,\n(e.g. Your requested instance type is not supported in your requested availability zone),\nyou can pass this flag to skip the validation and forcibly add the instance profile.\n",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "clusters.AutoScale" : {
        "required" : [ "min_workers", "max_workers" ],
        "properties" : {
          "max_workers" : {
            "format" : "int32",
            "description" : "The maximum number of workers to which the cluster can scale up when overloaded.\nNote that `max_workers` must be strictly greater than `min_workers`.",
            "type" : "integer"
          },
          "min_workers" : {
            "format" : "int32",
            "description" : "The minimum number of workers to which the cluster can scale down when underutilized.\nIt is also the initial number of workers the cluster will have after creation.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "clusters.AwsAttributes" : {
        "properties" : {
          "ebs_volume_count" : {
            "default" : "0",
            "format" : "int32",
            "description" : "The number of volumes launched for each instance. Users can choose up to 10 volumes.\nThis feature is only enabled for supported node types. Legacy node types cannot specify\ncustom EBS volumes.\nFor node types with no instance store, at least one EBS volume needs to be specified;\notherwise, cluster creation will fail.\n\nThese EBS volumes will be mounted at `/ebs0`, `/ebs1`, and etc.\nInstance store volumes will be mounted at `/local_disk0`, `/local_disk1`, and etc.\n\nIf EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for\nscratch storage because heterogenously sized scratch devices can lead to inefficient disk\nutilization. If no EBS volumes are attached, Databricks will configure Spark to use instance\nstore volumes.\n\nPlease note that if EBS volumes are specified, then the Spark configuration `spark.local.dir`\nwill be overridden.",
            "type" : "integer"
          },
          "availability" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.AwsAvailability"
          },
          "instance_profile_arn" : {
            "description" : "Nodes for this cluster will only be placed on AWS instances with this instance profile. If\nommitted, nodes will be placed on instances without an IAM instance profile. The instance\nprofile must have previously been added to the Databricks environment by an account\nadministrator.\n\nThis feature may only be available to certain customer plans.\n\nIf this field is ommitted, we will pull in the default from the conf if it exists.",
            "type" : "string"
          },
          "first_on_demand" : {
            "default" : "0",
            "format" : "int32",
            "description" : "The first `first_on_demand` nodes of the cluster will be placed on on-demand instances.\nIf this value is greater than 0, the cluster driver node in particular will be placed on an\non-demand instance. If this value is greater than or equal to the current cluster size, all\nnodes will be placed on on-demand instances. If this value is less than the current cluster\nsize, `first_on_demand` nodes will be placed on on-demand instances and the remainder will\nbe placed on `availability` instances. Note that this value does not affect\ncluster size and cannot currently be mutated over the lifetime of a cluster.",
            "type" : "integer"
          },
          "ebs_volume_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.EbsVolumeType"
          },
          "spot_bid_price_percent" : {
            "default" : "100",
            "format" : "int32",
            "description" : "The bid price for AWS spot instances, as a percentage of the corresponding instance type's\non-demand price.\nFor example, if this field is set to 50, and the cluster needs a new `r3.xlarge` spot\ninstance, then the bid price is half of the price of\non-demand `r3.xlarge` instances. Similarly, if this field is set to 200, the bid price is twice\nthe price of on-demand `r3.xlarge` instances. If not specified, the default value is 100.\nWhen spot instances are requested for this cluster, only spot instances whose bid price\npercentage matches this field will be considered.\nNote that, for safety, we enforce this field to be no more than 10000.\n\nThe default value and documentation here should be kept consistent with\nCommonConf.defaultSpotBidPricePercent and CommonConf.maxSpotBidPricePercent.",
            "type" : "integer"
          },
          "ebs_volume_throughput" : {
            "format" : "int32",
            "description" : "<needs content added>",
            "type" : "integer"
          },
          "zone_id" : {
            "description" : "Identifier for the availability zone/datacenter in which the cluster resides.\nThis string will be of a form like \"us-west-2a\". The provided availability\nzone must be in the same region as the Databricks deployment. For example, \"us-west-2a\"\nis not a valid zone id if the Databricks deployment resides in the \"us-east-1\" region.\nThis is an optional field at cluster creation, and if not specified, a default zone will be used.\nIf the zone specified is \"auto\", will try to place cluster in a zone with high availability,\nand will retry placement in a different AZ if there is not enough capacity.\nSee [[AutoAZHelper.scala]] for more details.\nThe list of available zones as well as the default value can be found by using the\n`List Zones`_ method.",
            "type" : "string"
          },
          "ebs_volume_size" : {
            "format" : "int32",
            "description" : "The size of each EBS volume (in GiB) launched for each instance. For general purpose\nSSD, this value must be within the range 100 - 4096. For throughput optimized HDD,\nthis value must be within the range 500 - 4096.",
            "type" : "integer"
          },
          "ebs_volume_iops" : {
            "format" : "int32",
            "description" : "<needs content added>",
            "type" : "integer"
          }
        },
        "type" : "object",
        "x-databricks-cloud" : "aws"
      },
      "clusters.AwsAvailability" : {
        "default" : "SPOT_WITH_FALLBACK",
        "description" : "Availability type used for all subsequent nodes past the `first_on_demand` ones.\n\nNote: If `first_on_demand` is zero, this availability type will be used for the entire cluster.\n",
        "type" : "string",
        "enum" : [ "SPOT", "ON_DEMAND", "SPOT_WITH_FALLBACK" ]
      },
      "clusters.AzureAttributes" : {
        "properties" : {
          "availability" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.AzureAvailability"
          },
          "first_on_demand" : {
            "default" : "1",
            "format" : "int32",
            "description" : "The first `first_on_demand` nodes of the cluster will be placed on on-demand instances.\nThis value should be greater than 0, to make sure the cluster driver node is placed on an\non-demand instance. If this value is greater than or equal to the current cluster size, all\nnodes will be placed on on-demand instances. If this value is less than the current cluster\nsize, `first_on_demand` nodes will be placed on on-demand instances and the remainder will\nbe placed on `availability` instances. Note that this value does not affect\ncluster size and cannot currently be mutated over the lifetime of a cluster.",
            "type" : "integer"
          },
          "log_analytics_info" : {
            "description" : "Defines values necessary to configure and run Azure Log Analytics agent",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.LogAnalyticsInfo"
          },
          "spot_bid_max_price" : {
            "default" : "-1.0",
            "format" : "double",
            "description" : "The max bid price to be used for Azure spot instances.\nThe Max price for the bid cannot be higher than the on-demand price of the instance.\nIf not specified, the default value is -1, which specifies that the instance cannot be evicted\non the basis of price, and only on the basis of availability. Further, the value should > 0 or -1.",
            "type" : "number"
          }
        },
        "type" : "object",
        "x-databricks-cloud" : "azure"
      },
      "clusters.AzureAvailability" : {
        "default" : "ON_DEMAND_AZURE",
        "description" : "Availability type used for all subsequent nodes past the `first_on_demand` ones.\nNote: If `first_on_demand` is zero (which only happens on pool clusters), this availability\ntype will be used for the entire cluster.",
        "type" : "string",
        "enum" : [ "SPOT_AZURE", "ON_DEMAND_AZURE", "SPOT_WITH_FALLBACK_AZURE" ]
      },
      "clusters.BaseClusterInfo" : {
        "properties" : {
          "workload_type" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.WorkloadType"
          },
          "cluster_name" : {
            "description" : "Cluster name requested by the user. This doesn't have to be unique.\nIf not specified at creation, the cluster name will be an empty string.\n",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "ssh_public_keys" : {
            "description" : "SSH public key contents that will be added to each Spark node in this cluster. The\ncorresponding private keys can be used to login with the user name `ubuntu` on port `2200`.\nUp to 10 keys can be specified.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "policy_id" : {
            "description" : "The ID of the cluster policy used to create the cluster if applicable.",
            "type" : "string"
          },
          "node_type_id" : {
            "description" : "This field encodes, through a single value, the resources available to each of\nthe Spark nodes in this cluster. For example, the Spark nodes can be provisioned\nand optimized for memory or compute intensive workloads. A list of available node\ntypes can be retrieved by using the :method:clusters/listNodeTypes API call.\n",
            "type" : "string"
          },
          "spark_version" : {
            "description" : "The Spark version of the cluster, e.g. `3.3.x-scala2.11`.\nA list of available Spark versions can be retrieved by using\nthe :method:clusters/sparkVersions API call.\n",
            "type" : "string"
          },
          "runtime_engine" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.RuntimeEngine"
          },
          "num_workers" : {
            "format" : "int32",
            "description" : "Number of worker nodes that this cluster should have. A cluster has one Spark Driver\nand `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.\n\nNote: When reading the properties of a cluster, this field reflects the desired number\nof workers rather than the actual current number of workers. For instance, if a cluster\nis resized from 5 to 10 workers, this field will immediately be updated to reflect\nthe target size of 10 workers, whereas the workers listed in `spark_info` will gradually\nincrease from 5 to 10 as the new nodes are provisioned.",
            "type" : "integer"
          },
          "driver_instance_pool_id" : {
            "description" : "The optional ID of the instance pool for the driver of the cluster belongs.\nThe pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not\nassigned.",
            "type" : "string"
          },
          "enable_local_disk_encryption" : {
            "description" : "Whether to enable LUKS on cluster VMs' local disks",
            "type" : "boolean"
          },
          "custom_tags" : {
            "properties" : { },
            "description" : "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS\ninstances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n- Currently, Databricks allows at most 45 custom tags\n\n- Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length must be less than or equal to 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          },
          "autoscale" : {
            "description" : "Parameters needed in order to automatically scale clusters up and down based on load.\nNote: autoscaling works best with DB runtime versions 3.0 or later.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.AutoScale"
          },
          "spark_conf" : {
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified Spark configuration key-value pairs.\nUsers can also pass in a string of extra JVM options to the driver and the executors via\n`spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively.\n",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "driver_node_type_id" : {
            "description" : "The node type of the Spark driver. Note that this field is optional;\nif unset, the driver node type will be set as the same value\nas `node_type_id` defined above.\n",
            "type" : "string"
          },
          "instance_pool_id" : {
            "description" : "The optional ID of the instance pool to which the cluster belongs.",
            "type" : "string"
          },
          "cluster_source" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterSource"
          },
          "gcp_attributes" : {
            "description" : "Attributes related to clusters running on Google Cloud Platform.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/clusters.GcpAttributes"
          },
          "azure_attributes" : {
            "description" : "Attributes related to clusters running on Microsoft Azure.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/clusters.AzureAttributes"
          },
          "aws_attributes" : {
            "description" : "Attributes related to clusters running on Amazon Web Services.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/clusters.AwsAttributes"
          },
          "spark_env_vars" : {
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified environment variable key-value pairs.\nPlease note that key-value pair of the form (X,Y) will be exported as is (i.e.,\n`export X='Y'`) while launching the driver and workers.\n\nIn order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending\nthem to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all\ndefault databricks managed environmental variables are included as well.\n\nExample Spark environment variables:\n`{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or\n`{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "autotermination_minutes" : {
            "format" : "int32",
            "description" : "Automatically terminates the cluster after it is inactive for this time in minutes. If not set,\nthis cluster will not be automatically terminated. If specified, the threshold must be between\n10 and 10000 minutes.\nUsers can also set this value to 0 to explicitly disable automatic termination.",
            "type" : "integer"
          },
          "cluster_log_conf" : {
            "description" : "The configuration for delivering spark logs to a long-term storage destination.\nTwo kinds of destinations (dbfs and s3) are supported. Only one destination can be specified\nfor one cluster. If the conf is given, the logs will be delivered to the destination every\n`5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while\nthe destination of executor logs is `$destination/$clusterId/executor`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterLogConf"
          },
          "enable_elastic_disk" : {
            "description" : "Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk\nspace when its Spark workers are running low on disk space. This feature requires specific AWS\npermissions to function correctly - refer to the User Guide for more details.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "clusters.ChangeClusterOwner" : {
        "required" : [ "cluster_id", "owner_username" ],
        "properties" : {
          "cluster_id" : {
            "description" : "<needs content added>",
            "type" : "string"
          },
          "owner_username" : {
            "description" : "New owner of the cluster_id after this RPC.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.ChangeClusterOwnerResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "clusters.ClientsTypes" : {
        "properties" : {
          "jobs" : {
            "default" : "true",
            "description" : "With jobs set, the cluster can be used for jobs",
            "type" : "boolean"
          },
          "notebooks" : {
            "default" : "true",
            "description" : "With notebooks set, this cluster can be used for notebooks",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "clusters.CloudProviderNodeInfo" : {
        "properties" : {
          "status" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusters.CloudProviderNodeStatus"
            }
          }
        },
        "type" : "object"
      },
      "clusters.CloudProviderNodeStatus" : {
        "type" : "string",
        "enum" : [ "NotEnabledOnSubscription", "NotAvailableInRegion" ]
      },
      "clusters.ClusterAttributes" : {
        "required" : [ "spark_version" ],
        "properties" : {
          "workload_type" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.WorkloadType"
          },
          "cluster_name" : {
            "description" : "Cluster name requested by the user. This doesn't have to be unique.\nIf not specified at creation, the cluster name will be an empty string.\n",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "ssh_public_keys" : {
            "description" : "SSH public key contents that will be added to each Spark node in this cluster. The\ncorresponding private keys can be used to login with the user name `ubuntu` on port `2200`.\nUp to 10 keys can be specified.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "policy_id" : {
            "description" : "The ID of the cluster policy used to create the cluster if applicable.",
            "type" : "string"
          },
          "node_type_id" : {
            "description" : "This field encodes, through a single value, the resources available to each of\nthe Spark nodes in this cluster. For example, the Spark nodes can be provisioned\nand optimized for memory or compute intensive workloads. A list of available node\ntypes can be retrieved by using the :method:clusters/listNodeTypes API call.\n",
            "type" : "string"
          },
          "spark_version" : {
            "description" : "The Spark version of the cluster, e.g. `3.3.x-scala2.11`.\nA list of available Spark versions can be retrieved by using\nthe :method:clusters/sparkVersions API call.\n",
            "type" : "string"
          },
          "runtime_engine" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.RuntimeEngine"
          },
          "driver_instance_pool_id" : {
            "description" : "The optional ID of the instance pool for the driver of the cluster belongs.\nThe pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not\nassigned.",
            "type" : "string"
          },
          "enable_local_disk_encryption" : {
            "description" : "Whether to enable LUKS on cluster VMs' local disks",
            "type" : "boolean"
          },
          "custom_tags" : {
            "properties" : { },
            "description" : "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS\ninstances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n- Currently, Databricks allows at most 45 custom tags\n\n- Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length must be less than or equal to 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          },
          "spark_conf" : {
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified Spark configuration key-value pairs.\nUsers can also pass in a string of extra JVM options to the driver and the executors via\n`spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively.\n",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "driver_node_type_id" : {
            "description" : "The node type of the Spark driver. Note that this field is optional;\nif unset, the driver node type will be set as the same value\nas `node_type_id` defined above.\n",
            "type" : "string"
          },
          "instance_pool_id" : {
            "description" : "The optional ID of the instance pool to which the cluster belongs.",
            "type" : "string"
          },
          "cluster_source" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterSource"
          },
          "gcp_attributes" : {
            "description" : "Attributes related to clusters running on Google Cloud Platform.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/clusters.GcpAttributes"
          },
          "azure_attributes" : {
            "description" : "Attributes related to clusters running on Microsoft Azure.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/clusters.AzureAttributes"
          },
          "aws_attributes" : {
            "description" : "Attributes related to clusters running on Amazon Web Services.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/clusters.AwsAttributes"
          },
          "spark_env_vars" : {
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified environment variable key-value pairs.\nPlease note that key-value pair of the form (X,Y) will be exported as is (i.e.,\n`export X='Y'`) while launching the driver and workers.\n\nIn order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending\nthem to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all\ndefault databricks managed environmental variables are included as well.\n\nExample Spark environment variables:\n`{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or\n`{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "autotermination_minutes" : {
            "format" : "int32",
            "description" : "Automatically terminates the cluster after it is inactive for this time in minutes. If not set,\nthis cluster will not be automatically terminated. If specified, the threshold must be between\n10 and 10000 minutes.\nUsers can also set this value to 0 to explicitly disable automatic termination.",
            "type" : "integer"
          },
          "cluster_log_conf" : {
            "description" : "The configuration for delivering spark logs to a long-term storage destination.\nTwo kinds of destinations (dbfs and s3) are supported. Only one destination can be specified\nfor one cluster. If the conf is given, the logs will be delivered to the destination every\n`5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while\nthe destination of executor logs is `$destination/$clusterId/executor`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterLogConf"
          },
          "enable_elastic_disk" : {
            "description" : "Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk\nspace when its Spark workers are running low on disk space. This feature requires specific AWS\npermissions to function correctly - refer to the User Guide for more details.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "clusters.ClusterEvent" : {
        "required" : [ "cluster_id" ],
        "properties" : {
          "timestamp" : {
            "format" : "int64",
            "description" : "The timestamp when the event occurred, stored as the number of milliseconds since\nthe Unix epoch. If not provided, this will be assigned by the Timeline service.",
            "type" : "integer"
          },
          "data_plane_event_details" : {
            "description" : "<needs content added>",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.DataPlaneEventDetails"
          },
          "details" : {
            "description" : "<needs content added>",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.EventDetails"
          },
          "type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.EventType"
          },
          "cluster_id" : {
            "description" : "<needs content added>",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.ClusterInfo" : {
        "properties" : {
          "workload_type" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.WorkloadType"
          },
          "cluster_name" : {
            "description" : "Cluster name requested by the user. This doesn't have to be unique.\nIf not specified at creation, the cluster name will be an empty string.\n",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "start_time" : {
            "format" : "int64",
            "description" : "Time (in epoch milliseconds) when the cluster creation request was received (when the cluster\nentered a `PENDING` state).",
            "type" : "integer"
          },
          "ssh_public_keys" : {
            "description" : "SSH public key contents that will be added to each Spark node in this cluster. The\ncorresponding private keys can be used to login with the user name `ubuntu` on port `2200`.\nUp to 10 keys can be specified.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "policy_id" : {
            "description" : "The ID of the cluster policy used to create the cluster if applicable.",
            "type" : "string"
          },
          "state" : {
            "description" : "Current state of the cluster.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.State"
          },
          "node_type_id" : {
            "description" : "This field encodes, through a single value, the resources available to each of\nthe Spark nodes in this cluster. For example, the Spark nodes can be provisioned\nand optimized for memory or compute intensive workloads. A list of available node\ntypes can be retrieved by using the :method:clusters/listNodeTypes API call.\n",
            "type" : "string"
          },
          "spark_version" : {
            "description" : "The Spark version of the cluster, e.g. `3.3.x-scala2.11`.\nA list of available Spark versions can be retrieved by using\nthe :method:clusters/sparkVersions API call.\n",
            "type" : "string"
          },
          "runtime_engine" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.RuntimeEngine"
          },
          "num_workers" : {
            "format" : "int32",
            "description" : "Number of worker nodes that this cluster should have. A cluster has one Spark Driver\nand `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.\n\nNote: When reading the properties of a cluster, this field reflects the desired number\nof workers rather than the actual current number of workers. For instance, if a cluster\nis resized from 5 to 10 workers, this field will immediately be updated to reflect\nthe target size of 10 workers, whereas the workers listed in `spark_info` will gradually\nincrease from 5 to 10 as the new nodes are provisioned.",
            "type" : "integer"
          },
          "data_security_mode" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.DataSecurityMode"
          },
          "cluster_cores" : {
            "format" : "double",
            "description" : "Number of CPU cores available for this cluster.\nNote that this can be fractional, e.g. 7.5 cores, since certain node types are configured to\nshare cores between Spark nodes on the same instance.",
            "type" : "number"
          },
          "executors" : {
            "description" : "Nodes on which the Spark executors reside.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusters.SparkNode"
            }
          },
          "driver_instance_pool_id" : {
            "description" : "The optional ID of the instance pool for the driver of the cluster belongs.\nThe pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not\nassigned.",
            "type" : "string"
          },
          "last_state_loss_time" : {
            "format" : "int64",
            "description" : "Time when the cluster driver last lost its state (due to a restart or driver failure).",
            "type" : "integer"
          },
          "cluster_memory_mb" : {
            "format" : "int64",
            "description" : "Total amount of cluster memory, in megabytes",
            "type" : "integer"
          },
          "enable_local_disk_encryption" : {
            "description" : "Whether to enable LUKS on cluster VMs' local disks",
            "type" : "boolean"
          },
          "custom_tags" : {
            "properties" : { },
            "description" : "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS\ninstances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n- Currently, Databricks allows at most 45 custom tags\n\n- Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length must be less than or equal to 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          },
          "autoscale" : {
            "description" : "Parameters needed in order to automatically scale clusters up and down based on load.\nNote: autoscaling works best with DB runtime versions 3.0 or later.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.AutoScale"
          },
          "driver" : {
            "description" : "Node on which the Spark driver resides. The driver node contains the Spark master and\nthe Databricks application that manages the per-notebook Spark REPLs.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.SparkNode"
          },
          "spark_conf" : {
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified Spark configuration key-value pairs.\nUsers can also pass in a string of extra JVM options to the driver and the executors via\n`spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively.\n",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "creator_user_name" : {
            "description" : "Creator user name.\nThe field won't be included in the response if the user has already been deleted.",
            "type" : "string"
          },
          "state_message" : {
            "description" : "A message associated with the most recent state transition (e.g., the reason why\nthe cluster entered a `TERMINATED` state).",
            "type" : "string"
          },
          "driver_node_type_id" : {
            "description" : "The node type of the Spark driver. Note that this field is optional;\nif unset, the driver node type will be set as the same value\nas `node_type_id` defined above.\n",
            "type" : "string"
          },
          "instance_pool_id" : {
            "description" : "The optional ID of the instance pool to which the cluster belongs.",
            "type" : "string"
          },
          "cluster_source" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterSource"
          },
          "terminated_time" : {
            "format" : "int64",
            "description" : "Time (in epoch milliseconds) when the cluster was terminated, if applicable.",
            "type" : "integer"
          },
          "spark_context_id" : {
            "format" : "int64",
            "description" : "A canonical SparkContext identifier. This value *does* change when the Spark driver restarts.\nThe pair `(cluster_id, spark_context_id)` is a globally unique identifier over all Spark\ncontexts.",
            "type" : "integer"
          },
          "gcp_attributes" : {
            "description" : "Attributes related to clusters running on Google Cloud Platform.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/clusters.GcpAttributes"
          },
          "azure_attributes" : {
            "description" : "Attributes related to clusters running on Microsoft Azure.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/clusters.AzureAttributes"
          },
          "cluster_log_status" : {
            "description" : "Cluster log delivery status.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.LogSyncStatus"
          },
          "aws_attributes" : {
            "description" : "Attributes related to clusters running on Amazon Web Services.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/clusters.AwsAttributes"
          },
          "spark_env_vars" : {
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified environment variable key-value pairs.\nPlease note that key-value pair of the form (X,Y) will be exported as is (i.e.,\n`export X='Y'`) while launching the driver and workers.\n\nIn order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending\nthem to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all\ndefault databricks managed environmental variables are included as well.\n\nExample Spark environment variables:\n`{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or\n`{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "last_restarted_time" : {
            "format" : "int64",
            "description" : "the timestamp that the cluster was started/restarted",
            "type" : "integer"
          },
          "cluster_id" : {
            "description" : "Canonical identifier for the cluster. This id is retained during cluster restarts and resizes,\nwhile each new cluster has a globally unique id.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "single_user_name" : {
            "description" : "Single user name if data_security_mode is `SINGLE_USER`",
            "type" : "string"
          },
          "autotermination_minutes" : {
            "format" : "int32",
            "description" : "Automatically terminates the cluster after it is inactive for this time in minutes. If not set,\nthis cluster will not be automatically terminated. If specified, the threshold must be between\n10 and 10000 minutes.\nUsers can also set this value to 0 to explicitly disable automatic termination.",
            "type" : "integer"
          },
          "jdbc_port" : {
            "format" : "int32",
            "description" : "Port on which Spark JDBC server is listening, in the driver nod. No service will be listeningon\non this port in executor nodes.",
            "type" : "integer"
          },
          "cluster_log_conf" : {
            "description" : "The configuration for delivering spark logs to a long-term storage destination.\nTwo kinds of destinations (dbfs and s3) are supported. Only one destination can be specified\nfor one cluster. If the conf is given, the logs will be delivered to the destination every\n`5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while\nthe destination of executor logs is `$destination/$clusterId/executor`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterLogConf"
          },
          "termination_reason" : {
            "description" : "Information about why the cluster was terminated.\nThis field only appears when the cluster is in a `TERMINATING` or `TERMINATED` state.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.TerminationReason"
          },
          "enable_elastic_disk" : {
            "description" : "Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk\nspace when its Spark workers are running low on disk space. This feature requires specific AWS\npermissions to function correctly - refer to the User Guide for more details.",
            "type" : "boolean"
          },
          "default_tags" : {
            "properties" : { },
            "description" : "Tags that are added by Databricks regardless of any `custom_tags`, including:\n\n  - Vendor: Databricks\n\n  - Creator: <username_of_creator>\n\n  - ClusterName: <name_of_cluster>\n\n  - ClusterId: <id_of_cluster>\n\n  - Name: <Databricks internal use>",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length must be less than or equal to 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          }
        },
        "type" : "object"
      },
      "clusters.ClusterLogConf" : {
        "properties" : {
          "dbfs" : {
            "description" : "destination needs to be provided. e.g.\n`{ \"dbfs\" : { \"destination\" : \"dbfs:/home/cluster_log\" } }`",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.DbfsStorageInfo"
          },
          "s3" : {
            "description" : "destination and either region or endpoint should also be provided. e.g.\n`{ \"s3\": { \"destination\" : \"s3://cluster_log_bucket/prefix\", \"region\" : \"us-west-2\" } }`\nCluster iam role is used to access s3, please make sure the cluster iam role in\n`instance_profile_arn` has permission to write data to the s3 destination.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/clusters.S3StorageInfo"
          }
        },
        "type" : "object"
      },
      "clusters.ClusterSize" : {
        "properties" : {
          "autoscale" : {
            "description" : "Parameters needed in order to automatically scale clusters up and down based on load.\nNote: autoscaling works best with DB runtime versions 3.0 or later.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.AutoScale"
          },
          "num_workers" : {
            "format" : "int32",
            "description" : "Number of worker nodes that this cluster should have. A cluster has one Spark Driver\nand `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.\n\nNote: When reading the properties of a cluster, this field reflects the desired number\nof workers rather than the actual current number of workers. For instance, if a cluster\nis resized from 5 to 10 workers, this field will immediately be updated to reflect\nthe target size of 10 workers, whereas the workers listed in `spark_info` will gradually\nincrease from 5 to 10 as the new nodes are provisioned.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "clusters.ClusterSource" : {
        "description" : "Determines whether the cluster was created by a user through the UI, created by the Databricks\nJobs Scheduler, or through an API request.\nThis is the same as cluster_creator, but read only.",
        "type" : "string",
        "enum" : [ "UI", "JOB", "API", "SQL", "MODELS", "PIPELINE", "PIPELINE_MAINTENANCE" ]
      },
      "clusters.CreateCluster" : {
        "required" : [ "spark_version" ],
        "properties" : {
          "workload_type" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.WorkloadType"
          },
          "cluster_name" : {
            "description" : "Cluster name requested by the user. This doesn't have to be unique.\nIf not specified at creation, the cluster name will be an empty string.",
            "type" : "string"
          },
          "ssh_public_keys" : {
            "description" : "SSH public key contents that will be added to each Spark node in this cluster. The\ncorresponding private keys can be used to login with the user name `ubuntu` on port `2200`.\nUp to 10 keys can be specified.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "policy_id" : {
            "description" : "The ID of the cluster policy used to create the cluster if applicable.",
            "type" : "string"
          },
          "node_type_id" : {
            "description" : "This field encodes, through a single value, the resources available to each of\nthe Spark nodes in this cluster. For example, the Spark nodes can be provisioned\nand optimized for memory or compute intensive workloads. A list of available node\ntypes can be retrieved by using the :method:clusters/listNodeTypes API call.\n",
            "type" : "string"
          },
          "spark_version" : {
            "description" : "The Spark version of the cluster, e.g. `3.3.x-scala2.11`.\nA list of available Spark versions can be retrieved by using\nthe :method:clusters/sparkVersions API call.\n",
            "type" : "string"
          },
          "runtime_engine" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.RuntimeEngine"
          },
          "num_workers" : {
            "format" : "int32",
            "description" : "Number of worker nodes that this cluster should have. A cluster has one Spark Driver\nand `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.\n\nNote: When reading the properties of a cluster, this field reflects the desired number\nof workers rather than the actual current number of workers. For instance, if a cluster\nis resized from 5 to 10 workers, this field will immediately be updated to reflect\nthe target size of 10 workers, whereas the workers listed in `spark_info` will gradually\nincrease from 5 to 10 as the new nodes are provisioned.",
            "type" : "integer"
          },
          "driver_instance_pool_id" : {
            "description" : "The optional ID of the instance pool for the driver of the cluster belongs.\nThe pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not\nassigned.",
            "type" : "string"
          },
          "apply_policy_default_values" : {
            "default" : "false",
            "description" : "Note: This field won't be true for webapp requests. Only API users will check this field.",
            "type" : "boolean"
          },
          "enable_local_disk_encryption" : {
            "description" : "Whether to enable LUKS on cluster VMs' local disks",
            "type" : "boolean"
          },
          "custom_tags" : {
            "properties" : { },
            "description" : "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS\ninstances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n- Currently, Databricks allows at most 45 custom tags\n\n- Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length must be less than or equal to 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          },
          "autoscale" : {
            "description" : "Parameters needed in order to automatically scale clusters up and down based on load.\nNote: autoscaling works best with DB runtime versions 3.0 or later.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.AutoScale"
          },
          "spark_conf" : {
            "example" : {
              "spark.speculation" : true,
              "spark.streaming.ui.retainedBatches" : 5
            },
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified Spark configuration key-value pairs.\nUsers can also pass in a string of extra JVM options to the driver and the executors via\n`spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively.\n",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "driver_node_type_id" : {
            "description" : "The node type of the Spark driver. Note that this field is optional;\nif unset, the driver node type will be set as the same value\nas `node_type_id` defined above.\n",
            "type" : "string"
          },
          "instance_pool_id" : {
            "description" : "The optional ID of the instance pool to which the cluster belongs.",
            "type" : "string"
          },
          "cluster_source" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterSource"
          },
          "gcp_attributes" : {
            "description" : "Attributes related to clusters running on Google Cloud Platform.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/clusters.GcpAttributes"
          },
          "azure_attributes" : {
            "description" : "Attributes related to clusters running on Microsoft Azure.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/clusters.AzureAttributes"
          },
          "aws_attributes" : {
            "description" : "Attributes related to clusters running on Amazon Web Services.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/clusters.AwsAttributes"
          },
          "spark_env_vars" : {
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified environment variable key-value pairs.\nPlease note that key-value pair of the form (X,Y) will be exported as is (i.e.,\n`export X='Y'`) while launching the driver and workers.\n\nIn order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending\nthem to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all\ndefault databricks managed environmental variables are included as well.\n\nExample Spark environment variables:\n`{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or\n`{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "autotermination_minutes" : {
            "format" : "int32",
            "description" : "Automatically terminates the cluster after it is inactive for this time in minutes. If not set,\nthis cluster will not be automatically terminated. If specified, the threshold must be between\n10 and 10000 minutes.\nUsers can also set this value to 0 to explicitly disable automatic termination.",
            "type" : "integer"
          },
          "cluster_log_conf" : {
            "description" : "The configuration for delivering spark logs to a long-term storage destination.\nTwo kinds of destinations (dbfs and s3) are supported. Only one destination can be specified\nfor one cluster. If the conf is given, the logs will be delivered to the destination every\n`5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while\nthe destination of executor logs is `$destination/$clusterId/executor`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterLogConf"
          },
          "enable_elastic_disk" : {
            "description" : "Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk\nspace when its Spark workers are running low on disk space. This feature requires specific AWS\npermissions to function correctly - refer to the User Guide for more details.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "clusters.CreateClusterResponse" : {
        "properties" : {
          "cluster_id" : {
            "description" : "",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.DataPlaneEventDetails" : {
        "properties" : {
          "event_type" : {
            "description" : "<needs content added>",
            "type" : "string",
            "enum" : [ "NODE_BLACKLISTED", "NODE_EXCLUDED_DECOMMISSIONED" ]
          },
          "executor_failures" : {
            "format" : "int32",
            "description" : "<needs content added>",
            "type" : "integer"
          },
          "host_id" : {
            "description" : "<needs content added>",
            "type" : "string"
          },
          "timestamp" : {
            "format" : "int64",
            "description" : "<needs content added>",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "clusters.DataSecurityMode" : {
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "NONE", "SINGLE_USER", "USER_ISOLATION", "LEGACY_TABLE_ACL", "LEGACY_PASSTHROUGH", "LEGACY_SINGLE_USER" ],
        "x-databricks-enum-descriptions" : {
          "NONE" : "No security isolation for multiple users sharing the cluster. Data governance features are not available in this mode.",
          "LEGACY_TABLE_ACL" : "This mode is for users migrating from legacy Table ACL clusters.",
          "LEGACY_PASSTHROUGH" : "This mode is for users migrating from legacy Passthrough on high concurrency clusters.",
          "USER_ISOLATION" : "A secure cluster that can be shared by multiple users. Cluster users are fully isolated so that they cannot see each other's data and credentials. Most data governance features are supported in this mode. But programming languages and cluster features might be limited.",
          "LEGACY_SINGLE_USER" : "This mode is for users migrating from legacy Passthrough on standard clusters.",
          "SINGLE_USER" : "A secure cluster that can only be exclusively used by a single user specified in `single_user_name`. Most programming languages, cluster features and data governance features are available in this mode."
        }
      },
      "clusters.DbfsStorageInfo" : {
        "properties" : {
          "destination" : {
            "description" : "dbfs destination, e.g. `dbfs:/my/path`",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.DeleteCluster" : {
        "required" : [ "cluster_id" ],
        "properties" : {
          "cluster_id" : {
            "description" : "The cluster to be terminated.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.DeleteClusterResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "clusters.EbsVolumeType" : {
        "description" : "The type of EBS volumes that will be launched with this cluster.",
        "type" : "string",
        "enum" : [ "GENERAL_PURPOSE_SSD", "THROUGHPUT_OPTIMIZED_HDD" ]
      },
      "clusters.EditCluster" : {
        "required" : [ "cluster_id", "spark_version" ],
        "properties" : {
          "workload_type" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.WorkloadType"
          },
          "cluster_name" : {
            "description" : "Cluster name requested by the user. This doesn't have to be unique.\nIf not specified at creation, the cluster name will be an empty string.\n",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "ssh_public_keys" : {
            "description" : "SSH public key contents that will be added to each Spark node in this cluster. The\ncorresponding private keys can be used to login with the user name `ubuntu` on port `2200`.\nUp to 10 keys can be specified.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "policy_id" : {
            "description" : "The ID of the cluster policy used to create the cluster if applicable.",
            "type" : "string"
          },
          "node_type_id" : {
            "description" : "This field encodes, through a single value, the resources available to each of\nthe Spark nodes in this cluster. For example, the Spark nodes can be provisioned\nand optimized for memory or compute intensive workloads. A list of available node\ntypes can be retrieved by using the :method:clusters/listNodeTypes API call.\n",
            "type" : "string"
          },
          "spark_version" : {
            "description" : "The Spark version of the cluster, e.g. `3.3.x-scala2.11`.\nA list of available Spark versions can be retrieved by using\nthe :method:clusters/sparkVersions API call.\n",
            "type" : "string"
          },
          "runtime_engine" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.RuntimeEngine"
          },
          "num_workers" : {
            "format" : "int32",
            "description" : "Number of worker nodes that this cluster should have. A cluster has one Spark Driver\nand `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.\n\nNote: When reading the properties of a cluster, this field reflects the desired number\nof workers rather than the actual current number of workers. For instance, if a cluster\nis resized from 5 to 10 workers, this field will immediately be updated to reflect\nthe target size of 10 workers, whereas the workers listed in `spark_info` will gradually\nincrease from 5 to 10 as the new nodes are provisioned.",
            "type" : "integer"
          },
          "driver_instance_pool_id" : {
            "description" : "The optional ID of the instance pool for the driver of the cluster belongs.\nThe pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not\nassigned.",
            "type" : "string"
          },
          "apply_policy_default_values" : {
            "default" : "false",
            "description" : "Note: This field won't be true for webapp requests. Only API users will check this field.",
            "type" : "boolean"
          },
          "enable_local_disk_encryption" : {
            "description" : "Whether to enable LUKS on cluster VMs' local disks",
            "type" : "boolean"
          },
          "custom_tags" : {
            "properties" : { },
            "description" : "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS\ninstances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n- Currently, Databricks allows at most 45 custom tags\n\n- Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length must be less than or equal to 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          },
          "autoscale" : {
            "description" : "Parameters needed in order to automatically scale clusters up and down based on load.\nNote: autoscaling works best with DB runtime versions 3.0 or later.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.AutoScale"
          },
          "spark_conf" : {
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified Spark configuration key-value pairs.\nUsers can also pass in a string of extra JVM options to the driver and the executors via\n`spark.driver.extraJavaOptions` and `spark.executor.extraJavaOptions` respectively.\n",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "driver_node_type_id" : {
            "description" : "The node type of the Spark driver. Note that this field is optional;\nif unset, the driver node type will be set as the same value\nas `node_type_id` defined above.\n",
            "type" : "string"
          },
          "instance_pool_id" : {
            "description" : "The optional ID of the instance pool to which the cluster belongs.",
            "type" : "string"
          },
          "cluster_source" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterSource"
          },
          "gcp_attributes" : {
            "description" : "Attributes related to clusters running on Google Cloud Platform.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/clusters.GcpAttributes"
          },
          "azure_attributes" : {
            "description" : "Attributes related to clusters running on Microsoft Azure.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/clusters.AzureAttributes"
          },
          "aws_attributes" : {
            "description" : "Attributes related to clusters running on Amazon Web Services.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/clusters.AwsAttributes"
          },
          "spark_env_vars" : {
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified environment variable key-value pairs.\nPlease note that key-value pair of the form (X,Y) will be exported as is (i.e.,\n`export X='Y'`) while launching the driver and workers.\n\nIn order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending\nthem to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all\ndefault databricks managed environmental variables are included as well.\n\nExample Spark environment variables:\n`{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or\n`{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "cluster_id" : {
            "description" : "ID of the cluser",
            "type" : "string"
          },
          "autotermination_minutes" : {
            "format" : "int32",
            "description" : "Automatically terminates the cluster after it is inactive for this time in minutes. If not set,\nthis cluster will not be automatically terminated. If specified, the threshold must be between\n10 and 10000 minutes.\nUsers can also set this value to 0 to explicitly disable automatic termination.",
            "type" : "integer"
          },
          "cluster_log_conf" : {
            "description" : "The configuration for delivering spark logs to a long-term storage destination.\nTwo kinds of destinations (dbfs and s3) are supported. Only one destination can be specified\nfor one cluster. If the conf is given, the logs will be delivered to the destination every\n`5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while\nthe destination of executor logs is `$destination/$clusterId/executor`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterLogConf"
          },
          "enable_elastic_disk" : {
            "description" : "Autoscaling Local Storage: when enabled, this cluster will dynamically acquire additional disk\nspace when its Spark workers are running low on disk space. This feature requires specific AWS\npermissions to function correctly - refer to the User Guide for more details.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "clusters.EditClusterResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "clusters.EventDetails" : {
        "properties" : {
          "driver_state_message" : {
            "description" : "More details about the change in driver's state",
            "type" : "string"
          },
          "disk_size" : {
            "format" : "int64",
            "description" : "Current disk size in bytes",
            "type" : "integer"
          },
          "did_not_expand_reason" : {
            "description" : "<needs content added>",
            "type" : "string"
          },
          "previous_cluster_size" : {
            "description" : "The size of the cluster before an edit or resize.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterSize"
          },
          "previous_attributes" : {
            "description" : "The cluster attributes before a cluster was edited.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterAttributes"
          },
          "job_run_name" : {
            "description" : "Unique identifier of the specific job run associated with this cluster event\n* For clusters created for jobs, this will be the same as the cluster name",
            "type" : "string"
          },
          "reason" : {
            "description" : "A termination reason:\n  * On a TERMINATED event, this is the reason of the termination.\n  * On a RESIZE_COMPLETE event, this indicates the reason that we failed to acquire some nodes.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.TerminationReason"
          },
          "attributes" : {
            "description" : "* For created clusters, the attributes of the cluster.\n* For edited clusters, the new attributes of the cluster.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterAttributes"
          },
          "current_num_workers" : {
            "format" : "int32",
            "description" : "The current number of nodes in the cluster.",
            "type" : "integer"
          },
          "current_num_vcpus" : {
            "format" : "int32",
            "description" : "The current number of vCPUs in the cluster.",
            "type" : "integer"
          },
          "enable_termination_for_node_blocklisted" : {
            "description" : "Whether or not a blocklisted node should be terminated. For ClusterEventType NODE_BLACKLISTED.",
            "type" : "boolean"
          },
          "cluster_size" : {
            "description" : "The actual cluster size that was set in the cluster creation or edit.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterSize"
          },
          "cause" : {
            "description" : "The cause of a change in target size.",
            "type" : "string",
            "enum" : [ "AUTOSCALE", "USER_REQUEST", "AUTORECOVERY", "REPLACE_BAD_NODES" ]
          },
          "previous_disk_size" : {
            "format" : "int64",
            "description" : "Previous disk size in bytes",
            "type" : "integer"
          },
          "free_space" : {
            "format" : "int64",
            "description" : "<needs content added>",
            "type" : "integer"
          },
          "instance_id" : {
            "description" : "Instance Id where the event originated from",
            "type" : "string"
          },
          "target_num_workers" : {
            "format" : "int32",
            "description" : "The targeted number of nodes in the cluster.",
            "type" : "integer"
          },
          "user" : {
            "description" : "The user that caused the event to occur. (Empty if it was done by the control plane.)",
            "type" : "string"
          },
          "target_num_vcpus" : {
            "format" : "int32",
            "description" : "The targeted number of vCPUs in the cluster.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "clusters.EventType" : {
        "type" : "string",
        "enum" : [ "CREATING", "STARTING", "RESTARTING", "TERMINATING", "EDITED", "RUNNING", "RESIZING", "NODES_LOST", "UPSIZE_COMPLETED", "INIT_SCRIPTS_STARTED", "INIT_SCRIPTS_FINISHED", "DID_NOT_EXPAND_DISK", "EXPANDED_DISK", "FAILED_TO_EXPAND_DISK", "DRIVER_HEALTHY", "DRIVER_NOT_RESPONDING", "DRIVER_UNAVAILABLE", "SPARK_EXCEPTION", "METASTORE_DOWN", "DBFS_DOWN", "AUTOSCALING_STATS_REPORT", "NODE_BLACKLISTED", "PINNED", "UNPINNED", "NODE_EXCLUDED_DECOMMISSIONED" ]
      },
      "clusters.GcpAttributes" : {
        "properties" : {
          "availability" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.GcpAvailability"
          },
          "boot_disk_size" : {
            "format" : "int32",
            "description" : "boot disk size in GB",
            "type" : "integer"
          },
          "google_service_account" : {
            "description" : "If provided, the cluster will impersonate the google service account when accessing\ngcloud services (like GCS). The google service account\nmust have previously been added to the Databricks environment by an account\nadministrator.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-cloud" : "gcp"
      },
      "clusters.GcpAvailability" : {
        "default" : "ON_DEMAND_GCP",
        "description" : "This field determines whether the spark executors will be scheduled to run on preemptible\nVMs, on-demand VMs, or preemptible VMs with a fallback to on-demand VMs if the former is unavailable.",
        "type" : "string",
        "enum" : [ "PREEMPTIBLE_GCP", "ON_DEMAND_GCP", "PREEMPTIBLE_WITH_FALLBACK_GCP" ]
      },
      "clusters.GetEvents" : {
        "required" : [ "cluster_id" ],
        "properties" : {
          "start_time" : {
            "format" : "int64",
            "description" : "The start time in epoch milliseconds.\nIf empty, returns events starting from the beginning of time.",
            "type" : "integer"
          },
          "end_time" : {
            "format" : "int64",
            "description" : "The end time in epoch milliseconds.\nIf empty, returns events up to the current time.",
            "type" : "integer"
          },
          "offset" : {
            "format" : "int64",
            "description" : "The offset in the result set. Defaults to 0 (no offset). When an offset is specified\nand the results are requested in descending order, the end_time field is required.",
            "type" : "integer"
          },
          "order" : {
            "default" : "DESC",
            "description" : "The order to list events in; either \"ASC\" or \"DESC\". Defaults to \"DESC\".",
            "type" : "string",
            "enum" : [ "DESC", "ASC" ]
          },
          "event_types" : {
            "description" : "An optional set of event types to filter on.\nIf empty, all event types are returned.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusters.EventType"
            }
          },
          "cluster_id" : {
            "description" : "The ID of the cluster to retrieve events about.",
            "type" : "string"
          },
          "limit" : {
            "default" : "50",
            "format" : "int64",
            "description" : "The maximum number of events to include in a page of events.\nDefaults to 50, and maximum allowed value is 500.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "clusters.GetEventsResponse" : {
        "properties" : {
          "events" : {
            "description" : "<content needs to be added>",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusters.ClusterEvent"
            }
          },
          "next_page" : {
            "description" : "The parameters required to retrieve the next page of events.\nOmitted if there are no more events to read.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.GetEvents"
          },
          "total_count" : {
            "format" : "int64",
            "description" : "The total number of events filtered by the start_time, end_time, and event_types.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "clusters.GetSparkVersionsResponse" : {
        "properties" : {
          "versions" : {
            "description" : "All the available Spark versions.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusters.SparkVersion"
            }
          }
        },
        "type" : "object"
      },
      "clusters.InstanceProfile" : {
        "required" : [ "instance_profile_arn" ],
        "properties" : {
          "iam_role_arn" : {
            "example" : "arn:aws:iam::<account-id>:role/<name>",
            "description" : "The AWS IAM role ARN of the role associated with the instance profile.\nThis field is required if your role name and instance profile name do\nnot match and you want to use the instance profile with\n[Databricks SQL Serverless](https://docs.databricks.com/sql/admin/serverless.html).\n\nOtherwise, this field is optional.\n",
            "type" : "string"
          },
          "instance_profile_arn" : {
            "example" : "arn:aws:iam::<account-id>:instance-profile/<name>",
            "description" : "The AWS ARN of the instance profile to register with Databricks. This field is required.",
            "type" : "string"
          },
          "is_meta_instance_profile" : {
            "description" : "By default, Databricks validates that it has sufficient permissions to launch\ninstances with the instance profile. This validation uses AWS dry-run mode for\nthe RunInstances API. If validation fails with an error message that does not\nindicate an IAM related permission issue, (e.g. `Your requested instance type\nis not supported in your requested availability zone`), you can pass this flag\nto skip the validation and forcibly add the instance profile.\n",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "clusters.ListAvailableZonesResponse" : {
        "properties" : {
          "default_zone" : {
            "description" : "The availability zone if no `zone_id` is provided in the cluster creation request.",
            "type" : "string"
          },
          "zones" : {
            "description" : "The list of available zones (e.g., ['us-west-2c', 'us-east-2']).",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          }
        },
        "type" : "object"
      },
      "clusters.ListClustersResponse" : {
        "properties" : {
          "clusters" : {
            "description" : "<needs content added>",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusters.ClusterInfo"
            }
          }
        },
        "type" : "object"
      },
      "clusters.ListInstanceProfilesResponse" : {
        "properties" : {
          "instance_profiles" : {
            "description" : "A list of instance profiles that the user can access.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusters.InstanceProfile"
            }
          }
        },
        "type" : "object"
      },
      "clusters.ListNodeTypesResponse" : {
        "properties" : {
          "node_types" : {
            "description" : "The list of available Spark node types.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/clusters.NodeType"
            }
          }
        },
        "type" : "object"
      },
      "clusters.LogAnalyticsInfo" : {
        "properties" : {
          "log_analytics_primary_key" : {
            "description" : "<needs content added>",
            "type" : "string"
          },
          "log_analytics_workspace_id" : {
            "description" : "<needs content added>",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.LogSyncStatus" : {
        "properties" : {
          "last_attempted" : {
            "format" : "int64",
            "description" : "The timestamp of last attempt. If the last attempt fails, `last_exception` will contain the\nexception in the last attempt.",
            "type" : "integer"
          },
          "last_exception" : {
            "description" : "The exception thrown in the last attempt, it would be null (omitted in the response) if\nthere is no exception in last attempted.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.NodeInstanceType" : {
        "properties" : {
          "instance_type_id" : {
            "type" : "string"
          },
          "local_disk_size_gb" : {
            "format" : "int32",
            "type" : "integer"
          },
          "local_disks" : {
            "format" : "int32",
            "type" : "integer"
          },
          "local_nvme_disks" : {
            "format" : "int32",
            "type" : "integer"
          },
          "local_nvme_disk_size_gb" : {
            "format" : "int32",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "clusters.NodeType" : {
        "required" : [ "instance_type_id", "node_type_id", "description", "memory_mb", "num_cores" ],
        "properties" : {
          "display_order" : {
            "format" : "int32",
            "type" : "integer"
          },
          "photon_driver_capable" : {
            "type" : "boolean"
          },
          "is_graviton" : {
            "type" : "boolean"
          },
          "num_gpus" : {
            "format" : "int32",
            "type" : "integer"
          },
          "instance_type_id" : {
            "description" : "An identifier for the type of hardware that this node runs on, e.g., \"r3.2xlarge\" in AWS.",
            "type" : "string"
          },
          "node_type_id" : {
            "description" : "Unique identifier for this node type.",
            "type" : "string"
          },
          "description" : {
            "description" : "A string description associated with this node type, e.g., \"r3.xlarge\".",
            "type" : "string"
          },
          "support_cluster_tags" : {
            "type" : "boolean"
          },
          "is_encrypted_in_transit" : {
            "default" : "false",
            "description" : "AWS specific, whether this instance supports encryption in transit, used for hipaa and pci\nworkloads.",
            "type" : "boolean"
          },
          "node_info" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.CloudProviderNodeInfo"
          },
          "node_instance_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.NodeInstanceType"
          },
          "photon_worker_capable" : {
            "type" : "boolean"
          },
          "memory_mb" : {
            "format" : "int32",
            "description" : "Memory (in MB) available for this node type.",
            "type" : "integer"
          },
          "is_hidden" : {
            "type" : "boolean"
          },
          "category" : {
            "type" : "string"
          },
          "num_cores" : {
            "format" : "double",
            "description" : "Number of CPU cores available for this node type.\nNote that this can be fractional, e.g., 2.5 cores, if the the number of cores on a\nmachine instance is not divisible by the number of Spark nodes on that machine.",
            "type" : "number"
          },
          "is_io_cache_enabled" : {
            "type" : "boolean"
          },
          "support_port_forwarding" : {
            "type" : "boolean"
          },
          "support_ebs_volumes" : {
            "type" : "boolean"
          },
          "is_deprecated" : {
            "default" : "false",
            "description" : "Whether the node type is deprecated. Non-deprecated node types offer greater performance.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "clusters.PermanentDeleteCluster" : {
        "required" : [ "cluster_id" ],
        "properties" : {
          "cluster_id" : {
            "description" : "The cluster to be deleted.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.PermanentDeleteClusterResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "clusters.PinCluster" : {
        "required" : [ "cluster_id" ],
        "properties" : {
          "cluster_id" : {
            "description" : "<needs content added>",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.PinClusterResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "clusters.RemoveInstanceProfile" : {
        "required" : [ "instance_profile_arn" ],
        "properties" : {
          "instance_profile_arn" : {
            "example" : "arn:aws:iam::<account-id>:instance-profile/<name>",
            "description" : "The ARN of the instance profile to remove. This field is required.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.ResizeCluster" : {
        "required" : [ "cluster_id" ],
        "properties" : {
          "autoscale" : {
            "description" : "Parameters needed in order to automatically scale clusters up and down based on load.\nNote: autoscaling works best with DB runtime versions 3.0 or later.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.AutoScale"
          },
          "cluster_id" : {
            "description" : "The cluster to be resized.",
            "type" : "string"
          },
          "num_workers" : {
            "format" : "int32",
            "description" : "Number of worker nodes that this cluster should have. A cluster has one Spark Driver\nand `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.\n\nNote: When reading the properties of a cluster, this field reflects the desired number\nof workers rather than the actual current number of workers. For instance, if a cluster\nis resized from 5 to 10 workers, this field will immediately be updated to reflect\nthe target size of 10 workers, whereas the workers listed in `spark_info` will gradually\nincrease from 5 to 10 as the new nodes are provisioned.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "clusters.ResizeClusterResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "clusters.RestartCluster" : {
        "required" : [ "cluster_id" ],
        "properties" : {
          "cluster_id" : {
            "description" : "The cluster to be started.",
            "type" : "string"
          },
          "restart_user" : {
            "description" : "<needs content added>",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.RestartClusterResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "clusters.RuntimeEngine" : {
        "description" : "Decides which runtime engine to be use, e.g. Standard vs. Photon. If unspecified, the runtime\nengine is inferred from spark_version.",
        "type" : "string",
        "enum" : [ "NULL", "STANDARD", "PHOTON" ]
      },
      "clusters.S3StorageInfo" : {
        "properties" : {
          "encryption_type" : {
            "description" : "(Optional) The encryption type, it could be `sse-s3` or `sse-kms`. It will be used only when\nencryption is enabled and the default type is `sse-s3`.",
            "type" : "string"
          },
          "kms_key" : {
            "description" : "(Optional) Kms key which will be used if encryption is enabled and encryption type is set to `sse-kms`.",
            "type" : "string"
          },
          "enable_encryption" : {
            "description" : "(Optional) Flag to enable server side encryption, `false` by default.",
            "type" : "boolean"
          },
          "region" : {
            "description" : "S3 region, e.g. `us-west-2`. Either region or endpoint needs to be set. If both are set,\nendpoint will be used.",
            "type" : "string"
          },
          "canned_acl" : {
            "description" : "(Optional) Set canned access control list for the logs, e.g. `bucket-owner-full-control`.\nIf `canned_cal` is set, please make sure the cluster iam role has `s3:PutObjectAcl` permission on\nthe destination bucket and prefix. The full list of possible canned acl can be found at\nhttp://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl.\nPlease also note that by default only the object owner gets full controls. If you are using cross account\nrole for writing data, you may want to set `bucket-owner-full-control` to make bucket owner able to\nread the logs.",
            "type" : "string"
          },
          "endpoint" : {
            "description" : "S3 endpoint, e.g. `https://s3-us-west-2.amazonaws.com`. Either region or endpoint needs to be set.\nIf both are set, endpoint will be used.",
            "type" : "string"
          },
          "destination" : {
            "description" : "S3 destination, e.g. `s3://my-bucket/some-prefix` Note that logs will be delivered using\ncluster iam role, please make sure you set cluster iam role and the role has write access to the\ndestination. Please also note that you cannot use AWS keys to deliver logs.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.SparkNode" : {
        "properties" : {
          "host_private_ip" : {
            "description" : "The private IP address of the host instance.",
            "type" : "string"
          },
          "node_id" : {
            "description" : "Globally unique identifier for this node.",
            "type" : "string"
          },
          "node_aws_attributes" : {
            "description" : "Attributes specific to AWS for a Spark node.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/clusters.SparkNodeAwsAttributes"
          },
          "start_timestamp" : {
            "format" : "int64",
            "description" : "The timestamp (in millisecond) when the Spark node is launched.\n\nThe start_timestamp is set right before the container is being launched.\nThe timestamp when the container is placed on the ResourceManager, before its launch and\nsetup by the NodeDaemon. This timestamp is the same as the creation timestamp in the\ndatabase.",
            "type" : "integer"
          },
          "private_ip" : {
            "description" : "Private IP address (typically a 10.x.x.x address) of the Spark node.\nNote that this is different from the private IP address of the host instance.",
            "type" : "string"
          },
          "instance_id" : {
            "description" : "Globally unique identifier for the host instance from the cloud provider.",
            "type" : "string"
          },
          "public_dns" : {
            "description" : "Public DNS address of this node. This address can be used to access\nthe Spark JDBC server on the driver node. To communicate with the JDBC server, traffic must\nbe manually authorized by adding security group rules to the \"worker-unmanaged\" security\ngroup via the AWS console.\n\nActually it's the public DNS address of the host instance.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.SparkNodeAwsAttributes" : {
        "properties" : {
          "is_spot" : {
            "description" : "Whether this node is on an Amazon spot instance.",
            "type" : "boolean"
          }
        },
        "type" : "object",
        "x-databricks-cloud" : "aws"
      },
      "clusters.SparkVersion" : {
        "properties" : {
          "key" : {
            "description" : "Spark version key, for example \"2.1.x-scala2.11\". This is the value which should be provided\nas the \"spark_version\" when creating a new cluster.\nNote that the exact Spark version may change over time for a \"wildcard\" version\n(i.e., \"2.1.x-scala2.11\" is a \"wildcard\" version) with minor bug fixes.",
            "type" : "string"
          },
          "name" : {
            "description" : "A descriptive name for this Spark version, for example \"Spark 2.1\".",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.StartCluster" : {
        "required" : [ "cluster_id" ],
        "properties" : {
          "cluster_id" : {
            "description" : "The cluster to be started.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.StartClusterResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "clusters.State" : {
        "description" : "Current state of the cluster.",
        "type" : "string",
        "enum" : [ "PENDING", "RUNNING", "RESTARTING", "RESIZING", "TERMINATING", "TERMINATED", "ERROR", "UNKNOWN" ]
      },
      "clusters.TerminationReason" : {
        "properties" : {
          "code" : {
            "description" : "status code indicating why the cluster was terminated",
            "type" : "string",
            "enum" : [ "UNKNOWN", "USER_REQUEST", "JOB_FINISHED", "INACTIVITY", "CLOUD_PROVIDER_SHUTDOWN", "COMMUNICATION_LOST", "CLOUD_PROVIDER_LAUNCH_FAILURE", "INIT_SCRIPT_FAILURE", "SPARK_STARTUP_FAILURE", "INVALID_ARGUMENT", "UNEXPECTED_LAUNCH_FAILURE", "INTERNAL_ERROR", "INSTANCE_UNREACHABLE", "REQUEST_REJECTED", "TRIAL_EXPIRED", "DRIVER_UNREACHABLE", "SPARK_ERROR", "DRIVER_UNRESPONSIVE", "METASTORE_COMPONENT_UNHEALTHY", "DBFS_COMPONENT_UNHEALTHY", "EXECUTION_COMPONENT_UNHEALTHY", "AZURE_RESOURCE_MANAGER_THROTTLING", "AZURE_RESOURCE_PROVIDER_THROTTLING", "NETWORK_CONFIGURATION_FAILURE", "CONTAINER_LAUNCH_FAILURE", "INSTANCE_POOL_CLUSTER_FAILURE", "SKIPPED_SLOW_NODES", "ATTACH_PROJECT_FAILURE", "UPDATE_INSTANCE_PROFILE_FAILURE", "DATABASE_CONNECTION_FAILURE", "REQUEST_THROTTLED", "SELF_BOOTSTRAP_FAILURE", "GLOBAL_INIT_SCRIPT_FAILURE", "SLOW_IMAGE_DOWNLOAD", "INVALID_SPARK_IMAGE", "NPIP_TUNNEL_TOKEN_FAILURE", "HIVE_METASTORE_PROVISIONING_FAILURE", "AZURE_INVALID_DEPLOYMENT_TEMPLATE", "AZURE_UNEXPECTED_DEPLOYMENT_TEMPLATE_FAILURE", "SUBNET_EXHAUSTED_FAILURE", "BOOTSTRAP_TIMEOUT", "STORAGE_DOWNLOAD_FAILURE", "CONTROL_PLANE_REQUEST_FAILURE", "BOOTSTRAP_TIMEOUT_CLOUD_PROVIDER_EXCEPTION", "AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE", "DOCKER_IMAGE_PULL_FAILURE", "AZURE_VNET_CONFIGURATION_FAILURE", "NPIP_TUNNEL_SETUP_FAILURE", "AWS_AUTHORIZATION_FAILURE", "NEPHOS_RESOURCE_MANAGEMENT", "STS_CLIENT_SETUP_FAILURE", "SECURITY_DAEMON_REGISTRATION_EXCEPTION", "AWS_REQUEST_LIMIT_EXCEEDED", "AWS_INSUFFICIENT_FREE_ADDRESSES_IN_SUBNET_FAILURE", "AWS_UNSUPPORTED_FAILURE", "AZURE_QUOTA_EXCEEDED_EXCEPTION", "AZURE_OPERATION_NOT_ALLOWED_EXCEPTION", "NFS_MOUNT_FAILURE", "K8S_AUTOSCALING_FAILURE", "K8S_DBR_CLUSTER_LAUNCH_TIMEOUT", "SPARK_IMAGE_DOWNLOAD_FAILURE", "AZURE_VM_EXTENSION_FAILURE", "WORKSPACE_CANCELLED_ERROR", "AWS_MAX_SPOT_INSTANCE_COUNT_EXCEEDED_FAILURE", "TEMPORARILY_UNAVAILABLE", "WORKER_SETUP_FAILURE", "IP_EXHAUSTION_FAILURE", "GCP_QUOTA_EXCEEDED", "CLOUD_PROVIDER_RESOURCE_STOCKOUT", "GCP_SERVICE_ACCOUNT_DELETED", "AZURE_BYOK_KEY_PERMISSION_FAILURE", "SPOT_INSTANCE_TERMINATION", "AZURE_EPHEMERAL_DISK_FAILURE", "ABUSE_DETECTED", "IMAGE_PULL_PERMISSION_DENIED", "WORKSPACE_CONFIGURATION_ERROR", "SECRET_RESOLUTION_ERROR", "UNSUPPORTED_INSTANCE_TYPE", "CLOUD_PROVIDER_DISK_SETUP_FAILURE" ]
          },
          "parameters" : {
            "properties" : { },
            "description" : "list of parameters that provide additional information about why the cluster was terminated",
            "type" : "object",
            "additionalProperties" : {
              "description" : "<needs content added>",
              "type" : "string"
            }
          },
          "type" : {
            "description" : "type of the termination",
            "type" : "string",
            "enum" : [ "SUCCESS", "CLIENT_ERROR", "SERVICE_FAULT", "CLOUD_FAILURE" ]
          }
        },
        "type" : "object"
      },
      "clusters.UnpinCluster" : {
        "required" : [ "cluster_id" ],
        "properties" : {
          "cluster_id" : {
            "description" : "<needs content added>",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "clusters.UnpinClusterResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "clusters.WorkloadType" : {
        "properties" : {
          "clients" : {
            "description" : " defined what type of clients can use the cluster. E.g. Notebooks, Jobs",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClientsTypes"
          }
        },
        "type" : "object"
      },
      "commands.CancelCommand" : {
        "properties" : {
          "clusterId" : {
            "type" : "string"
          },
          "commandId" : {
            "type" : "string"
          },
          "contextId" : {
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "commands.Command" : {
        "properties" : {
          "clusterId" : {
            "description" : "Running cluster id",
            "type" : "string"
          },
          "command" : {
            "description" : "Executable code",
            "type" : "string"
          },
          "contextId" : {
            "description" : "Running context id",
            "type" : "string"
          },
          "language" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/commands.Language"
          }
        },
        "type" : "object"
      },
      "commands.CommandStatus" : {
        "type" : "string",
        "enum" : [ "Cancelled", "Cancelling", "Error", "Finished", "Queued", "Running" ]
      },
      "commands.CommandStatusResponse" : {
        "properties" : {
          "id" : {
            "type" : "string"
          },
          "results" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/commands.Results"
          },
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/commands.CommandStatus"
          }
        },
        "type" : "object"
      },
      "commands.ContextStatus" : {
        "type" : "string",
        "enum" : [ "Running", "Pending", "Error" ]
      },
      "commands.ContextStatusResponse" : {
        "properties" : {
          "id" : {
            "type" : "string"
          },
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/commands.ContextStatus"
          }
        },
        "type" : "object"
      },
      "commands.CreateContext" : {
        "properties" : {
          "clusterId" : {
            "description" : "Running cluster id",
            "type" : "string"
          },
          "language" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/commands.Language"
          }
        },
        "type" : "object"
      },
      "commands.Created" : {
        "properties" : {
          "id" : {
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "commands.DestroyContext" : {
        "required" : [ "clusterId", "contextId" ],
        "properties" : {
          "clusterId" : {
            "type" : "string"
          },
          "contextId" : {
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "commands.Language" : {
        "type" : "string",
        "enum" : [ "python", "scala", "sql" ]
      },
      "commands.ResultType" : {
        "type" : "string",
        "enum" : [ "error", "image", "images", "table", "text" ]
      },
      "commands.Results" : {
        "properties" : {
          "resultType" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/commands.ResultType"
          },
          "data" : {
            "properties" : { },
            "type" : "object"
          },
          "pos" : {
            "description" : "internal field used by SDK",
            "type" : "integer"
          },
          "truncated" : {
            "description" : "true if partial results are returned.",
            "type" : "boolean"
          },
          "fileName" : {
            "description" : "The image filename",
            "type" : "string"
          },
          "fileNames" : {
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "isJsonSchema" : {
            "description" : "true if a JSON schema is returned instead of a string representation of the Hive type.",
            "type" : "boolean"
          },
          "schema" : {
            "description" : "The table schema",
            "type" : "array",
            "items" : {
              "type" : "object",
              "additionalProperties" : {
                "type" : "object",
                "x-databricks-any" : true
              }
            }
          },
          "cause" : {
            "description" : "The cause of the error",
            "type" : "string"
          },
          "summary" : {
            "description" : "The summary of the error",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "dbfs.AddBlock" : {
        "required" : [ "handle", "data" ],
        "properties" : {
          "data" : {
            "format" : "string",
            "description" : "The base64-encoded data to append to the stream. This has a limit of 1 MB.",
            "x-databricks-base64" : true,
            "type" : "string"
          },
          "handle" : {
            "format" : "int64",
            "description" : "The handle on an open stream.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "dbfs.AddBlockResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "dbfs.Close" : {
        "required" : [ "handle" ],
        "properties" : {
          "handle" : {
            "format" : "int64",
            "description" : "The handle on an open stream.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "dbfs.CloseResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "dbfs.Create" : {
        "required" : [ "path" ],
        "properties" : {
          "overwrite" : {
            "default" : "false",
            "description" : "The flag that specifies whether to overwrite existing file/files.",
            "type" : "boolean"
          },
          "path" : {
            "example" : "/mnt/foo",
            "description" : "The path of the new file. The path should be the absolute DBFS path.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "dbfs.CreateResponse" : {
        "properties" : {
          "handle" : {
            "format" : "int64",
            "description" : "Handle which should subsequently be passed into the AddBlock and Close calls when writing to a file through a stream.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "dbfs.Delete" : {
        "required" : [ "path" ],
        "properties" : {
          "path" : {
            "example" : "/mnt/foo",
            "description" : "The path of the file or directory to delete. The path should be the absolute DBFS path.",
            "type" : "string"
          },
          "recursive" : {
            "default" : "false",
            "description" : "Whether or not to recursively delete the directory's contents. Deleting empty directories can be done without providing the recursive flag.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "dbfs.DeleteResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "dbfs.FileInfo" : {
        "properties" : {
          "file_size" : {
            "format" : "int64",
            "description" : "The length of the file in bytes or zero if the path is a directory.",
            "type" : "integer"
          },
          "is_dir" : {
            "description" : "True if the path is a directory.",
            "type" : "boolean"
          },
          "modification_time" : {
            "format" : "int64",
            "description" : "Last modification time of given file/dir in milliseconds since Epoch.",
            "type" : "integer"
          },
          "path" : {
            "description" : "The path of the file or directory.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "dbfs.ListStatusResponse" : {
        "properties" : {
          "files" : {
            "description" : "A list of FileInfo's that describe contents of directory or file. See example above.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/dbfs.FileInfo"
            }
          }
        },
        "type" : "object"
      },
      "dbfs.MkDirs" : {
        "required" : [ "path" ],
        "properties" : {
          "path" : {
            "example" : "/mnt/foo",
            "description" : "The path of the new directory. The path should be the absolute DBFS path.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "dbfs.MkDirsResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "dbfs.Move" : {
        "required" : [ "source_path", "destination_path" ],
        "properties" : {
          "destination_path" : {
            "description" : "The destination path of the file or directory. The path should be the absolute DBFS path.",
            "type" : "string"
          },
          "source_path" : {
            "example" : "/mnt/foo",
            "description" : "The source path of the file or directory. The path should be the absolute DBFS path.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "dbfs.MoveResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "dbfs.Put" : {
        "required" : [ "path" ],
        "properties" : {
          "contents" : {
            "format" : "string",
            "description" : "This parameter might be absent, and instead a posted file will be used.",
            "x-databricks-base64" : true,
            "type" : "string"
          },
          "overwrite" : {
            "default" : "false",
            "description" : "The flag that specifies whether to overwrite existing file/files.",
            "type" : "boolean"
          },
          "path" : {
            "example" : "/mnt/foo",
            "description" : "The path of the new file. The path should be the absolute DBFS path.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "dbfs.PutResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "dbfs.ReadResponse" : {
        "properties" : {
          "bytes_read" : {
            "format" : "int64",
            "description" : "The number of bytes read (could be less than `length` if we hit end of file). This refers to number of\nbytes read in unencoded version (response data is base64-encoded).\n",
            "type" : "integer"
          },
          "data" : {
            "format" : "string",
            "description" : "The base64-encoded contents of the file read.",
            "x-databricks-base64" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "deployment.AwsCredentials" : {
        "properties" : {
          "sts_role" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.StsRole"
          }
        },
        "type" : "object"
      },
      "deployment.AwsKeyInfo" : {
        "required" : [ "key_arn", "key_region" ],
        "properties" : {
          "key_alias" : {
            "example" : "alias/projectKey1",
            "description" : "The AWS KMS key alias.",
            "type" : "string"
          },
          "key_arn" : {
            "example" : "arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321",
            "description" : "The AWS KMS key's Amazon Resource Name (ARN).",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "key_region" : {
            "example" : "us-east-1",
            "description" : "The AWS KMS key region.",
            "type" : "string"
          },
          "reuse_key_for_cluster_volumes" : {
            "example" : true,
            "description" : "This field applies only if the `use_cases` property includes `STORAGE`. If this is set to `true` or omitted, the key is also used to encrypt cluster EBS volumes. If you do not want to use this key for encrypting EBS volumes, set to `false`.",
            "type" : "boolean"
          }
        },
        "x-databricks-name" : true,
        "type" : "object"
      },
      "deployment.CloudResourceContainer" : {
        "properties" : {
          "gcp" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.CustomerFacingGcpCloudResourceContainer"
          }
        },
        "description" : "The general workspace configurations that are specific to cloud providers.",
        "type" : "object",
        "x-databricks-cloud" : "gcp"
      },
      "deployment.CreateAwsKeyInfo" : {
        "required" : [ "key_arn" ],
        "properties" : {
          "key_alias" : {
            "example" : "alias/projectKey1",
            "description" : "The AWS KMS key alias.",
            "type" : "string"
          },
          "key_arn" : {
            "example" : "arn:aws:kms:us-west-2:111122223333:key/0987dcba-09fe-87dc-65ba-ab0987654321",
            "description" : "The AWS KMS key's Amazon Resource Name (ARN). Note that the key's AWS region is inferred from the ARN.",
            "type" : "string"
          },
          "reuse_key_for_cluster_volumes" : {
            "example" : true,
            "description" : "This field applies only if the `use_cases` property includes `STORAGE`. If this is set to `true` or omitted, the key is also used to encrypt cluster EBS volumes. To not use this key also for encrypting EBS volumes, set this to `false`.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "deployment.CreateCredentialRequest" : {
        "required" : [ "credentials_name", "aws_credentials" ],
        "properties" : {
          "aws_credentials" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.AwsCredentials"
          },
          "credentials_name" : {
            "example" : "credential_1",
            "description" : "The human-readable name of the credential configuration object.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "deployment.CreateCustomerManagedKeyRequest" : {
        "required" : [ "aws_key_info", "use_cases" ],
        "properties" : {
          "aws_key_info" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.CreateAwsKeyInfo"
          },
          "use_cases" : {
            "example" : [ "MANAGED_SERVICES", "STORAGE" ],
            "description" : "The cases that the key can be used for.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/deployment.KeyUseCase"
            }
          }
        },
        "type" : "object"
      },
      "deployment.CreateNetworkRequest" : {
        "required" : [ "network_name" ],
        "properties" : {
          "vpc_endpoints" : {
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/deployment.NetworkVpcEndpoints"
          },
          "gcp_network_info" : {
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/deployment.GcpNetworkInfo"
          },
          "vpc_id" : {
            "description" : "The ID of the VPC associated with this network. VPC IDs can be used in multiple network configurations.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "security_group_ids" : {
            "description" : "IDs of one to five security groups associated with this network. Security group IDs **cannot** be used in multiple network configurations.",
            "type" : "array",
            "items" : {
              "description" : "ID of security groups associated with this network. Security group IDs *cannot** be used in multiple network configurations.",
              "type" : "string"
            },
            "x-databricks-cloud" : "aws"
          },
          "network_name" : {
            "description" : "The human-readable name of the network configuration.",
            "type" : "string"
          },
          "subnet_ids" : {
            "description" : "IDs of at least two subnets associated with this network. Subnet IDs **cannot** be used in multiple network configurations.",
            "type" : "array",
            "items" : {
              "description" : "ID of subnet associated with this network. Subnet IDs **cannot** be used in multiple network configurations.",
              "type" : "string"
            },
            "x-databricks-cloud" : "aws"
          }
        },
        "type" : "object"
      },
      "deployment.CreateStorageConfigurationRequest" : {
        "required" : [ "storage_configuration_name", "root_bucket_info" ],
        "properties" : {
          "root_bucket_info" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.RootBucketInfo"
          },
          "storage_configuration_name" : {
            "example" : "storage_conf_1",
            "description" : "The human-readable name of the storage configuration.",
            "x-databricks-name" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "deployment.CreateVPCEndpointRequest" : {
        "required" : [ "vpc_endpoint_name", "aws_vpc_endpoint_id", "region" ],
        "properties" : {
          "aws_vpc_endpoint_id" : {
            "description" : "The ID of the VPC endpoint object in AWS.",
            "type" : "string"
          },
          "region" : {
            "description" : "The AWS region in which this VPC endpoint object exists.",
            "type" : "string"
          },
          "vpc_endpoint_name" : {
            "description" : "The human-readable name of the storage configuration.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "deployment.CreateWorkspaceRequest" : {
        "required" : [ "workspace_name" ],
        "properties" : {
          "aws_region" : {
            "example" : "us-west-2",
            "description" : "The AWS region of the workspace's data plane.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "network_id" : {
            "example" : "fd0cc5bc-683c-47e9-b15e-144d7744a496",
            "format" : "uuid",
            "type" : "string"
          },
          "location" : {
            "example" : "us-east4",
            "description" : "The Google Cloud region of the workspace data plane in your Google account. For example, `us-east4`.",
            "type" : "string",
            "x-databricks-cloud" : "gcp"
          },
          "deployment_name" : {
            "example" : "workspace_1",
            "description" : "The deployment name defines part of the subdomain for the workspace. The workspace URL for web application and REST APIs is `<workspace-deployment-name>.cloud.databricks.com`. For example, if the deployment name is `abcsales`, your workspace URL will be `https://abcsales.cloud.databricks.com`. Hyphens are allowed.  This property supports only the set of characters that are allowed in a subdomain.\n\nIf your account has a non-empty deployment name prefix at workspace creation time, the workspace deployment name changes so that the beginning has the account prefix and a hyphen. For example, if your account's deployment prefix is `acme` and the workspace deployment name is `workspace-1`, the `deployment_name` field becomes `acme-workspace-1` and that is the value that is returned in JSON responses for the `deployment_name` field. The workspace URL is `acme-workspace-1.cloud.databricks.com`.\n\nIf your account has a non-empty deployment name prefix and you set `deployment_name` to the reserved keyword `EMPTY`, `deployment_name` is just the account prefix only. For example, if your account's deployment prefix is `acme` and the workspace deployment name is `EMPTY`, `deployment_name` becomes `acme` only and the workspace URL is `acme.cloud.databricks.com`.\n\nContact your Databricks representatives to add an account deployment name prefix to your account. If you do not have a deployment name prefix, the special deployment name value `EMPTY` is invalid.\n\nThis value must be unique across all non-deleted deployments across all AWS regions.\n\nIf a new workspace omits this property, the server generates a unique deployment name for you with the pattern `dbc-xxxxxxxx-xxxx`.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "storage_customer_managed_key_id" : {
            "format" : "uuid",
            "description" : "The ID of the workspace's storage encryption key configuration object. This is used to encrypt the workspace's root S3 bucket (root DBFS and system data) and, optionally, cluster EBS volumes. The provided key configuration object property `use_cases` must contain `STORAGE`.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "cloud_resource_container" : {
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/deployment.CloudResourceContainer"
          },
          "workspace_name" : {
            "description" : "The workspace's human-readable name.",
            "type" : "string"
          },
          "cloud" : {
            "example" : "gcp",
            "description" : "The cloud provider which the workspace uses. For Google Cloud workspaces, always set this field to `gcp`.",
            "type" : "string",
            "x-databricks-cloud" : "gcp"
          },
          "credentials_id" : {
            "example" : "ccc64f28-ebdc-4c89-add9-5dcb6d7727d8",
            "format" : "uuid",
            "description" : "ID of the workspace's credential configuration object.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "pricing_tier" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.PricingTier"
          },
          "storage_configuration_id" : {
            "example" : "b43a6064-04c1-4e1c-88b6-d91e5b136b13",
            "format" : "uuid",
            "description" : "The ID of the workspace's storage configuration object.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "private_access_settings_id" : {
            "format" : "uuid",
            "description" : "ID of the workspace's private access settings object. Only used for PrivateLink (Public Preview). This ID must be specified for customers using [AWS PrivateLink](https://aws.amazon.com/privatelink/) for either front-end (user-to-workspace connection), back-end (data plane to control plane connection), or both connection types.\n\nBefore configuring PrivateLink, read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "managed_services_customer_managed_key_id" : {
            "example" : "849b3d6b-e68e-468d-b3e5-deb08b03c56d",
            "format" : "uuid",
            "description" : "The ID of the workspace's managed services encryption key configuration object. This is used to encrypt the workspace's notebook and secret data in the control plane, in addition to Databricks SQL queries and query history. The provided key configuration object property `use_cases` must contain `MANAGED_SERVICES`.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          }
        },
        "type" : "object"
      },
      "deployment.Credential" : {
        "properties" : {
          "creation_time" : {
            "format" : "int64",
            "description" : "Time in epoch milliseconds when the credential was created.",
            "x-databricks-computed" : true,
            "type" : "integer"
          },
          "aws_credentials" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.AwsCredentials"
          },
          "account_id" : {
            "format" : "uuid",
            "description" : "The Databricks account ID that hosts the credential.",
            "type" : "string"
          },
          "credentials_id" : {
            "format" : "uuid",
            "description" : "Databricks credential configuration ID.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "credentials_name" : {
            "description" : "The human-readable name of the credential configuration object.",
            "x-databricks-name" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "deployment.CustomerFacingGcpCloudResourceContainer" : {
        "properties" : {
          "project_id" : {
            "example" : "my-gcp-project",
            "description" : "The Google Cloud project ID, which the workspace uses to instantiate cloud resources for your workspace.",
            "type" : "string"
          }
        },
        "description" : "The general workspace configurations that are specific to Google Cloud.",
        "type" : "object",
        "x-databricks-cloud" : "gcp"
      },
      "deployment.CustomerManagedKey" : {
        "properties" : {
          "creation_time" : {
            "format" : "int64",
            "description" : "Time in epoch milliseconds when the customer key was created.",
            "x-databricks-computed" : true,
            "type" : "integer"
          },
          "account_id" : {
            "format" : "uuid",
            "description" : "The Databricks account ID that holds the customer-managed key.",
            "type" : "string"
          },
          "customer_managed_key_id" : {
            "format" : "uuid",
            "description" : "ID of the encryption key configuration object.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "use_cases" : {
            "example" : [ "MANAGED_SERVICES", "STORAGE" ],
            "description" : "The cases that the key can be used for.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/deployment.KeyUseCase"
            }
          },
          "aws_key_info" : {
            "extRef" : true,
            "ref" : true,
            "x-databricks-name" : true,
            "$ref" : "#/components/schemas/deployment.AwsKeyInfo"
          }
        },
        "type" : "object"
      },
      "deployment.EndpointUseCase" : {
        "description" : "This enumeration represents the type of Databricks VPC [endpoint service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html) that was used when creating this VPC endpoint.\n\nIf the VPC endpoint connects to the Databricks control plane for either the front-end connection or the back-end REST API connection, the value is `WORKSPACE_ACCESS`.\n\nIf the VPC endpoint connects to the Databricks workspace for the back-end [secure cluster connectivity](https://docs.databricks.com/security/secure-cluster-connectivity.html) relay, the value is `DATAPLANE_RELAY_ACCESS`.",
        "type" : "string",
        "enum" : [ "WORKSPACE_ACCESS", "DATAPLANE_RELAY_ACCESS" ]
      },
      "deployment.ErrorType" : {
        "description" : "The AWS resource associated with this error: credentials, VPC, subnet, security group, or network ACL.",
        "type" : "string",
        "enum" : [ "credentials", "vpc", "subnet", "securityGroup", "networkAcl" ]
      },
      "deployment.GcpManagedNetworkConfig" : {
        "properties" : {
          "gke_cluster_pod_ip_range" : {
            "default" : "10.1.0.0/16",
            "description" : "The IP range from which to allocate GKE cluster pods. No bigger than `/9` and no smaller than `/21`.",
            "type" : "string"
          },
          "gke_cluster_service_ip_range" : {
            "default" : "10.2.0.0/20",
            "description" : "The IP range from which to allocate GKE cluster services. No bigger than `/16` and no smaller than `/27`.",
            "type" : "string"
          },
          "subnet_cidr" : {
            "default" : "10.0.0.0/16",
            "description" : "The IP range from which to allocate GKE cluster nodes. No bigger than `/9` and no smaller than `/29`.",
            "type" : "string"
          }
        },
        "description" : "The network settings for the workspace. The configurations are only for Databricks-managed VPCs. It is ignored if you specify a customer-managed VPC in the `network_id` field.\",\nAll the IP range configurations must be mutually exclusive. An attempt to create a workspace\nfails if Databricks detects an IP range overlap.\n\nSpecify custom IP ranges in CIDR format. The IP ranges for these fields must not\n overlap, and all IP addresses must be entirely within the following ranges: `10.0.0.0/8`, `100.64.0.0/10`, `172.16.0.0/12`, `192.168.0.0/16`, and `240.0.0.0/4`.\n\nThe sizes of these IP ranges affect the maximum number of nodes for the workspace.\n\n**Important**: Confirm the IP ranges used by your Databricks workspace before creating the workspace.\nYou cannot change them after your workspace is deployed. If the IP address ranges\nfor your Databricks are too small, IP exhaustion can occur, causing your Databricks\njobs to fail. To determine the address range sizes that you need,\nDatabricks provides a calculator as a Microsoft Excel spreadsheet.\nSee [calculate subnet sizes for a new workspace](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/network-sizing.html).\n",
        "type" : "object",
        "x-databricks-cloud" : "gcp"
      },
      "deployment.GcpNetworkInfo" : {
        "required" : [ "network_project_id", "vpc_id", "subnet_region", "service_ip_range_name", "pod_ip_range_name", "subnet_ids" ],
        "properties" : {
          "network_project_id" : {
            "example" : "my-gcp-project-123",
            "description" : "The Google Cloud project ID of the VPC network.",
            "type" : "string"
          },
          "vpc_id" : {
            "example" : "my-vpc-123",
            "description" : "The ID of the VPC associated with this network. VPC IDs can be used in multiple network configurations.",
            "type" : "string"
          },
          "subnet_region" : {
            "example" : "us-east4",
            "description" : "The Google Cloud region of the workspace data plane (for example, `us-east4`).",
            "type" : "string"
          },
          "service_ip_range_name" : {
            "example" : "my-service-ip-range-123",
            "description" : "The name of the secondary IP range for services. A Databricks-managed GKE cluster uses this IP range for its services. This secondary IP range can be used by only one workspace.",
            "type" : "string"
          },
          "pod_ip_range_name" : {
            "example" : "my-pod-ip-range-123",
            "description" : "The name of the secondary IP range for pods. A Databricks-managed GKE cluster uses this IP range for its pods. This secondary IP range can be used by only one workspace.",
            "type" : "string"
          },
          "subnet_id" : {
            "example" : "my-subnet-123",
            "description" : "The ID of the subnet associated with this network.",
            "type" : "string"
          }
        },
        "description" : "The Google Cloud specific information for this network (for example, the VPC ID, subnet ID, and secondary IP ranges).",
        "type" : "object",
        "x-databricks-cloud" : "gcp"
      },
      "deployment.GkeConfig" : {
        "properties" : {
          "connectivity_type" : {
            "description" : "Specifies the network connectivity types for the GKE nodes and the GKE master network. \\n\n\nSet to `PRIVATE_NODE_PUBLIC_MASTER` for a private GKE cluster\nfor the workspace. The GKE nodes will not have public IPs.\\n\n\nSet to `PUBLIC_NODE_PUBLIC_MASTER` for a public GKE cluster.\nThe nodes of a public GKE cluster have public IP addresses.\n",
            "type" : "string",
            "enum" : [ "PRIVATE_NODE_PUBLIC_MASTER", "PUBLIC_NODE_PUBLIC_MASTER" ]
          },
          "master_ip_range" : {
            "default" : "10.3.0.0/28",
            "description" : "The IP range from which to allocate GKE cluster master resources. This field will be ignored if GKE private cluster is not enabled.\n\nIt must be exactly as big as `/28`.",
            "type" : "string"
          }
        },
        "description" : "The configurations for the GKE cluster of a Databricks workspace.",
        "type" : "object",
        "x-databricks-cloud" : "gcp"
      },
      "deployment.KeyUseCase" : {
        "example" : "STORAGE",
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "MANAGED_SERVICES", "STORAGE" ],
        "x-databricks-enum-descriptions" : {
          "MANAGED_SERVICES" : "Encrypts notebook and secret data in the control plane",
          "STORAGE" : "Encrypts the workspace's root S3 bucket (root DBFS and system data) and, optionally, cluster EBS volumes."
        }
      },
      "deployment.ListCredentialsResponse" : {
        "description" : "List of credential configuration objects.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/deployment.Credential"
        }
      },
      "deployment.ListCustomerManagedKeysResponse" : {
        "description" : "Array of key configuration objects.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/deployment.CustomerManagedKey"
        }
      },
      "deployment.ListNetworksResponse" : {
        "description" : "Array of network configuration objects.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/deployment.Network"
        }
      },
      "deployment.ListPrivateAccessSettingsResponse" : {
        "description" : "Private access settings objects.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/deployment.PrivateAccessSettings"
        }
      },
      "deployment.ListStorageConfigurationsResponse" : {
        "description" : "Storage configuration objects.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/deployment.StorageConfiguration"
        }
      },
      "deployment.ListVPCEndpointsResponse" : {
        "description" : "List VPC endpoint configurations.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/deployment.VPCEndpoint"
        }
      },
      "deployment.ListWorkspacesResponse" : {
        "description" : "An array of workspaces.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/deployment.Workspace"
        }
      },
      "deployment.Network" : {
        "properties" : {
          "creation_time" : {
            "example" : 1643668346544,
            "format" : "int64",
            "description" : "Time in epoch milliseconds when the network was created.",
            "x-databricks-computed" : true,
            "type" : "integer"
          },
          "vpc_endpoints" : {
            "extRef" : true,
            "ref" : true,
            "x-databricks-computed" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/deployment.NetworkVpcEndpoints"
          },
          "network_id" : {
            "format" : "uuid",
            "description" : "The Databricks network configuration ID.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "gcp_network_info" : {
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/deployment.GcpNetworkInfo"
          },
          "warning_messages" : {
            "description" : "Array of warning messages about the network configuration.",
            "x-databricks-computed" : true,
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/deployment.NetworkWarning"
            },
            "x-databricks-cloud" : "aws"
          },
          "vpc_id" : {
            "description" : "The ID of the VPC associated with this network configuration. VPC IDs can be used in multiple networks.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "security_group_ids" : {
            "type" : "array",
            "items" : {
              "description" : "ID of a security group associated with this network configuration. Security group IDs **cannot** be used in multiple networks.",
              "type" : "string"
            },
            "x-databricks-cloud" : "aws"
          },
          "vpc_status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.VpcStatus"
          },
          "error_messages" : {
            "description" : "Array of error messages about the network configuration.",
            "x-databricks-computed" : true,
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/deployment.NetworkHealth"
            },
            "x-databricks-cloud" : "aws"
          },
          "account_id" : {
            "format" : "uuid",
            "description" : "The Databricks account ID associated with this network configuration.",
            "type" : "string"
          },
          "network_name" : {
            "example" : "my-databricks-network",
            "description" : "The human-readable name of the network configuration.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "subnet_ids" : {
            "type" : "array",
            "items" : {
              "description" : "The ID of a subnet associated with this network configuration. Subnet IDs **cannot** be used in multiple network configurations.",
              "type" : "string"
            },
            "x-databricks-cloud" : "aws"
          },
          "workspace_id" : {
            "example" : 1614665312930232,
            "format" : "int64",
            "description" : "Workspace ID associated with this network configuration.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "deployment.NetworkHealth" : {
        "properties" : {
          "error_message" : {
            "description" : "Details of the error.",
            "type" : "string"
          },
          "error_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.ErrorType"
          }
        },
        "type" : "object"
      },
      "deployment.NetworkVpcEndpoints" : {
        "required" : [ "rest_api", "dataplane_relay" ],
        "properties" : {
          "dataplane_relay" : {
            "properties" : { },
            "description" : "The VPC endpoint ID used by this network to access the Databricks secure cluster connectivity relay. See [Secure Cluster Connectivity](https://docs.databricks.com/security/secure-cluster-connectivity.html).\n\nThis is a list type for future compatibility, but currently only one VPC endpoint ID should be supplied.\n\n**Note**: This is the Databricks-specific ID of the VPC endpoint object in the Account API, not the AWS VPC endpoint ID that you see for your endpoint in the AWS Console.",
            "type" : "object",
            "items" : {
              "format" : "uuid",
              "type" : "string"
            }
          },
          "rest_api" : {
            "properties" : { },
            "description" : "The VPC endpoint ID used by this network to access the Databricks REST API. Databricks clusters make calls to our REST API as part of cluster creation, mlflow tracking, and many other features. Thus, this is required even if your workspace allows public access to the REST API.\n\nThis is a list type for future compatibility, but currently only one VPC endpoint ID should be supplied.\n\n**Note**: This is the Databricks-specific ID of the VPC endpoint object in the Account API, not the AWS VPC endpoint ID that you see for your endpoint in the AWS Console.",
            "type" : "object",
            "items" : {
              "format" : "uuid",
              "type" : "string"
            }
          }
        },
        "description" : "If specified, contains the VPC endpoints used to allow cluster communication from this VPC over [AWS PrivateLink](https://aws.amazon.com/privatelink/).",
        "type" : "object"
      },
      "deployment.NetworkWarning" : {
        "properties" : {
          "warning_message" : {
            "description" : "Details of the warning.",
            "type" : "string"
          },
          "warning_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.WarningType"
          }
        },
        "type" : "object"
      },
      "deployment.PricingTier" : {
        "example" : "PREMIUM",
        "description" : "The pricing tier of the workspace.\nFor pricing tier information, see [AWS Pricing](https://databricks.com/product/aws-pricing).\n",
        "type" : "string",
        "enum" : [ "UNKNOWN", "COMMUNITY_EDITION", "STANDARD", "PREMIUM", "ENTERPRISE", "DEDICATED" ]
      },
      "deployment.PrivateAccessLevel" : {
        "example" : "ENDPOINT",
        "description" : "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object.\n* `ANY` (deprecated): Any VPC endpoint can connect to your workspace.\n* `ACCOUNT` level access (the default) allows only VPC endpoints that are registered in your Databricks account connect to your workspace.\n* `ENDPOINT` level access allows only specified VPC endpoints connect to your workspace. For details, see `allowed_vpc_endpoint_ids`.",
        "type" : "string",
        "enum" : [ "ANY", "ACCOUNT", "ENDPOINT" ]
      },
      "deployment.PrivateAccessSettings" : {
        "properties" : {
          "allowed_vpc_endpoint_ids" : {
            "description" : "An array of Databricks VPC endpoint IDs. This is the Databricks ID returned when registering the VPC endpoint configuration in your Databricks account. This is _not_ the ID of the VPC endpoint in AWS.\n\nOnly used when `private_access_level` is set to `ENDPOINT`. This is an allow list of VPC endpoints registered in your Databricks account that can connect to your workspace over AWS PrivateLink.\n\n**Note**: If hybrid access to your workspace is enabled by setting `public_access_enabled` to `true`, this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see [IP access lists](https://docs.databricks.com/security/network/ip-access-list.html).",
            "type" : "array",
            "items" : {
              "format" : "uuid",
              "type" : "string"
            }
          },
          "account_id" : {
            "format" : "uuid",
            "description" : "The Databricks account ID that hosts the credential.",
            "type" : "string"
          },
          "private_access_settings_id" : {
            "format" : "uuid",
            "description" : "Databricks private access settings ID.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "region" : {
            "description" : "The AWS region for workspaces attached to this private access settings object.",
            "type" : "string"
          },
          "public_access_enabled" : {
            "description" : "Determines if the workspace can be accessed over public internet. For fully private workspaces, you can optionally specify `false`, but only if you implement both the front-end and the back-end PrivateLink connections. Otherwise, specify `true`, which means that public access is enabled.",
            "type" : "boolean"
          },
          "private_access_settings_name" : {
            "description" : "The human-readable name of the private access settings object.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "private_access_level" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.PrivateAccessLevel"
          }
        },
        "type" : "object"
      },
      "deployment.RootBucketInfo" : {
        "properties" : {
          "bucket_name" : {
            "description" : "The name of the S3 bucket.",
            "type" : "string"
          }
        },
        "description" : "Root S3 bucket information.",
        "type" : "object"
      },
      "deployment.StorageConfiguration" : {
        "properties" : {
          "creation_time" : {
            "format" : "int64",
            "description" : "Time in epoch milliseconds when the storage configuration was created.",
            "x-databricks-computed" : true,
            "type" : "integer"
          },
          "storage_configuration_name" : {
            "description" : "The human-readable name of the storage configuration.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "account_id" : {
            "format" : "uuid",
            "description" : "The Databricks account ID that hosts the credential.",
            "x-databricks-computed" : true,
            "type" : "string"
          },
          "root_bucket_info" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.RootBucketInfo"
          },
          "storage_configuration_id" : {
            "format" : "uuid",
            "description" : "Databricks storage configuration ID.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "deployment.StsRole" : {
        "properties" : {
          "external_id" : {
            "description" : "The external ID that needs to be trusted by the cross-account role. This is always your Databricks account ID.",
            "type" : "string"
          },
          "role_arn" : {
            "example" : "arn-aws-iam::111110000000:role/test_role",
            "description" : "The Amazon Resource Name (ARN) of the cross account role.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "deployment.UpdateWorkspaceRequest" : {
        "properties" : {
          "aws_region" : {
            "example" : "us-west-2",
            "description" : "The AWS region of the workspace's data plane (for example, `us-west-2`). This parameter is available only for updating failed workspaces.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "network_id" : {
            "format" : "uuid",
            "description" : "The ID of the workspace's network configuration object. Used only if you already use a customer-managed VPC. This change is supported only if you specified a network configuration ID when the workspace was created. In other words, you cannot switch from a Databricks-managed VPC to a customer-managed VPC. This parameter is available for updating both failed and running workspaces. **Note**: You cannot use a network configuration update in this API to add support for PrivateLink (Public Preview). To add PrivateLink to an existing workspace, contact your Databricks representative.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "storage_customer_managed_key_id" : {
            "format" : "uuid",
            "description" : "The ID of the key configuration object for workspace storage. This parameter is available for updating both failed and running workspaces.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "credentials_id" : {
            "format" : "uuid",
            "description" : "ID of the workspace's credential configuration object. This parameter is available for updating both failed and running workspaces.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "storage_configuration_id" : {
            "format" : "uuid",
            "description" : "The ID of the workspace's storage configuration object. This parameter is available only for updating failed workspaces.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "managed_services_customer_managed_key_id" : {
            "format" : "uuid",
            "description" : "The ID of the workspace's managed services encryption key configuration object. This parameter is available only for updating failed workspaces.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          }
        },
        "type" : "object",
        "x-databricks-cloud" : "aws"
      },
      "deployment.UpsertPrivateAccessSettingsRequest" : {
        "required" : [ "private_access_settings_name", "region" ],
        "properties" : {
          "allowed_vpc_endpoint_ids" : {
            "description" : "An array of Databricks VPC endpoint IDs. This is the Databricks ID that is returned when registering the VPC endpoint configuration in your Databricks account. This is not the ID of the VPC endpoint in AWS.\n\nOnly used when `private_access_level` is set to `ENDPOINT`. This is an allow list of VPC endpoints that in your account that can connect to your workspace over AWS PrivateLink.\n\nIf hybrid access to your workspace is enabled by setting `public_access_enabled` to `true`, this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see [IP access lists](https://docs.databricks.com/security/network/ip-access-list.html).",
            "type" : "array",
            "items" : {
              "format" : "uuid",
              "type" : "string"
            }
          },
          "region" : {
            "description" : "The AWS region for workspaces associated with this private access settings object. This must be a [region that Databricks supports for PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/regions.html).",
            "type" : "string"
          },
          "public_access_enabled" : {
            "default" : false,
            "description" : "Determines if the workspace can be accessed over public internet. For fully private workspaces, you can optionally specify `false`, but only if you implement both the front-end and the back-end PrivateLink connections. Otherwise, specify `true`, which means that public access is enabled.",
            "type" : "boolean"
          },
          "private_access_settings_name" : {
            "description" : "The human-readable name of the private access settings object.",
            "type" : "string"
          },
          "private_access_level" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.PrivateAccessLevel"
          }
        },
        "type" : "object"
      },
      "deployment.VPCEndpoint" : {
        "properties" : {
          "aws_vpc_endpoint_id" : {
            "description" : "The ID of the VPC endpoint object in AWS.",
            "type" : "string"
          },
          "state" : {
            "description" : "The current state (such as `available` or `rejected`) of the VPC endpoint. Derived from AWS. For the full set of values, see [AWS DescribeVpcEndpoint documentation](https://docs.aws.amazon.com/cli/latest/reference/ec2/describe-vpc-endpoints.html).",
            "type" : "string"
          },
          "aws_account_id" : {
            "description" : "The AWS Account in which the VPC endpoint object exists.",
            "type" : "string"
          },
          "aws_endpoint_service_id" : {
            "description" : "The ID of the Databricks [endpoint service](https://docs.aws.amazon.com/vpc/latest/privatelink/endpoint-service.html) that this VPC endpoint is connected to. For a list of endpoint service IDs for each supported AWS region, see the [Databricks PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).",
            "type" : "string"
          },
          "account_id" : {
            "format" : "uuid",
            "description" : "The Databricks account ID that hosts the VPC endpoint configuration.",
            "type" : "string"
          },
          "vpc_endpoint_name" : {
            "description" : "The human-readable name of the storage configuration.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "region" : {
            "description" : "The AWS region in which this VPC endpoint object exists.",
            "type" : "string"
          },
          "use_case" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.EndpointUseCase"
          },
          "vpc_endpoint_id" : {
            "format" : "uuid",
            "description" : "Databricks VPC endpoint ID. This is the Databricks-specific name of the VPC endpoint. Do not confuse this with the `aws_vpc_endpoint_id`, which is the ID within AWS of the VPC endpoint.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "deployment.VpcStatus" : {
        "description" : "This describes an enum",
        "x-databricks-computed" : true,
        "type" : "string",
        "enum" : [ "UNATTACHED", "VALID", "BROKEN", "WARNED" ],
        "x-databricks-enum-descriptions" : {
          "UNATTACHED" : "Unattached.",
          "VALID" : "Valid.",
          "BROKEN" : "Broken.",
          "WARNED" : "Warned."
        }
      },
      "deployment.WarningType" : {
        "description" : "The AWS resource associated with this warning: a subnet or a security group.",
        "type" : "string",
        "enum" : [ "subnet", "securityGroup" ]
      },
      "deployment.Workspace" : {
        "properties" : {
          "aws_region" : {
            "description" : "The AWS region of the workspace data plane (for example, `us-west-2`).",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "creation_time" : {
            "format" : "int64",
            "description" : "Time in epoch milliseconds when the workspace was created.",
            "x-databricks-computed" : true,
            "type" : "integer"
          },
          "gke_config" : {
            "extRef" : true,
            "ref" : true,
            "type" : "object",
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/deployment.GkeConfig"
          },
          "network_id" : {
            "format" : "uuid",
            "description" : "The network configuration ID that is attached to the workspace. This field is available only if the network is a customer-managed network.",
            "type" : "string"
          },
          "location" : {
            "description" : "The Google Cloud region of the workspace data plane in your Google account (for example, `us-east4`).",
            "type" : "string",
            "x-databricks-cloud" : "gcp"
          },
          "deployment_name" : {
            "description" : "The deployment name defines part of the subdomain for the workspace. The workspace URL for web application and REST APIs is `<deployment-name>.cloud.databricks.com`.\n\nThis value must be unique across all non-deleted deployments across all AWS regions.",
            "type" : "string"
          },
          "workspace_status_message" : {
            "example" : "Workspace resources are being set up.",
            "description" : "Message describing the current workspace status.",
            "x-databricks-computed" : true,
            "type" : "string"
          },
          "storage_customer_managed_key_id" : {
            "format" : "uuid",
            "description" : "ID of the key configuration for encrypting workspace storage.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "cloud_resource_container" : {
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/deployment.CloudResourceContainer"
          },
          "workspace_name" : {
            "description" : "The human-readable name of the workspace.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "account_id" : {
            "format" : "uuid",
            "description" : "Databricks account ID.",
            "type" : "string"
          },
          "cloud" : {
            "example" : "gcp",
            "description" : "The cloud name. This field always has the value `gcp`.",
            "type" : "string",
            "x-databricks-cloud" : "gcp"
          },
          "credentials_id" : {
            "format" : "uuid",
            "description" : "ID of the workspace's credential configuration object.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "pricing_tier" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.PricingTier"
          },
          "storage_configuration_id" : {
            "format" : "uuid",
            "description" : "ID of the workspace's storage configuration object.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "private_access_settings_id" : {
            "format" : "uuid",
            "description" : "ID of the workspace's private access settings object. Only used for PrivateLink (Public Preview). You must specify this ID if you are using [AWS PrivateLink](https://aws.amazon.com/privatelink/) for either front-end (user-to-workspace connection), back-end (data plane to control plane connection), or both connection types.\n\nBefore configuring PrivateLink, read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "managed_services_customer_managed_key_id" : {
            "format" : "uuid",
            "description" : "ID of the key configuration for encrypting managed services.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "workspace_status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/deployment.WorkspaceStatus"
          },
          "gcp_managed_network_config" : {
            "extRef" : true,
            "ref" : true,
            "type" : "object",
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/deployment.GcpManagedNetworkConfig"
          },
          "workspace_id" : {
            "example" : 1614665312930232,
            "format" : "int64",
            "description" : "A unique integer ID for the workspace",
            "x-databricks-id" : true,
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "deployment.WorkspaceStatus" : {
        "example" : "RUNNING",
        "description" : "The status of the workspace.\nFor workspace creation, usually it is set to `PROVISIONING` initially.\nContinue to check the status until the status is `RUNNING`.\n",
        "x-databricks-computed" : true,
        "type" : "string",
        "enum" : [ "NOT_PROVISIONED", "PROVISIONING", "RUNNING", "FAILED", "BANNED", "CANCELLING" ]
      },
      "endpoints.AnyValue" : {
        "properties" : { },
        "type" : "object"
      },
      "endpoints.BuildLogsResponse" : {
        "required" : [ "logs" ],
        "properties" : {
          "logs" : {
            "example" : "- conda-forge\ndependencies:\n- python=3.9.5\n- pip<=21.2.4\n- pip:\n  - mlflow\n  - cloudpickle==2.2.0\nname: mlflow-env\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\nInstalling pip dependencies: ...working... done\n",
            "description" : "The logs associated with building the served model's environment.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "endpoints.CreateServingEndpoint" : {
        "required" : [ "name", "config" ],
        "properties" : {
          "config" : {
            "description" : "The core config of the serving endpoint.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/endpoints.EndpointCoreConfigInput"
          },
          "name" : {
            "example" : "feed-ads",
            "description" : "The name of the serving endpoint. This field is required and must be unique across a Databricks Workspace.\nAn endpoint name can consist of alphanumeric characters, dashes, and underscores.\n",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "endpoints.EndpointCoreConfigInput" : {
        "required" : [ "served_models" ],
        "properties" : {
          "served_models" : {
            "description" : "A list of served models for the endpoint to serve. A serving endpoint can have up to 10 served models.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.ServedModelInput"
            }
          },
          "traffic_config" : {
            "description" : "The traffic config defining how invocations to the serving endpoint should be routed.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/endpoints.TrafficConfig"
          }
        },
        "type" : "object"
      },
      "endpoints.EndpointCoreConfigOutput" : {
        "properties" : {
          "config_version" : {
            "format" : "int32",
            "description" : "The config version that the serving endpoint is currently serving.",
            "type" : "integer"
          },
          "served_models" : {
            "description" : "The list of served models under the serving endpoint config.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.ServedModelOutput"
            }
          },
          "traffic_config" : {
            "description" : "The traffic configuration associated with the serving endpoint config.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/endpoints.TrafficConfig"
          }
        },
        "type" : "object"
      },
      "endpoints.EndpointCoreConfigSummary" : {
        "properties" : {
          "served_models" : {
            "description" : "The list of served models under the serving endpoint config.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.ServedModelSpec"
            }
          }
        },
        "type" : "object"
      },
      "endpoints.EndpointPendingConfig" : {
        "properties" : {
          "config_version" : {
            "format" : "int32",
            "description" : "The config version that the serving endpoint is currently serving.",
            "type" : "integer"
          },
          "served_models" : {
            "description" : "The list of served models belonging to the last issued update to the serving endpoint.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.ServedModelOutput"
            }
          },
          "start_time" : {
            "format" : "int64",
            "description" : "The timestamp when the update to the pending config started.",
            "type" : "integer"
          },
          "traffic_config" : {
            "description" : "The traffic config defining how invocations to the serving endpoint should be routed.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/endpoints.TrafficConfig"
          }
        },
        "type" : "object"
      },
      "endpoints.EndpointState" : {
        "properties" : {
          "config_update" : {
            "description" : "The state of an endpoint's config update. This informs the user if the pending_config \nis in progress, if the update failed, or if there is no update in progress.\nNote that if the endpoint's config_update state value is IN_PROGRESS, another update\ncan not be made until the update completes or fails.\"\n",
            "type" : "string",
            "enum" : [ "NOT_UPDATING", "IN_PROGRESS", "UPDATE_FAILED" ]
          },
          "ready" : {
            "description" : "The state of an endpoint, indicating whether or not the endpoint is queryable.\nAn endpoint is READY if all of the served models in its active configuration are ready.\nIf any of the actively served models are in a non-ready state, the endpoint state will be NOT_READY.\n",
            "type" : "string",
            "enum" : [ "READY", "NOT_READY" ]
          }
        },
        "type" : "object"
      },
      "endpoints.ListEndpointsResponse" : {
        "properties" : {
          "endpoints" : {
            "description" : "The list of endpoints.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.ServingEndpoint"
            }
          }
        },
        "type" : "object"
      },
      "endpoints.QueryEndpointResponse" : {
        "required" : [ "predictions" ],
        "properties" : {
          "predictions" : {
            "example" : [ 1.24, 2.3, 6.2, 3.4 ],
            "description" : "The predictions returned by the serving endpoint.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.AnyValue"
            }
          }
        },
        "type" : "object"
      },
      "endpoints.Route" : {
        "required" : [ "served_model_name", "traffic_percentage" ],
        "properties" : {
          "served_model_name" : {
            "example" : "ads-model-3",
            "description" : "The name of the served model this route configures traffic for.",
            "type" : "string"
          },
          "traffic_percentage" : {
            "example" : 100,
            "description" : "The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "endpoints.ServedModelInput" : {
        "required" : [ "model_name", "model_version", "workload_size", "scale_to_zero_enabled" ],
        "properties" : {
          "name" : {
            "example" : "ads-model-3",
            "description" : "The name of a served model. It must be unique across an endpoint. If not specified, this field will default to <model-name>-<model-version>.\nA served model name can consist of alphanumeric characters, dashes, and underscores.\n",
            "type" : "string"
          },
          "scale_to_zero_enabled" : {
            "example" : false,
            "description" : "Whether the compute resources for the served model should scale down to zero.",
            "type" : "boolean"
          },
          "model_version" : {
            "example" : "3",
            "description" : "The version of the model in Databricks Model Registry to be served.",
            "type" : "string"
          },
          "model_name" : {
            "example" : "ads-model",
            "description" : "The name of the model in Databricks Model Registry to be served.",
            "type" : "string"
          },
          "workload_size" : {
            "example" : "Small",
            "description" : "The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between.\nA single unit of provisioned concurrency can process one request at a time.\nValid workload sizes are \"Small\" (4 - 4 provisioned concurrency), \"Medium\" (8 - 16 provisioned concurrency), and \"Large\" (16 - 64 provisioned concurrency).\nIf scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0.\n",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "endpoints.ServedModelOutput" : {
        "properties" : {
          "name" : {
            "example" : "ads-model-3",
            "description" : "The name of the served model.",
            "type" : "string"
          },
          "state" : {
            "description" : "Information corresponding to the state of the Served Model.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/endpoints.ServedModelState"
          },
          "scale_to_zero_enabled" : {
            "example" : false,
            "description" : "Whether the compute resources for the Served Model should scale down to zero.",
            "type" : "boolean"
          },
          "creator" : {
            "description" : "The email of the user who created the served model.",
            "type" : "string"
          },
          "creation_timestamp" : {
            "format" : "int64",
            "description" : "The creation timestamp of the served model in Unix time.",
            "type" : "integer"
          },
          "model_version" : {
            "example" : "3",
            "description" : "The version of the model in Databricks Model Registry.",
            "type" : "string"
          },
          "model_name" : {
            "example" : "ads-model",
            "description" : "The name of the model in Databricks Model Registry.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "endpoints.ServedModelSpec" : {
        "properties" : {
          "model_name" : {
            "example" : "ads-model",
            "description" : "The name of the model in Databricks Model Registry.",
            "type" : "string"
          },
          "model_version" : {
            "example" : "3",
            "description" : "The version of the model in Databricks Model Registry.",
            "type" : "string"
          },
          "name" : {
            "example" : "ads-model-3",
            "description" : "The name of the served model.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "endpoints.ServedModelState" : {
        "properties" : {
          "deployment" : {
            "description" : "The state of the served model deployment.\nDEPLOYMENT_CREATING indicates that the served model is not ready yet because the deployment is still being\ncreated (i.e container image is building, model server is deploying for the first time, etc.).\nDEPLOYMENT_RECOVERING indicates that the served model was previously in a ready state but no longer is and is\nattempting to recover.\nDEPLOYMENT_READY indicates that the served model is ready to receive traffic.\nDEPLOYMENT_FAILED indicates that there was an error trying to bring up the served model (e.g container image\nbuild failed, the model server failed to start due to a model loading error, etc.)\nDEPLOYMENT_ABORTED indicates that the deployment was terminated likely due to a failure in bringing up another \nserved model under the same endpoint and config version.\n",
            "type" : "string",
            "enum" : [ "DEPLOYMENT_CREATING", "DEPLOYMENT_RECOVERING", "DEPLOYMENT_READY", "DEPLOYMENT_FAILED", "DEPLOYMENT_ABORTED" ]
          },
          "deployment_state_message" : {
            "description" : "More information about the state of the served model, if available.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "endpoints.ServerLogsResponse" : {
        "required" : [ "logs" ],
        "properties" : {
          "logs" : {
            "example" : "[hc4q8] [2023-01-10 19:12:58 +0000] [2] [INFO] Starting gunicorn 20.1.0\n[hc4q8] [2023-01-10 19:12:58 +0000] [2] [INFO] Listening at: http://0.0.0.0:8080 (2)\n[hc4q8] [2023-01-10 19:12:58 +0000] [2] [INFO] Using worker: sync\n[hc4q8] [2023-01-10 19:12:58 +0000] [3] [INFO] Booting worker with pid: 3\n[hc4q8] [2023-01-10 19:12:58 +0000] [4] [INFO] Booting worker with pid: 4\n[hc4q8] [2023-01-10 19:12:58 +0000] [5] [INFO] Booting worker with pid: 5\n[hc4q8] [2023-01-10 19:12:58 +0000] [6] [INFO] Booting worker with pid: 6\n",
            "description" : "The most recent log lines of the model server processing invocation requests.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "endpoints.ServingEndpoint" : {
        "properties" : {
          "name" : {
            "example" : "feed-ads",
            "description" : "The name of the serving endpoint.",
            "type" : "string"
          },
          "state" : {
            "description" : "Information corresponding to the state of the serving endpoint.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/endpoints.EndpointState"
          },
          "config" : {
            "description" : "The config that is currently being served by the endpoint.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/endpoints.EndpointCoreConfigSummary"
          },
          "creator" : {
            "example" : "alice@company.com",
            "description" : "The email of the user who created the serving endpoint.",
            "type" : "string"
          },
          "creation_timestamp" : {
            "format" : "int64",
            "description" : "The timestamp when the endpoint was created in Unix time.",
            "type" : "integer"
          },
          "id" : {
            "example" : "88fd3f75a0d24b0380ddc40484d7a31b",
            "description" : "System-generated ID of the endpoint. This is used to refer to the endpoint in the Permissions API",
            "type" : "string"
          },
          "last_updated_timestamp" : {
            "format" : "int64",
            "description" : "The timestamp when the endpoint was last updated by a user in Unix time.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "endpoints.ServingEndpointDetailed" : {
        "properties" : {
          "name" : {
            "example" : "feed-ads",
            "description" : "The name of the serving endpoint.",
            "type" : "string"
          },
          "pending_config" : {
            "description" : "The config that the endpoint is attempting to update to.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/endpoints.EndpointPendingConfig"
          },
          "state" : {
            "description" : "Information corresponding to the state of the serving endpoint.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/endpoints.EndpointState"
          },
          "permission_level" : {
            "example" : "CAN_MANAGE",
            "description" : "The permission level of the principal making the request.",
            "type" : "string",
            "enum" : [ "CAN_MANAGE", "CAN_QUERY", "CAN_VIEW" ]
          },
          "config" : {
            "description" : "The config that is currently being served by the endpoint.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/endpoints.EndpointCoreConfigOutput"
          },
          "creator" : {
            "example" : "alice@company.com",
            "description" : "The email of the user who created the serving endpoint.",
            "type" : "string"
          },
          "creation_timestamp" : {
            "format" : "int64",
            "description" : "The timestamp when the endpoint was created in Unix time.",
            "type" : "integer"
          },
          "id" : {
            "example" : "88fd3f75a0d24b0380ddc40484d7a31b",
            "description" : "System-generated ID of the endpoint. This is used to refer to the endpoint in the Permissions API",
            "type" : "string"
          },
          "last_updated_timestamp" : {
            "format" : "int64",
            "description" : "The timestamp when the endpoint was last updated by a user in Unix time.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "endpoints.TrafficConfig" : {
        "properties" : {
          "routes" : {
            "description" : "The list of routes that define traffic to each served model.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/endpoints.Route"
            }
          }
        },
        "type" : "object"
      },
      "gitcredentials.CreateCredentials" : {
        "required" : [ "git_provider" ],
        "properties" : {
          "git_provider" : {
            "example" : "gitHub",
            "description" : "Git provider. This field is case-insensitive. The available Git providers are awsCodeCommit, azureDevOpsServices, ",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "git_username" : {
            "example" : "testuser",
            "description" : "Git username.",
            "type" : "string"
          },
          "personal_access_token" : {
            "example" : "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",
            "description" : "The personal access token used to authenticate to the corresponding Git provider.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "gitcredentials.CreateCredentialsResponse" : {
        "properties" : {
          "credential_id" : {
            "example" : 93488329053511,
            "format" : "int64",
            "description" : "ID of the credential object in the workspace.",
            "x-databricks-id" : true,
            "type" : "integer"
          },
          "git_provider" : {
            "example" : "gitHub",
            "description" : "Git provider. This field is case-insensitive. The available Git providers are awsCodeCommit, azureDevOpsServices, ",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "git_username" : {
            "example" : "testuser",
            "description" : "Git username.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "gitcredentials.CredentialInfo" : {
        "properties" : {
          "credential_id" : {
            "example" : 93488329053511,
            "format" : "int64",
            "description" : "ID of the credential object in the workspace.",
            "x-databricks-id" : true,
            "type" : "integer"
          },
          "git_provider" : {
            "example" : "gitHub",
            "description" : "Git provider. This field is case-insensitive. The available Git providers are awsCodeCommit, azureDevOpsServices, ",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "git_username" : {
            "example" : "testuser",
            "description" : "Git username.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "gitcredentials.GetCredentialsResponse" : {
        "properties" : {
          "credentials" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/gitcredentials.CredentialInfo"
            }
          }
        },
        "type" : "object"
      },
      "gitcredentials.UpdateCredentials" : {
        "properties" : {
          "git_provider" : {
            "example" : "gitHub",
            "description" : "Git provider. This field is case-insensitive. The available Git providers are awsCodeCommit, azureDevOpsServices, ",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "git_username" : {
            "example" : "testuser",
            "description" : "Git username.",
            "type" : "string"
          },
          "personal_access_token" : {
            "example" : "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",
            "description" : "The personal access token used to authenticate to the corresponding Git provider.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "globalinitscripts.GlobalInitScriptCreateRequest" : {
        "required" : [ "name", "script" ],
        "properties" : {
          "enabled" : {
            "default" : false,
            "example" : false,
            "description" : "Specifies whether the script is enabled. The script runs only if enabled.",
            "type" : "boolean"
          },
          "name" : {
            "example" : "My example script name",
            "description" : "The name of the script",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "position" : {
            "example" : 0,
            "description" : "The position of a global init script, where 0 represents the first script to run, 1 is the second script to run, in ascending order.\n\nIf you omit the numeric position for a new global init script, it defaults to last position. It will run after all current scripts.\nSetting any value greater than the position of the last script is equivalent to the last position. Example: Take three existing scripts with positions 0, 1, and 2. Any position of (3) or greater puts the script in the last position.\nIf an explicit position value conflicts with an existing script value, your request succeeds, but the original script at that position and all later scripts have their positions incremented by 1.",
            "type" : "integer"
          },
          "script" : {
            "example" : "ZWNobyBoZWxsbw==",
            "format" : "byte",
            "description" : "The Base64-encoded content of the script.",
            "x-databricks-base64" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "globalinitscripts.GlobalInitScriptDetails" : {
        "properties" : {
          "created_by" : {
            "example" : "john.doe@databricks.com",
            "format" : "email",
            "description" : "The username of the user who created the script.",
            "type" : "string"
          },
          "name" : {
            "example" : "My example script name",
            "description" : "The name of the script",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "updated_by" : {
            "example" : "jane.smith@example.com",
            "format" : "email",
            "description" : "The username of the user who last updated the script",
            "type" : "string"
          },
          "enabled" : {
            "example" : false,
            "description" : "Specifies whether the script is enabled. The script runs only if enabled.",
            "type" : "boolean"
          },
          "position" : {
            "example" : 0,
            "description" : "The position of a script, where 0 represents the first script to run, 1 is the second script to run, in ascending order.",
            "type" : "integer"
          },
          "created_at" : {
            "example" : 1594437249910,
            "description" : "Time when the script was created, represented as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "updated_at" : {
            "example" : 1594444684786,
            "description" : "Time when the script was updated, represented as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "script_id" : {
            "example" : "714B166709FBD56F",
            "description" : "The global init script ID.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "globalinitscripts.GlobalInitScriptDetailsWithContent" : {
        "properties" : {
          "created_by" : {
            "example" : "john.doe@databricks.com",
            "format" : "email",
            "description" : "The username of the user who created the script.",
            "type" : "string"
          },
          "name" : {
            "example" : "My example script name",
            "description" : "The name of the script",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "updated_by" : {
            "example" : "jane.smith@example.com",
            "format" : "email",
            "description" : "The username of the user who last updated the script",
            "type" : "string"
          },
          "enabled" : {
            "example" : false,
            "description" : "Specifies whether the script is enabled. The script runs only if enabled.",
            "type" : "boolean"
          },
          "position" : {
            "example" : 0,
            "description" : "The position of a script, where 0 represents the first script to run, 1 is the second script to run, in ascending order.",
            "type" : "integer"
          },
          "created_at" : {
            "example" : 1594437249910,
            "description" : "Time when the script was created, represented as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "script" : {
            "example" : "ZWNobyBoZWxsbw==",
            "format" : "byte",
            "description" : "The Base64-encoded content of the script.",
            "x-databricks-base64" : true,
            "type" : "string"
          },
          "updated_at" : {
            "example" : 1594444684786,
            "description" : "Time when the script was updated, represented as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "script_id" : {
            "example" : "714B166709FBD56F",
            "description" : "The global init script ID.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "globalinitscripts.GlobalInitScriptUpdateRequest" : {
        "required" : [ "name", "script" ],
        "properties" : {
          "enabled" : {
            "example" : false,
            "description" : "Specifies whether the script is enabled. The script runs only if enabled.",
            "type" : "boolean"
          },
          "name" : {
            "example" : "My example script name",
            "description" : "The name of the script",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "position" : {
            "example" : 0,
            "description" : "The position of a script, where 0 represents the first script to run,\n1 is the second script to run, in ascending order.\nTo move the script to run first, set its position to 0.\n\nTo move the script to the end, set its position to any value\ngreater or equal to the position of the last script.\nExample, three existing scripts with positions 0, 1, and 2.\nAny position value of 2 or greater puts the script in the last position (2).\n\nIf an explicit position value conflicts with an existing script, your request succeeds,\nbut the original script at that position and all later scripts have their positions\nincremented by 1.\n",
            "type" : "integer"
          },
          "script" : {
            "example" : "ZWNobyBoZWxsbw==",
            "format" : "byte",
            "description" : "The Base64-encoded content of the script.",
            "x-databricks-base64" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "globalinitscripts.ListGlobalInitScriptsResponse" : {
        "properties" : {
          "scripts" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/globalinitscripts.GlobalInitScriptDetails"
            }
          }
        },
        "type" : "object"
      },
      "instancepools.CreateInstancePool" : {
        "required" : [ "instance_pool_name", "node_type_id" ],
        "properties" : {
          "max_capacity" : {
            "format" : "int32",
            "description" : "Maximum number of outstanding instances to keep in the pool, including both instances used by\nclusters and idle instances. Clusters that require further instance provisioning will fail during\nupsize requests.",
            "type" : "integer"
          },
          "node_type_id" : {
            "description" : "This field encodes, through a single value, the resources available to each of\nthe Spark nodes in this cluster. For example, the Spark nodes can be provisioned\nand optimized for memory or compute intensive workloads. A list of available node\ntypes can be retrieved by using the :method:clusters/listNodeTypes API call.\n",
            "type" : "string"
          },
          "idle_instance_autotermination_minutes" : {
            "format" : "int32",
            "description" : "Automatically terminates the extra instances in the pool cache after they are inactive for this\ntime in minutes if min_idle_instances requirement is already met. If not set, the extra pool\ninstances will be automatically terminated after a default timeout. If specified, the\nthreshold must be between 0 and 10000 minutes.\nUsers can also set this value to 0 to instantly remove idle instances from the cache if\nmin cache size could still hold.",
            "type" : "integer"
          },
          "custom_tags" : {
            "properties" : { },
            "description" : "Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS\ninstances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n- Currently, Databricks allows at most 45 custom tags",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length must be less than or equal to 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          },
          "instance_pool_fleet_attributes" : {
            "description" : "The fleet related setting to power the instance pool.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.InstancePoolFleetAttributes"
          },
          "min_idle_instances" : {
            "format" : "int32",
            "description" : "Minimum number of idle instances to keep in the instance pool",
            "type" : "integer"
          },
          "preloaded_spark_versions" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.preloaded_spark_versions"
          },
          "azure_attributes" : {
            "description" : "Attributes related to pool running on Azure.\nIf not specified at pool creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/instancepools.InstancePoolAzureAttributes"
          },
          "preloaded_docker_images" : {
            "description" : "Custom Docker Image BYOC",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/instancepools.DockerImage"
            }
          },
          "aws_attributes" : {
            "description" : "Attributes related to pool running on Amazon Web Services.\nIf not specified at pool creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/instancepools.InstancePoolAwsAttributes"
          },
          "enable_elastic_disk" : {
            "description" : "Autoscaling Local Storage: when enabled, this instances in this pool will dynamically acquire\nadditional disk space when its Spark workers are running low on disk space. In AWS, this\nfeature requires specific AWS permissions to function correctly - refer to the User Guide for\nmore details.",
            "type" : "boolean"
          },
          "instance_pool_name" : {
            "description" : "Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100\ncharacters.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "disk_spec" : {
            "description" : "Defines the specification of the disks that will be attached to all spark containers.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.DiskSpec"
          }
        },
        "type" : "object"
      },
      "instancepools.CreateInstancePoolResponse" : {
        "properties" : {
          "instance_pool_id" : {
            "description" : "The ID of the created instance pool.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "instancepools.DeleteInstancePool" : {
        "required" : [ "instance_pool_id" ],
        "properties" : {
          "instance_pool_id" : {
            "description" : "The instance pool to be terminated.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "instancepools.DeleteInstancePoolResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "instancepools.DiskSpec" : {
        "properties" : {
          "disk_size" : {
            "format" : "int32",
            "description" : "The size of each disk (in GiB) launched for each instance.\nValues must fall into the supported range for a particular instance type.\n\nFor AWS:\n- General Purpose SSD: 100 - 4096 GiB\n- Throughput Optimized HDD: 500 - 4096 GiB\n\nFor Azure:\n- Premium LRS (SSD): 1 - 1023 GiB\n- Standard LRS (HDD): 1- 1023 GiB",
            "type" : "integer"
          },
          "disk_type" : {
            "description" : "The type of disks that will be launched with this cluster.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.DiskType"
          },
          "disk_count" : {
            "default" : "0",
            "format" : "int32",
            "description" : "The number of disks launched for each instance:\n- This feature is only enabled for supported node types.\n- Users can choose up to the limit of the disks supported by the node type.\n- For node types with no OS disk, at least one disk must be specified;\n  otherwise, cluster creation will fail.\n\nIf disks are attached, Databricks will configure Spark to use only the disks for\nscratch storage, because heterogenously sized scratch devices can lead to inefficient disk\nutilization. If no disks are attached, Databricks will configure Spark to use\ninstance store disks.\n\nNote: If disks are specified, then the Spark configuration\n`spark.local.dir` will be overridden.\n\nDisks will be mounted at:\n- For AWS: `/ebs0`, `/ebs1`, and etc.\n- For Azure: `/remote_volume0`, `/remote_volume1`, and etc.",
            "type" : "integer"
          },
          "disk_iops" : {
            "format" : "int32",
            "description" : "",
            "type" : "integer"
          },
          "disk_throughput" : {
            "format" : "int32",
            "description" : "",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "instancepools.DiskType" : {
        "properties" : {
          "azure_disk_volume_type" : {
            "description" : "",
            "type" : "string",
            "enum" : [ "PREMIUM_LRS", "STANDARD_LRS" ]
          },
          "ebs_volume_type" : {
            "description" : "",
            "type" : "string",
            "enum" : [ "GENERAL_PURPOSE_SSD", "THROUGHPUT_OPTIMIZED_HDD" ]
          }
        },
        "type" : "object"
      },
      "instancepools.DockerBasicAuth" : {
        "properties" : {
          "password" : {
            "description" : "",
            "type" : "string"
          },
          "username" : {
            "description" : "",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "instancepools.DockerImage" : {
        "properties" : {
          "basic_auth" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.DockerBasicAuth"
          },
          "url" : {
            "description" : "URL of the docker image.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "instancepools.EditInstancePool" : {
        "required" : [ "instance_pool_id", "instance_pool_name", "node_type_id" ],
        "properties" : {
          "max_capacity" : {
            "format" : "int32",
            "description" : "Maximum number of outstanding instances to keep in the pool, including both instances used by\nclusters and idle instances. Clusters that require further instance provisioning will fail during\nupsize requests.",
            "type" : "integer"
          },
          "node_type_id" : {
            "description" : "This field encodes, through a single value, the resources available to each of\nthe Spark nodes in this cluster. For example, the Spark nodes can be provisioned\nand optimized for memory or compute intensive workloads. A list of available node\ntypes can be retrieved by using the :method:clusters/listNodeTypes API call.\n",
            "type" : "string"
          },
          "idle_instance_autotermination_minutes" : {
            "format" : "int32",
            "description" : "Automatically terminates the extra instances in the pool cache after they are inactive for this\ntime in minutes if min_idle_instances requirement is already met. If not set, the extra pool\ninstances will be automatically terminated after a default timeout. If specified, the\nthreshold must be between 0 and 10000 minutes.\nUsers can also set this value to 0 to instantly remove idle instances from the cache if\nmin cache size could still hold.",
            "type" : "integer"
          },
          "custom_tags" : {
            "properties" : { },
            "description" : "Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS\ninstances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n- Currently, Databricks allows at most 45 custom tags",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length must be less than or equal to 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          },
          "instance_pool_fleet_attributes" : {
            "description" : "The fleet related setting to power the instance pool.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.InstancePoolFleetAttributes"
          },
          "min_idle_instances" : {
            "format" : "int32",
            "description" : "Minimum number of idle instances to keep in the instance pool",
            "type" : "integer"
          },
          "instance_pool_id" : {
            "description" : "Instance pool ID",
            "type" : "string"
          },
          "preloaded_spark_versions" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.preloaded_spark_versions"
          },
          "azure_attributes" : {
            "description" : "Attributes related to pool running on Azure.\nIf not specified at pool creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/instancepools.InstancePoolAzureAttributes"
          },
          "preloaded_docker_images" : {
            "description" : "Custom Docker Image BYOC",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/instancepools.DockerImage"
            }
          },
          "aws_attributes" : {
            "description" : "Attributes related to pool running on Amazon Web Services.\nIf not specified at pool creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/instancepools.InstancePoolAwsAttributes"
          },
          "enable_elastic_disk" : {
            "description" : "Autoscaling Local Storage: when enabled, this instances in this pool will dynamically acquire\nadditional disk space when its Spark workers are running low on disk space. In AWS, this\nfeature requires specific AWS permissions to function correctly - refer to the User Guide for\nmore details.",
            "type" : "boolean"
          },
          "instance_pool_name" : {
            "description" : "Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100\ncharacters.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "disk_spec" : {
            "description" : "Defines the specification of the disks that will be attached to all spark containers.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.DiskSpec"
          }
        },
        "type" : "object"
      },
      "instancepools.EditInstancePoolResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "instancepools.FleetLaunchTemplateOverride" : {
        "required" : [ "availability_zone", "instance_type" ],
        "properties" : {
          "availability_zone" : {
            "description" : "User-assigned preferred availability zone. It will adjust to the default zone of the worker\nenvironment if the preferred zone does not exist in the subnet.",
            "type" : "string"
          },
          "instance_type" : {
            "description" : "",
            "type" : "string"
          },
          "max_price" : {
            "format" : "double",
            "description" : "The maximum price per unit hour that you are willing to pay for a Spot Instance.",
            "type" : "number"
          },
          "priority" : {
            "format" : "double",
            "description" : "The priority for the launch template override. If AllocationStrategy is set to prioritized,\nEC2 Fleet uses priority to determine which launch template override or to use first in fulfilling\nOn-Demand capacity. The highest priority is launched first. Valid values are whole numbers\nstarting at 0. The lower the number, the higher the priority. If no number is set, the\nlaunch template override has the lowest priority.",
            "type" : "number"
          }
        },
        "type" : "object"
      },
      "instancepools.FleetOnDemandOption" : {
        "properties" : {
          "allocation_strategy" : {
            "default" : "LOWEST_PRICE",
            "description" : "Only lowest-price and prioritized are allowed",
            "type" : "string",
            "enum" : [ "LOWEST_PRICE", "DIVERSIFIED", "CAPACITY_OPTIMIZED", "PRIORITIZED" ]
          },
          "max_total_price" : {
            "format" : "double",
            "description" : "The maximum amount per hour for On-Demand Instances that you're willing to pay.",
            "type" : "number"
          },
          "use_capacity_reservations_first" : {
            "default" : "false",
            "description" : "If you specify use-capacity-reservations-first, the fleet uses unused Capacity Reservations\nto fulfill On-Demand capacity up to the target On-Demand capacity. If multiple instance pools\nhave unused Capacity Reservations, the On-Demand allocation strategy\n(lowest-price or prioritized) is applied. If the number of unused Capacity Reservations is\nless than the On-Demand target capacity, the remaining On-Demand target capacity is launched\naccording to the On-Demand allocation strategy (lowest-price or prioritized).",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "instancepools.FleetSpotOption" : {
        "properties" : {
          "allocation_strategy" : {
            "default" : "LOWEST_PRICE",
            "description" : "lowest-price | diversified | capacity-optimized",
            "type" : "string",
            "enum" : [ "LOWEST_PRICE", "DIVERSIFIED", "CAPACITY_OPTIMIZED", "PRIORITIZED" ]
          },
          "instance_pools_to_use_count" : {
            "format" : "int32",
            "description" : "The number of Spot pools across which to allocate your target Spot capacity.\nValid only when Spot Allocation Strategy is set to lowest-price. EC2 Fleet selects the cheapest\nSpot pools and evenly allocates your target Spot capacity across the number of Spot pools that\nyou specify.",
            "type" : "integer"
          },
          "max_total_price" : {
            "format" : "double",
            "description" : "The maximum amount per hour for Spot Instances that you're willing to pay.",
            "type" : "number"
          }
        },
        "type" : "object"
      },
      "instancepools.GetInstancePool" : {
        "required" : [ "instance_pool_id" ],
        "properties" : {
          "stats" : {
            "description" : "Usage statistics about the instance pool.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.InstancePoolStats"
          },
          "state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.InstancePoolState"
          },
          "max_capacity" : {
            "format" : "int32",
            "description" : "Maximum number of outstanding instances to keep in the pool, including both instances used by\nclusters and idle instances. Clusters that require further instance provisioning will fail during\nupsize requests.",
            "type" : "integer"
          },
          "node_type_id" : {
            "description" : "This field encodes, through a single value, the resources available to each of\nthe Spark nodes in this cluster. For example, the Spark nodes can be provisioned\nand optimized for memory or compute intensive workloads. A list of available node\ntypes can be retrieved by using the :method:clusters/listNodeTypes API call.\n",
            "type" : "string"
          },
          "idle_instance_autotermination_minutes" : {
            "format" : "int32",
            "description" : "Automatically terminates the extra instances in the pool cache after they are inactive for this\ntime in minutes if min_idle_instances requirement is already met. If not set, the extra pool\ninstances will be automatically terminated after a default timeout. If specified, the\nthreshold must be between 0 and 10000 minutes.\nUsers can also set this value to 0 to instantly remove idle instances from the cache if\nmin cache size could still hold.",
            "type" : "integer"
          },
          "custom_tags" : {
            "properties" : { },
            "description" : "Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS\ninstances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n- Currently, Databricks allows at most 45 custom tags",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length must be less than or equal to 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          },
          "instance_pool_fleet_attributes" : {
            "description" : "The fleet related setting to power the instance pool.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.InstancePoolFleetAttributes"
          },
          "status" : {
            "description" : "Status of failed pending instances in the pool.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.InstancePoolStatus"
          },
          "min_idle_instances" : {
            "format" : "int32",
            "description" : "Minimum number of idle instances to keep in the instance pool",
            "type" : "integer"
          },
          "instance_pool_id" : {
            "description" : "Canonical unique identifier for the pool.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "preloaded_spark_versions" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.preloaded_spark_versions"
          },
          "azure_attributes" : {
            "description" : "Attributes related to pool running on Azure.\nIf not specified at pool creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/instancepools.InstancePoolAzureAttributes"
          },
          "preloaded_docker_images" : {
            "description" : "Custom Docker Image BYOC",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/instancepools.DockerImage"
            }
          },
          "aws_attributes" : {
            "description" : "Attributes related to pool running on Amazon Web Services.\nIf not specified at pool creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/instancepools.InstancePoolAwsAttributes"
          },
          "enable_elastic_disk" : {
            "description" : "Autoscaling Local Storage: when enabled, this instances in this pool will dynamically acquire\nadditional disk space when its Spark workers are running low on disk space. In AWS, this\nfeature requires specific AWS permissions to function correctly - refer to the User Guide for\nmore details.",
            "type" : "boolean"
          },
          "instance_pool_name" : {
            "description" : "Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100\ncharacters.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "default_tags" : {
            "properties" : { },
            "description" : "Tags that are added by Databricks regardless of any `custom_tags`, including:\n\n  - Vendor: Databricks\n\n  - InstancePoolCreator: <user_id_of_creator>\n\n  - InstancePoolName: <name_of_pool>\n\n  - InstancePoolId: <id_of_pool>",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length cannot exceed 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          },
          "disk_spec" : {
            "description" : "Defines the specification of the disks that will be attached to all spark containers.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.DiskSpec"
          }
        },
        "type" : "object"
      },
      "instancepools.InstancePoolAndStats" : {
        "properties" : {
          "stats" : {
            "description" : "Usage statistics about the instance pool.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.InstancePoolStats"
          },
          "state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.InstancePoolState"
          },
          "max_capacity" : {
            "format" : "int32",
            "description" : "Maximum number of outstanding instances to keep in the pool, including both instances used by\nclusters and idle instances. Clusters that require further instance provisioning will fail during\nupsize requests.",
            "type" : "integer"
          },
          "node_type_id" : {
            "description" : "This field encodes, through a single value, the resources available to each of\nthe Spark nodes in this cluster. For example, the Spark nodes can be provisioned\nand optimized for memory or compute intensive workloads. A list of available node\ntypes can be retrieved by using the :method:clusters/listNodeTypes API call.\n",
            "type" : "string"
          },
          "idle_instance_autotermination_minutes" : {
            "format" : "int32",
            "description" : "Automatically terminates the extra instances in the pool cache after they are inactive for this\ntime in minutes if min_idle_instances requirement is already met. If not set, the extra pool\ninstances will be automatically terminated after a default timeout. If specified, the\nthreshold must be between 0 and 10000 minutes.\nUsers can also set this value to 0 to instantly remove idle instances from the cache if\nmin cache size could still hold.",
            "type" : "integer"
          },
          "custom_tags" : {
            "properties" : { },
            "description" : "Additional tags for pool resources. Databricks will tag all pool resources (e.g., AWS\ninstances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n- Currently, Databricks allows at most 45 custom tags",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length must be less than or equal to 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          },
          "instance_pool_fleet_attributes" : {
            "description" : "The fleet related setting to power the instance pool.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.InstancePoolFleetAttributes"
          },
          "status" : {
            "description" : "Status of failed pending instances in the pool.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.InstancePoolStatus"
          },
          "min_idle_instances" : {
            "format" : "int32",
            "description" : "Minimum number of idle instances to keep in the instance pool",
            "type" : "integer"
          },
          "instance_pool_id" : {
            "description" : "Canonical unique identifier for the pool.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "preloaded_spark_versions" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.preloaded_spark_versions"
          },
          "azure_attributes" : {
            "description" : "Attributes related to pool running on Azure.\nIf not specified at pool creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/instancepools.InstancePoolAzureAttributes"
          },
          "preloaded_docker_images" : {
            "description" : "Custom Docker Image BYOC",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/instancepools.DockerImage"
            }
          },
          "aws_attributes" : {
            "description" : "Attributes related to pool running on Amazon Web Services.\nIf not specified at pool creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/instancepools.InstancePoolAwsAttributes"
          },
          "enable_elastic_disk" : {
            "description" : "Autoscaling Local Storage: when enabled, this instances in this pool will dynamically acquire\nadditional disk space when its Spark workers are running low on disk space. In AWS, this\nfeature requires specific AWS permissions to function correctly - refer to the User Guide for\nmore details.",
            "type" : "boolean"
          },
          "instance_pool_name" : {
            "description" : "Pool name requested by the user. Pool name must be unique. Length must be between 1 and 100\ncharacters.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "default_tags" : {
            "properties" : { },
            "description" : "Tags that are added by Databricks regardless of any `custom_tags`, including:\n\n  - Vendor: Databricks\n\n  - InstancePoolCreator: <user_id_of_creator>\n\n  - InstancePoolName: <name_of_pool>\n\n  - InstancePoolId: <id_of_pool>",
            "type" : "object",
            "additionalProperties" : {
              "description" : "The value of the tag. The value length cannot exceed 255 UTF-8 characters.\nFor a list of all restrictions, see the AWS docs here:\nhttp://docs.aws.amazon.com/AWSEC2/latest/UserGuide/Using_Tags.html#tag-restrictions",
              "type" : "string"
            }
          },
          "disk_spec" : {
            "description" : "Defines the specification of the disks that will be attached to all spark containers.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.DiskSpec"
          }
        },
        "type" : "object"
      },
      "instancepools.InstancePoolAwsAttributes" : {
        "properties" : {
          "availability" : {
            "description" : "Availability type used for the spot nodes.\n\nThe default value is defined by InstancePoolConf.instancePoolDefaultAwsAvailability",
            "type" : "string",
            "enum" : [ "SPOT", "ON_DEMAND", "SPOT_WITH_FALLBACK" ]
          },
          "spot_bid_price_percent" : {
            "default" : "100",
            "format" : "int32",
            "description" : "Calculates the bid price for AWS spot instances, as a percentage of the corresponding instance type's\non-demand price.\nFor example, if this field is set to 50, and the cluster needs a new `r3.xlarge` spot\ninstance, then the bid price is half of the price of\non-demand `r3.xlarge` instances. Similarly, if this field is set to 200, the bid price is twice\nthe price of on-demand `r3.xlarge` instances. If not specified, the default value is 100.\nWhen spot instances are requested for this cluster, only spot instances whose bid price\npercentage matches this field will be considered.\nNote that, for safety, we enforce this field to be no more than 10000.\n\nThe default value and documentation here should be kept consistent with\nCommonConf.defaultSpotBidPricePercent and CommonConf.maxSpotBidPricePercent.",
            "type" : "integer"
          },
          "zone_id" : {
            "description" : "Identifier for the availability zone/datacenter in which the cluster resides.\nThis string will be of a form like \"us-west-2a\". The provided availability\nzone must be in the same region as the Databricks deployment. For example, \"us-west-2a\"\nis not a valid zone id if the Databricks deployment resides in the \"us-east-1\" region.\nThis is an optional field at cluster creation, and if not specified, a default zone will be used.\nThe list of available zones as well as the default value can be found by using the\n`List Zones`_ method.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "instancepools.InstancePoolAzureAttributes" : {
        "properties" : {
          "availability" : {
            "default" : "ON_DEMAND_AZURE",
            "description" : "Shows the Availability type used for the spot nodes.\n\nThe default value is defined by InstancePoolConf.instancePoolDefaultAzureAvailability",
            "type" : "string",
            "enum" : [ "SPOT_AZURE", "ON_DEMAND_AZURE", "SPOT_WITH_FALLBACK_AZURE" ]
          },
          "spot_bid_max_price" : {
            "default" : "-1.0",
            "format" : "double",
            "description" : "The default value and documentation here should be kept consistent with\nCommonConf.defaultSpotBidMaxPrice.",
            "type" : "number"
          }
        },
        "type" : "object"
      },
      "instancepools.InstancePoolFleetAttributes" : {
        "properties" : {
          "fleet_on_demand_option" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.FleetOnDemandOption"
          },
          "fleet_spot_option" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/instancepools.FleetSpotOption"
          },
          "launch_template_overrides" : {
            "description" : "",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/instancepools.FleetLaunchTemplateOverride"
            }
          }
        },
        "type" : "object"
      },
      "instancepools.InstancePoolState" : {
        "description" : "Current state of the instance pool.",
        "type" : "string",
        "enum" : [ "ACTIVE", "STOPPED", "DELETED" ]
      },
      "instancepools.InstancePoolStats" : {
        "properties" : {
          "idle_count" : {
            "format" : "int32",
            "description" : "Number of active instances in the pool that are NOT part of a cluster.",
            "type" : "integer"
          },
          "pending_idle_count" : {
            "format" : "int32",
            "description" : "Number of pending instances in the pool that are NOT part of a cluster.",
            "type" : "integer"
          },
          "pending_used_count" : {
            "format" : "int32",
            "description" : "Number of pending instances in the pool that are part of a cluster.",
            "type" : "integer"
          },
          "used_count" : {
            "format" : "int32",
            "description" : "Number of active instances in the pool that are part of a cluster.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "instancepools.InstancePoolStatus" : {
        "properties" : {
          "pending_instance_errors" : {
            "description" : "List of error messages for the failed pending instances.\nThe pending_instance_errors follows FIFO with maximum length of the min_idle of the pool.\nThe pending_instance_errors is emptied once the number of exiting available instances reaches\nthe min_idle of the pool.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/instancepools.PendingInstanceError"
            }
          }
        },
        "type" : "object"
      },
      "instancepools.ListInstancePools" : {
        "properties" : {
          "instance_pools" : {
            "description" : "",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/instancepools.InstancePoolAndStats"
            }
          }
        },
        "type" : "object"
      },
      "instancepools.PendingInstanceError" : {
        "properties" : {
          "instance_id" : {
            "description" : "",
            "type" : "string"
          },
          "message" : {
            "description" : "",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "instancepools.preloaded_spark_versions" : {
        "description" : "A list of preloaded Spark image versions for the pool. Pool-backed clusters started\nwith the preloaded Spark version will start faster. A list of available Spark versions\n can be retrieved by using the :method:clusters/sparkVersions API call.\n",
        "type" : "array",
        "items" : {
          "type" : "string"
        }
      },
      "ipaccesslists.CreateIPAccessList" : {
        "required" : [ "label", "list_type", "ip_addresses" ],
        "properties" : {
          "ip_addresses" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/ipaccesslists.ip_addresses"
          },
          "label" : {
            "example" : "Office VPN",
            "description" : "Label for the IP access list. This **cannot** be empty.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "list_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/ipaccesslists.list_type"
          }
        },
        "type" : "object"
      },
      "ipaccesslists.CreateIPAccessListResponse" : {
        "properties" : {
          "ip_access_list" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/ipaccesslists.IpAccessListInfo"
          }
        },
        "type" : "object"
      },
      "ipaccesslists.FetchIpAccessListResponse" : {
        "properties" : {
          "ip_access_list" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/ipaccesslists.IpAccessListInfo"
          }
        },
        "type" : "object"
      },
      "ipaccesslists.GetIPAccessListResponse" : {
        "properties" : {
          "ip_access_lists" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/ipaccesslists.IpAccessListInfo"
            }
          }
        },
        "type" : "object"
      },
      "ipaccesslists.IpAccessListInfo" : {
        "properties" : {
          "created_by" : {
            "example" : 202480738464078,
            "format" : "int64",
            "description" : "User ID of the user who created this list.",
            "type" : "integer"
          },
          "updated_by" : {
            "example" : 202480738464089,
            "format" : "int64",
            "description" : "User ID of the user who updated this list.",
            "type" : "integer"
          },
          "address_count" : {
            "example" : 2,
            "description" : "Total number of IP or CIDR values.",
            "type" : "integer"
          },
          "enabled" : {
            "description" : "Specifies whether this IP access list is enabled.",
            "type" : "boolean"
          },
          "label" : {
            "example" : "Office VPN",
            "description" : "Label for the IP access list. This **cannot** be empty.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "list_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/ipaccesslists.list_type"
          },
          "ip_addresses" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/ipaccesslists.ip_addresses"
          },
          "created_at" : {
            "example" : 1580265020299,
            "format" : "int64",
            "description" : "Creation timestamp in milliseconds.",
            "type" : "integer"
          },
          "updated_at" : {
            "example" : 1580265020299,
            "format" : "int64",
            "description" : "Update timestamp in milliseconds.",
            "type" : "integer"
          },
          "list_id" : {
            "format" : "uuid",
            "description" : "Universally unique identifier(UUID) of the IP access list.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "ipaccesslists.ReplaceIPAccessList" : {
        "required" : [ "label", "list_type", "ip_addresses", "enabled" ],
        "properties" : {
          "enabled" : {
            "description" : "Specifies whether this IP access list is enabled.",
            "type" : "boolean"
          },
          "label" : {
            "example" : "Office VPN",
            "description" : "Label for the IP access list. This **cannot** be empty.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "list_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/ipaccesslists.list_type"
          },
          "ip_addresses" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/ipaccesslists.ip_addresses"
          },
          "list_id" : {
            "format" : "uuid",
            "description" : "Universally unique identifier(UUID) of the IP access list.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "ipaccesslists.UpdateIPAccessList" : {
        "required" : [ "label", "list_type", "ip_addresses", "enabled" ],
        "properties" : {
          "enabled" : {
            "description" : "Specifies whether this IP access list is enabled.",
            "type" : "boolean"
          },
          "label" : {
            "example" : "Office VPN",
            "description" : "Label for the IP access list. This **cannot** be empty.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "list_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/ipaccesslists.list_type"
          },
          "ip_addresses" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/ipaccesslists.ip_addresses"
          },
          "list_id" : {
            "format" : "uuid",
            "description" : "Universally unique identifier(UUID) of the IP access list.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "ipaccesslists.ip_addresses" : {
        "example" : [ "32.19.112.0", "192.168.100.0/22" ],
        "description" : "Array of IP addresses or CIDR values to be added to the IP access list.",
        "type" : "array",
        "items" : {
          "example" : "192.168.100.0/22",
          "description" : "IP addresses or CIDR values to be added to the IP access list.",
          "type" : "string"
        }
      },
      "ipaccesslists.list_type" : {
        "example" : "ALLOW",
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "ALLOW", "BLOCK" ],
        "x-databricks-enum-descriptions" : {
          "ALLOW" : "An allow list. Include this IP or range.",
          "BLOCK" : "A block list. Exclude this IP or range. IP addresses in the block list are excluded even if they are included in an allow list."
        }
      },
      "jobs.BaseJob" : {
        "properties" : {
          "created_time" : {
            "example" : 1601370337343,
            "format" : "int64",
            "description" : "The time at which this job was created in epoch milliseconds (milliseconds since 1/1/1970 UTC).",
            "type" : "integer"
          },
          "creator_user_name" : {
            "example" : "user.name@databricks.com",
            "description" : "The creator user name. This field wont be included in the response if the user has already been deleted.",
            "type" : "string"
          },
          "job_id" : {
            "example" : 11223344,
            "format" : "int64",
            "description" : "The canonical identifier for this job.",
            "x-databricks-id" : true,
            "type" : "integer"
          },
          "settings" : {
            "description" : "Settings for this job and all of its runs. These settings can be updated using the `resetJob` method.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-name" : true,
            "$ref" : "#/components/schemas/jobs.JobSettings"
          }
        },
        "type" : "object"
      },
      "jobs.BaseRun" : {
        "properties" : {
          "continuous" : {
            "description" : "The continuous trigger that triggered this run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.Continuous"
          },
          "setup_duration" : {
            "example" : 0,
            "format" : "int64",
            "description" : "The time in milliseconds it took to set up the cluster. For runs that run on new clusters this is the cluster creation time, for runs that run on existing clusters this time should be very short. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `setup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.",
            "type" : "integer"
          },
          "start_time" : {
            "example" : 1625060460483,
            "format" : "int64",
            "description" : "The time at which this run was started in epoch milliseconds (milliseconds since 1/1/1970 UTC). This may not be the time when the job task starts executing, for example, if the job is scheduled to run on a new cluster, this is the time the cluster creation call is issued.",
            "type" : "integer"
          },
          "job_clusters" : {
            "example" : [ {
              "job_cluster_key" : "auto_scaling_cluster",
              "new_cluster" : {
                "autoscale" : {
                  "max_workers" : 16,
                  "min_workers" : 2
                },
                "node_type_id" : null,
                "spark_conf" : {
                  "spark.speculation" : true
                },
                "spark_version" : "7.3.x-scala2.12"
              }
            } ],
            "description" : "A list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.JobCluster"
            }
          },
          "state" : {
            "description" : "The result and lifecycle states of the run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.RunState"
          },
          "original_attempt_run_id" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "If this run is a retry of a prior run attempt, this field contains the run_id of the original attempt; otherwise, it is the same as the run_id.",
            "type" : "integer"
          },
          "overriding_parameters" : {
            "description" : "The parameters used for this run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.RunParameters"
          },
          "end_time" : {
            "example" : 1625060863413,
            "format" : "int64",
            "description" : "The time at which this run ended in epoch milliseconds (milliseconds since 1/1/1970 UTC). This field is set to 0 if the job is still running.",
            "type" : "integer"
          },
          "job_id" : {
            "example" : 11223344,
            "format" : "int64",
            "description" : "The canonical identifier of the job that contains this run.",
            "type" : "integer"
          },
          "run_page_url" : {
            "example" : "https://my-workspace.cloud.databricks.com/#job/11223344/run/123",
            "description" : "The URL to the detail page of the run.",
            "type" : "string"
          },
          "number_in_job" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "A unique identifier for this job run. This is set to the same value as `run_id`.",
            "deprecated" : true,
            "type" : "integer"
          },
          "run_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.RunType"
          },
          "run_id" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "The canonical identifier of the run. This ID is unique across all runs of all jobs.",
            "x-databricks-id" : true,
            "type" : "integer"
          },
          "cluster_instance" : {
            "description" : "The cluster used for this run. If the run is specified to use a new cluster, this field is set once the Jobs service has requested a cluster for the run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.ClusterInstance"
          },
          "tasks" : {
            "example" : [ {
              "setup_duration" : 0,
              "start_time" : 1629989929660,
              "task_key" : "Orders_Ingest",
              "state" : {
                "life_cycle_state" : "INTERNAL_ERROR",
                "result_state" : "FAILED",
                "state_message" : "Library installation failed for library due to user error. Error messages:\n'Manage' permissions are required to install libraries on a cluster",
                "user_cancelled_or_timedout" : false
              },
              "description" : "Ingests order data",
              "job_cluster_key" : "auto_scaling_cluster",
              "end_time" : 1629989930171,
              "run_page_url" : "https://my-workspace.cloud.databricks.com/#job/39832/run/20",
              "run_id" : 2112892,
              "cluster_instance" : {
                "cluster_id" : "0923-164208-meows279",
                "spark_context_id" : "4348585301701786933"
              },
              "spark_jar_task" : {
                "main_class_name" : "com.databricks.OrdersIngest"
              },
              "libraries" : [ {
                "jar" : "dbfs:/mnt/databricks/OrderIngest.jar"
              } ],
              "attempt_number" : 0,
              "cleanup_duration" : 0,
              "execution_duration" : 0
            }, {
              "setup_duration" : 0,
              "start_time" : 0,
              "task_key" : "Match",
              "state" : {
                "life_cycle_state" : "SKIPPED",
                "state_message" : "An upstream task failed.",
                "user_cancelled_or_timedout" : false
              },
              "description" : "Matches orders with user sessions",
              "notebook_task" : {
                "notebook_path" : "/Users/user.name@databricks.com/Match",
                "source" : "WORKSPACE"
              },
              "end_time" : 1629989930238,
              "depends_on" : [ {
                "task_key" : "Orders_Ingest"
              }, {
                "task_key" : "Sessionize"
              } ],
              "run_page_url" : "https://my-workspace.cloud.databricks.com/#job/39832/run/21",
              "new_cluster" : {
                "autoscale" : {
                  "max_workers" : 16,
                  "min_workers" : 2
                },
                "node_type_id" : null,
                "spark_conf" : {
                  "spark.speculation" : true
                },
                "spark_version" : "7.3.x-scala2.12"
              },
              "run_id" : 2112897,
              "cluster_instance" : {
                "cluster_id" : "0923-164208-meows279"
              },
              "attempt_number" : 0,
              "cleanup_duration" : 0,
              "execution_duration" : 0
            }, {
              "setup_duration" : 0,
              "start_time" : 1629989929668,
              "task_key" : "Sessionize",
              "state" : {
                "life_cycle_state" : "INTERNAL_ERROR",
                "result_state" : "FAILED",
                "state_message" : "Library installation failed for library due to user error. Error messages:\n'Manage' permissions are required to install libraries on a cluster",
                "user_cancelled_or_timedout" : false
              },
              "description" : "Extracts session data from events",
              "end_time" : 1629989930144,
              "run_page_url" : "https://my-workspace.cloud.databricks.com/#job/39832/run/22",
              "run_id" : 2112902,
              "cluster_instance" : {
                "cluster_id" : "0923-164208-meows279",
                "spark_context_id" : "4348585301701786933"
              },
              "spark_jar_task" : {
                "main_class_name" : "com.databricks.Sessionize"
              },
              "libraries" : [ {
                "jar" : "dbfs:/mnt/databricks/Sessionize.jar"
              } ],
              "attempt_number" : 0,
              "existing_cluster_id" : "0923-164208-meows279",
              "cleanup_duration" : 0,
              "execution_duration" : 0
            } ],
            "description" : "The list of tasks performed by the run. Each task has its own `run_id` which you can use to call `JobsGetOutput` to retrieve the run resutls.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.RunTask"
            }
          },
          "run_name" : {
            "default" : "Untitled",
            "example" : "A multitask job run",
            "description" : "An optional name for the run. The maximum allowed length is 4096 bytes in UTF-8 encoding.",
            "type" : "string"
          },
          "creator_user_name" : {
            "example" : "user.name@databricks.com",
            "description" : "The creator user name. This field wont be included in the response if the user has already been deleted.",
            "type" : "string"
          },
          "schedule" : {
            "description" : "The cron schedule that triggered this run if it was triggered by the periodic scheduler.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.CronSchedule"
          },
          "attempt_number" : {
            "example" : 0,
            "format" : "int32",
            "description" : "The sequence number of this run attempt for a triggered job run. The initial attempt of a run has an attempt_number of 0\\. If the initial run attempt fails, and the job has a retry policy (`max_retries` \\> 0), subsequent runs are created with an `original_attempt_run_id` of the original attempts ID and an incrementing `attempt_number`. Runs are retried only until they succeed, and the maximum `attempt_number` is the same as the `max_retries` value for the job.",
            "type" : "integer"
          },
          "run_duration" : {
            "example" : 110183,
            "description" : "The time in milliseconds it took the job run and all of its repairs to finish.",
            "type" : "integer"
          },
          "git_source" : {
            "example" : {
              "git_branch" : "main",
              "git_provider" : "gitHub",
              "git_url" : "https://github.com/databricks/databricks-cli"
            },
            "description" : "An optional specification for a remote repository containing the notebooks used by this job's notebook tasks.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.GitSource"
          },
          "cleanup_duration" : {
            "example" : 0,
            "format" : "int64",
            "description" : "The time in milliseconds it took to terminate the cluster and clean up any associated artifacts. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `cleanup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.",
            "type" : "integer"
          },
          "execution_duration" : {
            "example" : 0,
            "format" : "int64",
            "description" : "The time in milliseconds it took to execute the commands in the JAR or notebook until they  completed, failed, timed out, were cancelled, or encountered an unexpected error. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the  `cleanup_duration`. The `execution_duration` field is set to 0 for multitask job runs. The total  duration of a multitask job run is the value of the `run_duration` field.",
            "type" : "integer"
          },
          "cluster_spec" : {
            "description" : "A snapshot of the jobs cluster specification when this run was created.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.ClusterSpec"
          },
          "trigger" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.TriggerType"
          }
        },
        "type" : "object"
      },
      "jobs.CancelAllRuns" : {
        "required" : [ "job_id" ],
        "properties" : {
          "job_id" : {
            "example" : 11223344,
            "format" : "int64",
            "description" : "The canonical identifier of the job to cancel all runs of. This field is required.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "jobs.CancelRun" : {
        "required" : [ "run_id" ],
        "properties" : {
          "run_id" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "This field is required.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "jobs.ClusterInstance" : {
        "properties" : {
          "cluster_id" : {
            "example" : "0923-164208-meows279",
            "description" : "The canonical identifier for the cluster used by a run. This field is always available for runs on existing clusters. For runs on new clusters, it becomes available once the cluster is created. This value can be used to view logs by browsing to `/#setting/sparkui/$cluster_id/driver-logs`. The logs continue to be available after the run completes.\n\nThe response wont include this field if the identifier is not available yet.",
            "type" : "string"
          },
          "spark_context_id" : {
            "description" : "The canonical identifier for the Spark context used by a run. This field is filled in once the run begins execution. This value can be used to view the Spark UI by browsing to `/#setting/sparkui/$cluster_id/$spark_context_id`. The Spark UI continues to be available after the run has completed.\n\nThe response wont include this field if the identifier is not available yet.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "jobs.ClusterSpec" : {
        "properties" : {
          "existing_cluster_id" : {
            "example" : "0923-164208-meows279",
            "description" : "If existing_cluster_id, the ID of an existing cluster that is used for all runs of\nthis job. When running jobs on an existing cluster, you may need to manually restart\nthe cluster if it stops responding. We suggest running jobs on new clusters for\ngreater reliability\n",
            "type" : "string"
          },
          "libraries" : {
            "description" : "An optional list of libraries to be installed on the cluster that executes the job.\nThe default value is an empty list.\n",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/libraries.Library"
            }
          },
          "new_cluster" : {
            "description" : "If new_cluster, a description of a cluster that is created for each run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.BaseClusterInfo"
          }
        },
        "type" : "object"
      },
      "jobs.Continuous" : {
        "properties" : {
          "pause_status" : {
            "example" : "PAUSED",
            "description" : "Indicate whether the continuous execution of the job is paused or not. Defaults to UNPAUSED.",
            "type" : "string",
            "enum" : [ "PAUSED", "UNPAUSED" ]
          }
        },
        "type" : "object"
      },
      "jobs.CreateJob" : {
        "properties" : {
          "format" : {
            "example" : "MULTI_TASK",
            "description" : "Used to tell what is the format of the job. This field is ignored in Create/Update/Reset calls. When using the Jobs API 2.1 this value is always set to `\"MULTI_TASK\"`.",
            "type" : "string",
            "enum" : [ "SINGLE_TASK", "MULTI_TASK" ]
          },
          "continuous" : {
            "description" : "An optional continuous property for this job. The continuous property will ensure that there is always one run executing. Only one of `schedule` and `continuous` can be used.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.Continuous"
          },
          "name" : {
            "default" : "Untitled",
            "example" : "A multitask job",
            "description" : "An optional name for the job.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "job_clusters" : {
            "example" : [ {
              "job_cluster_key" : "auto_scaling_cluster",
              "new_cluster" : {
                "autoscale" : {
                  "max_workers" : 16,
                  "min_workers" : 2
                },
                "node_type_id" : null,
                "spark_conf" : {
                  "spark.speculation" : true
                },
                "spark_version" : "7.3.x-scala2.12"
              }
            } ],
            "description" : "A list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.JobCluster"
            }
          },
          "email_notifications" : {
            "description" : "An optional set of email addresses that is notified when runs of this job begin or complete as well as when this job is deleted. The default behavior is to not send any emails.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.JobEmailNotifications"
          },
          "tags" : {
            "default" : "{}",
            "example" : {
              "cost-center" : "engineering",
              "team" : "jobs"
            },
            "properties" : { },
            "description" : "A map of tags associated with the job. These are forwarded to the cluster as cluster tags for jobs clusters, and are subject to the same limitations as cluster tags. A maximum of 25 tags can be added to the job.",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "timeout_seconds" : {
            "example" : 86400,
            "format" : "int32",
            "description" : "An optional timeout applied to each run of this job. The default behavior is to have no timeout.",
            "type" : "integer"
          },
          "tasks" : {
            "example" : [ {
              "max_retries" : 3,
              "task_key" : "Sessionize",
              "description" : "Extracts session data from events",
              "min_retry_interval_millis" : 2000,
              "depends_on" : [ ],
              "timeout_seconds" : 86400,
              "spark_jar_task" : {
                "main_class_name" : "com.databricks.Sessionize",
                "parameters" : [ "--data", "dbfs:/path/to/data.json" ]
              },
              "libraries" : [ {
                "jar" : "dbfs:/mnt/databricks/Sessionize.jar"
              } ],
              "retry_on_timeout" : false,
              "existing_cluster_id" : "0923-164208-meows279"
            }, {
              "max_retries" : 3,
              "task_key" : "Orders_Ingest",
              "description" : "Ingests order data",
              "job_cluster_key" : "auto_scaling_cluster",
              "min_retry_interval_millis" : 2000,
              "depends_on" : [ ],
              "timeout_seconds" : 86400,
              "spark_jar_task" : {
                "main_class_name" : "com.databricks.OrdersIngest",
                "parameters" : [ "--data", "dbfs:/path/to/order-data.json" ]
              },
              "libraries" : [ {
                "jar" : "dbfs:/mnt/databricks/OrderIngest.jar"
              } ],
              "retry_on_timeout" : false
            }, {
              "max_retries" : 3,
              "task_key" : "Match",
              "description" : "Matches orders with user sessions",
              "notebook_task" : {
                "base_parameters" : {
                  "age" : "35",
                  "name" : "John Doe"
                },
                "notebook_path" : "/Users/user.name@databricks.com/Match"
              },
              "min_retry_interval_millis" : 2000,
              "depends_on" : [ {
                "task_key" : "Orders_Ingest"
              }, {
                "task_key" : "Sessionize"
              } ],
              "new_cluster" : {
                "autoscale" : {
                  "max_workers" : 16,
                  "min_workers" : 2
                },
                "node_type_id" : null,
                "spark_conf" : {
                  "spark.speculation" : true
                },
                "spark_version" : "7.3.x-scala2.12"
              },
              "timeout_seconds" : 86400,
              "retry_on_timeout" : false
            } ],
            "description" : "A list of task specifications to be executed by this job.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.JobTaskSettings"
            }
          },
          "max_concurrent_runs" : {
            "example" : 10,
            "format" : "int32",
            "description" : "An optional maximum allowed number of concurrent runs of the job.\n\nSet this value if you want to be able to execute multiple runs of the same job concurrently. This is useful for example if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or if you want to trigger multiple runs which differ by their input parameters.\n\nThis setting affects only new runs. For example, suppose the jobs concurrency is 4 and there are 4 concurrent active runs. Then setting the concurrency to 3 wont kill any of the active runs. However, from then on, new runs are skipped unless there are fewer than 3 active runs.\n\nThis value cannot exceed 1000\\. Setting this value to 0 causes all new runs to be skipped. The default behavior is to allow only 1 concurrent run.",
            "type" : "integer"
          },
          "schedule" : {
            "description" : "An optional periodic schedule for this job. The default behavior is that the job only runs when triggered by clicking Run Now in the Jobs UI or sending an API request to `runNow`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.CronSchedule"
          },
          "webhook_notifications" : {
            "description" : "A collection of system notification IDs to notify when the run begins or completes. The default behavior is to not send any system notifications.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.JobWebhookNotifications"
          },
          "access_control_list" : {
            "description" : "List of permissions to set on the job.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.AccessControlRequest"
            }
          },
          "git_source" : {
            "example" : {
              "git_branch" : "main",
              "git_provider" : "gitHub",
              "git_url" : "https://github.com/databricks/databricks-cli"
            },
            "description" : "An optional specification for a remote repository containing the notebooks used by this job's notebook tasks.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.GitSource"
          }
        },
        "type" : "object"
      },
      "jobs.CronSchedule" : {
        "required" : [ "quartz_cron_expression", "timezone_id" ],
        "properties" : {
          "pause_status" : {
            "example" : "PAUSED",
            "description" : "Indicate whether this schedule is paused or not.",
            "type" : "string",
            "enum" : [ "PAUSED", "UNPAUSED" ]
          },
          "quartz_cron_expression" : {
            "example" : "20 30 * * * ?",
            "description" : "A Cron expression using Quartz syntax that describes the schedule for a job.\nSee [Cron Trigger](http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html)\nfor details. This field is required.\"\n",
            "type" : "string"
          },
          "timezone_id" : {
            "example" : "Europe/London",
            "description" : "A Java timezone ID. The schedule for a job is resolved with respect to this timezone.\nSee [Java TimeZone](https://docs.oracle.com/javase/7/docs/api/java/util/TimeZone.html) for details.\nThis field is required.\n",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "jobs.DbtOutput" : {
        "properties" : {
          "artifacts_headers" : {
            "properties" : { },
            "description" : "An optional map of headers to send when retrieving the artifact from the `artifacts_link`.",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "artifacts_link" : {
            "description" : "A pre-signed URL to download the (compressed) dbt artifacts. This link is valid for a limited time (30 minutes). This information is only available after the run has finished.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "jobs.DbtTask" : {
        "required" : [ "commands" ],
        "properties" : {
          "catalog" : {
            "example" : "main",
            "description" : "Optional name of the catalog to use. The value is the top level in the 3-level namespace of Unity Catalog (catalog / schema / relation). The catalog value can only be specified if a warehouse_id is specified. Requires dbt-databricks >= 1.1.1.",
            "type" : "string"
          },
          "project_directory" : {
            "description" : "Optional (relative) path to the project directory, if no value is provided, the root of the git repository is used.",
            "type" : "string"
          },
          "schema" : {
            "description" : "Optional schema to write to. This parameter is only used when a warehouse_id is also provided. If not provided, the `default` schema is used.",
            "type" : "string"
          },
          "profiles_directory" : {
            "description" : "Optional (relative) path to the profiles directory. Can only be specified if no warehouse_id is specified. If no warehouse_id is specified and this folder is unset, the root directory is used.",
            "type" : "string"
          },
          "commands" : {
            "example" : [ "dbt deps", "dbt seed", "dbt run --models 123" ],
            "description" : "A list of dbt commands to execute. All commands must start with `dbt`. This parameter must not be empty. A maximum of up to 10 commands can be provided.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "warehouse_id" : {
            "example" : "30dade0507d960d1",
            "description" : "ID of the SQL warehouse to connect to. If provided, we automatically generate and provide the profile and connection details to dbt. It can be overridden on a per-command basis by using the `--profiles-dir` command line argument.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.DeleteJob" : {
        "required" : [ "job_id" ],
        "properties" : {
          "job_id" : {
            "example" : 11223344,
            "format" : "int64",
            "description" : "The canonical identifier of the job to delete. This field is required.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "jobs.DeleteRun" : {
        "required" : [ "run_id" ],
        "properties" : {
          "run_id" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "The canonical identifier of the run for which to retrieve the metadata.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "jobs.ExportRunOutput" : {
        "properties" : {
          "views" : {
            "description" : "The exported content in HTML format (one for every view item).",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.ViewItem"
            }
          }
        },
        "type" : "object"
      },
      "jobs.GitSnapshot" : {
        "readOnly" : true,
        "properties" : {
          "used_commit" : {
            "example" : "4506fdf41e9fa98090570a34df7a5bce163ff15f",
            "description" : "Commit that was used to execute the run. If git_branch was specified, this points to the HEAD of the branch at the time of the run; if git_tag was specified, this points to the commit the tag points to.",
            "type" : "string"
          }
        },
        "description" : "Read-only state of the remote repository at the time the job was run. This field is only included on job runs.",
        "type" : "object"
      },
      "jobs.GitSource" : {
        "example" : {
          "git_branch" : "main",
          "git_provider" : "gitHub",
          "git_url" : "https://github.com/databricks/databricks-cli"
        },
        "required" : [ "git_url", "git_provider" ],
        "properties" : {
          "git_tag" : {
            "example" : "release-1.0.0",
            "description" : "Name of the tag to be checked out and used by this job.\nThis field cannot be specified in conjunction with git_branch or git_commit.\n\nThe maximum length is 255 characters.\n",
            "type" : "string"
          },
          "git_provider" : {
            "example" : "github",
            "description" : "Unique identifier of the service used to host the Git repository. The value is case insensitive.",
            "type" : "string",
            "enum" : [ "gitHub", "bitbucketCloud", "azureDevOpsServices", "gitHubEnterprise", "bitbucketServer", "gitLab", "gitLabEnterpriseEdition", "awsCodeCommit" ]
          },
          "git_url" : {
            "example" : "https://github.com/databricks/databricks-cli",
            "description" : "URL of the repository to be cloned by this job.\nThe maximum length is 300 characters.",
            "type" : "string"
          },
          "git_branch" : {
            "example" : "main",
            "description" : "Name of the branch to be checked out and used by this job.\nThis field cannot be specified in conjunction with git_tag or git_commit.\n\nThe maximum length is 255 characters.\n",
            "type" : "string"
          },
          "git_commit" : {
            "example" : "e0056d01",
            "description" : "Commit to be checked out and used by this job. This field cannot be specified in conjunction with git_branch or git_tag.\nThe maximum length is 64 characters.",
            "type" : "string"
          },
          "git_snapshot" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.GitSnapshot"
          }
        },
        "description" : "An optional specification for a remote repository containing the notebooks used by this job's notebook tasks.",
        "type" : "object",
        "x-databricks-preview" : "PUBLIC"
      },
      "jobs.Job" : {
        "properties" : {
          "created_time" : {
            "example" : 1601370337343,
            "format" : "int64",
            "description" : "The time at which this job was created in epoch milliseconds (milliseconds since 1/1/1970 UTC).",
            "type" : "integer"
          },
          "job_id" : {
            "example" : 11223344,
            "format" : "int64",
            "description" : "The canonical identifier for this job.",
            "x-databricks-id" : true,
            "type" : "integer"
          },
          "creator_user_name" : {
            "example" : "user.name@databricks.com",
            "description" : "The creator user name. This field wont be included in the response if the user has already been deleted.",
            "type" : "string"
          },
          "run_as_user_name" : {
            "example" : "user.name@databricks.com",
            "description" : "The user name that the job runs as. `run_as_user_name` is based on the current job settings, and is set to the creator of the job if job access control is disabled, or the `is_owner` permission if job access control is enabled.",
            "type" : "string"
          },
          "settings" : {
            "description" : "Settings for this job and all of its runs. These settings can be updated using the `resetJob` method.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-name" : true,
            "$ref" : "#/components/schemas/jobs.JobSettings"
          }
        },
        "type" : "object"
      },
      "jobs.JobCluster" : {
        "required" : [ "job_cluster_key" ],
        "properties" : {
          "job_cluster_key" : {
            "example" : "auto_scaling_cluster",
            "description" : "A unique name for the job cluster. This field is required and must be unique within the job.\n`JobTaskSettings` may refer to this field to determine which cluster to launch for the task execution.",
            "type" : "string"
          },
          "new_cluster" : {
            "description" : "If new_cluster, a description of a cluster that is created for each task.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.BaseClusterInfo"
          }
        },
        "type" : "object"
      },
      "jobs.JobEmailNotifications" : {
        "properties" : {
          "no_alert_for_skipped_runs" : {
            "example" : false,
            "description" : "If true, do not send email to recipients specified in `on_failure` if the run is skipped.",
            "type" : "boolean"
          },
          "on_failure" : {
            "example" : [ "user.name@databricks.com" ],
            "description" : "A list of email addresses to be notified when a run unsuccessfully completes. A run is considered to have completed unsuccessfully if it ends with an `INTERNAL_ERROR` `life_cycle_state` or a `SKIPPED`, `FAILED`, or `TIMED_OUT` result_state. If this is not specified on job creation, reset, or update the list is empty, and notifications are not sent.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "on_start" : {
            "example" : [ "user.name@databricks.com" ],
            "description" : "A list of email addresses to be notified when a run begins. If not specified on job creation, reset, or update, the list is empty, and notifications are not sent.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "on_success" : {
            "example" : [ "user.name@databricks.com" ],
            "description" : "A list of email addresses to be notified when a run successfully completes. A run is considered to have completed successfully if it ends with a `TERMINATED` `life_cycle_state` and a `SUCCESSFUL` result_state. If not specified on job creation, reset, or update, the list is empty, and notifications are not sent.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          }
        },
        "type" : "object"
      },
      "jobs.JobSettings" : {
        "properties" : {
          "format" : {
            "example" : "MULTI_TASK",
            "description" : "Used to tell what is the format of the job. This field is ignored in Create/Update/Reset calls. When using the Jobs API 2.1 this value is always set to `\"MULTI_TASK\"`.",
            "type" : "string",
            "enum" : [ "SINGLE_TASK", "MULTI_TASK" ]
          },
          "continuous" : {
            "description" : "An optional continuous property for this job. The continuous property will ensure that there is always one run executing. Only one of `schedule` and `continuous` can be used.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.Continuous"
          },
          "name" : {
            "default" : "Untitled",
            "example" : "A multitask job",
            "description" : "An optional name for the job.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "job_clusters" : {
            "example" : [ {
              "job_cluster_key" : "auto_scaling_cluster",
              "new_cluster" : {
                "autoscale" : {
                  "max_workers" : 16,
                  "min_workers" : 2
                },
                "node_type_id" : null,
                "spark_conf" : {
                  "spark.speculation" : true
                },
                "spark_version" : "7.3.x-scala2.12"
              }
            } ],
            "description" : "A list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.JobCluster"
            }
          },
          "email_notifications" : {
            "description" : "An optional set of email addresses that is notified when runs of this job begin or complete as well as when this job is deleted. The default behavior is to not send any emails.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.JobEmailNotifications"
          },
          "tags" : {
            "default" : "{}",
            "example" : {
              "cost-center" : "engineering",
              "team" : "jobs"
            },
            "properties" : { },
            "description" : "A map of tags associated with the job. These are forwarded to the cluster as cluster tags for jobs clusters, and are subject to the same limitations as cluster tags. A maximum of 25 tags can be added to the job.",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "timeout_seconds" : {
            "example" : 86400,
            "format" : "int32",
            "description" : "An optional timeout applied to each run of this job. The default behavior is to have no timeout.",
            "type" : "integer"
          },
          "tasks" : {
            "example" : [ {
              "max_retries" : 3,
              "task_key" : "Sessionize",
              "description" : "Extracts session data from events",
              "min_retry_interval_millis" : 2000,
              "depends_on" : [ ],
              "timeout_seconds" : 86400,
              "spark_jar_task" : {
                "main_class_name" : "com.databricks.Sessionize",
                "parameters" : [ "--data", "dbfs:/path/to/data.json" ]
              },
              "libraries" : [ {
                "jar" : "dbfs:/mnt/databricks/Sessionize.jar"
              } ],
              "retry_on_timeout" : false,
              "existing_cluster_id" : "0923-164208-meows279"
            }, {
              "max_retries" : 3,
              "task_key" : "Orders_Ingest",
              "description" : "Ingests order data",
              "job_cluster_key" : "auto_scaling_cluster",
              "min_retry_interval_millis" : 2000,
              "depends_on" : [ ],
              "timeout_seconds" : 86400,
              "spark_jar_task" : {
                "main_class_name" : "com.databricks.OrdersIngest",
                "parameters" : [ "--data", "dbfs:/path/to/order-data.json" ]
              },
              "libraries" : [ {
                "jar" : "dbfs:/mnt/databricks/OrderIngest.jar"
              } ],
              "retry_on_timeout" : false
            }, {
              "max_retries" : 3,
              "task_key" : "Match",
              "description" : "Matches orders with user sessions",
              "notebook_task" : {
                "base_parameters" : {
                  "age" : "35",
                  "name" : "John Doe"
                },
                "notebook_path" : "/Users/user.name@databricks.com/Match"
              },
              "min_retry_interval_millis" : 2000,
              "depends_on" : [ {
                "task_key" : "Orders_Ingest"
              }, {
                "task_key" : "Sessionize"
              } ],
              "new_cluster" : {
                "autoscale" : {
                  "max_workers" : 16,
                  "min_workers" : 2
                },
                "node_type_id" : null,
                "spark_conf" : {
                  "spark.speculation" : true
                },
                "spark_version" : "7.3.x-scala2.12"
              },
              "timeout_seconds" : 86400,
              "retry_on_timeout" : false
            } ],
            "description" : "A list of task specifications to be executed by this job.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.JobTaskSettings"
            }
          },
          "max_concurrent_runs" : {
            "example" : 10,
            "format" : "int32",
            "description" : "An optional maximum allowed number of concurrent runs of the job.\n\nSet this value if you want to be able to execute multiple runs of the same job concurrently. This is useful for example if you trigger your job on a frequent schedule and want to allow consecutive runs to overlap with each other, or if you want to trigger multiple runs which differ by their input parameters.\n\nThis setting affects only new runs. For example, suppose the jobs concurrency is 4 and there are 4 concurrent active runs. Then setting the concurrency to 3 wont kill any of the active runs. However, from then on, new runs are skipped unless there are fewer than 3 active runs.\n\nThis value cannot exceed 1000\\. Setting this value to 0 causes all new runs to be skipped. The default behavior is to allow only 1 concurrent run.",
            "type" : "integer"
          },
          "schedule" : {
            "description" : "An optional periodic schedule for this job. The default behavior is that the job only runs when triggered by clicking Run Now in the Jobs UI or sending an API request to `runNow`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.CronSchedule"
          },
          "webhook_notifications" : {
            "description" : "A collection of system notification IDs to notify when the run begins or completes. The default behavior is to not send any system notifications.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.JobWebhookNotifications"
          },
          "git_source" : {
            "example" : {
              "git_branch" : "main",
              "git_provider" : "gitHub",
              "git_url" : "https://github.com/databricks/databricks-cli"
            },
            "description" : "An optional specification for a remote repository containing the notebooks used by this job's notebook tasks.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.GitSource"
          }
        },
        "x-databricks-name" : true,
        "type" : "object"
      },
      "jobs.JobTaskSettings" : {
        "required" : [ "task_key" ],
        "properties" : {
          "dbt_task" : {
            "description" : "If dbt_task, indicates that this must execute a dbt task. It requires both Databricks SQL and the ability to use a serverless or a pro SQL warehouse.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.DbtTask"
          },
          "spark_python_task" : {
            "description" : "If spark_python_task, indicates that this task must run a Python file.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SparkPythonTask"
          },
          "pipeline_task" : {
            "description" : "If pipeline_task, indicates that this task must execute a Pipeline.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.PipelineTask"
          },
          "max_retries" : {
            "example" : 10,
            "format" : "int32",
            "description" : "An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with the `FAILED` result_state or `INTERNAL_ERROR` `life_cycle_state`. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.",
            "type" : "integer"
          },
          "task_key" : {
            "example" : "Task_Key",
            "description" : "A unique name for the task. This field is used to refer to this task from other tasks.\nThis field is required and must be unique within its parent job.\nOn Update or Reset, this field is used to reference the tasks to be updated or reset.\nThe maximum length is 100 characters.",
            "type" : "string"
          },
          "email_notifications" : {
            "description" : "An optional set of email addresses that is notified when runs of this task begin or complete as well as when this task is deleted. The default behavior is to not send any emails.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.JobEmailNotifications"
          },
          "description" : {
            "example" : "This is the description for this task.",
            "description" : "An optional description for this task.\nThe maximum length is 4096 bytes.",
            "type" : "string"
          },
          "job_cluster_key" : {
            "description" : "If job_cluster_key, this task is executed reusing the cluster specified in `job.settings.job_clusters`.",
            "type" : "string"
          },
          "notebook_task" : {
            "description" : "If notebook_task, indicates that this task must run a notebook. This field may not be specified in conjunction with spark_jar_task.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.NotebookTask"
          },
          "python_wheel_task" : {
            "description" : "If python_wheel_task, indicates that this job must execute a PythonWheel.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.PythonWheelTask"
          },
          "min_retry_interval_millis" : {
            "example" : 2000,
            "format" : "int32",
            "description" : "An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.",
            "type" : "integer"
          },
          "depends_on" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.TaskDependencies"
          },
          "sql_task" : {
            "description" : "If sql_task, indicates that this job must execute a SQL task.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-not-cloud" : "gcp",
            "$ref" : "#/components/schemas/jobs.SqlTask"
          },
          "new_cluster" : {
            "description" : "If new_cluster, a description of a cluster that is created for only for this task.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.BaseClusterInfo"
          },
          "timeout_seconds" : {
            "example" : 86400,
            "format" : "int32",
            "description" : "An optional timeout applied to each run of this job task. The default behavior is to have no timeout.",
            "type" : "integer"
          },
          "spark_jar_task" : {
            "description" : "If spark_jar_task, indicates that this task must run a JAR.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SparkJarTask"
          },
          "libraries" : {
            "description" : "An optional list of libraries to be installed on the cluster that executes the task. The default value is an empty list.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/libraries.Library"
            }
          },
          "spark_submit_task" : {
            "description" : "If spark_submit_task, indicates that this task must be launched by the spark submit script. This task can run only on new clusters.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SparkSubmitTask"
          },
          "retry_on_timeout" : {
            "example" : true,
            "description" : "An optional policy to specify whether to retry a task when it times out. The default behavior is to not retry on timeout.",
            "type" : "boolean"
          },
          "existing_cluster_id" : {
            "example" : "0923-164208-meows279",
            "description" : "If existing_cluster_id, the ID of an existing cluster that is used for all runs of this task. When running tasks on an existing cluster, you may need to manually restart the cluster if it stops responding. We suggest running jobs on new clusters for greater reliability.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "jobs.JobWebhookNotifications" : {
        "properties" : {
          "on_failure" : {
            "example" : [ {
              "id" : "0481e838-0a59-4eff-9541-a4ca6f149574"
            } ],
            "description" : "An optional list of system notification IDs to call when the run fails. A maximum of 3 destinations can be specified for the `on_failure` property.",
            "type" : "array",
            "items" : {
              "properties" : {
                "id" : {
                  "type" : "string"
                }
              },
              "type" : "object"
            }
          },
          "on_start" : {
            "example" : [ {
              "id" : "0481e838-0a59-4eff-9541-a4ca6f149574"
            } ],
            "description" : "An optional list of system notification IDs to call when the run starts. A maximum of 3 destinations can be specified for the `on_start` property.",
            "type" : "array",
            "items" : {
              "properties" : {
                "id" : {
                  "type" : "string"
                }
              },
              "type" : "object"
            }
          },
          "on_success" : {
            "example" : [ {
              "id" : "0481e838-0a59-4eff-9541-a4ca6f149574"
            } ],
            "description" : "An optional list of system notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified for the `on_success` property.",
            "type" : "array",
            "items" : {
              "properties" : {
                "id" : {
                  "type" : "string"
                }
              },
              "type" : "object"
            }
          }
        },
        "type" : "object"
      },
      "jobs.ListJobsResponse" : {
        "properties" : {
          "has_more" : {
            "example" : false,
            "type" : "boolean"
          },
          "jobs" : {
            "description" : "The list of jobs.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.BaseJob"
            }
          }
        },
        "type" : "object"
      },
      "jobs.ListRunsResponse" : {
        "properties" : {
          "has_more" : {
            "description" : "If true, additional runs matching the provided filter are available for listing.",
            "type" : "boolean"
          },
          "runs" : {
            "description" : "A list of runs, from most recently started to least.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.BaseRun"
            }
          }
        },
        "type" : "object"
      },
      "jobs.NotebookOutput" : {
        "properties" : {
          "result" : {
            "example" : "An arbitrary string passed by calling dbutils.notebook.exit(...)",
            "description" : "The value passed to [dbutils.notebook.exit()](/notebooks/notebook-workflows.html#notebook-workflows-exit).\nDatabricks restricts this API to return the first 5 MB of the value.\nFor a larger result, your job can store the results in a cloud storage service. This field is absent if\n`dbutils.notebook.exit()` was never called.\n",
            "type" : "string"
          },
          "truncated" : {
            "example" : false,
            "description" : "Whether or not the result was truncated.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "jobs.NotebookTask" : {
        "required" : [ "notebook_path" ],
        "properties" : {
          "base_parameters" : {
            "example" : {
              "age" : "35",
              "name" : "John Doe"
            },
            "properties" : { },
            "description" : "Base parameters to be used for each run of this job. If the run is initiated by a call to\n:method:jobs/runNow with parameters specified, the two parameters maps are merged. If the same key is specified in\n`base_parameters` and in `run-now`, the value from `run-now` is used.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nIf the notebook takes a parameter that is not specified in the jobs `base_parameters` or the `run-now` override parameters,\nthe default value from the notebook is used.\n\nRetrieve these parameters in a notebook using [dbutils.widgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-widgets).\n",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "notebook_path" : {
            "example" : "/Users/user.name@databricks.com/notebook_to_run",
            "description" : "The path of the notebook to be run in the Databricks workspace or remote repository.\nFor notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash.\nFor notebooks stored in a remote repository, the path must be relative. This field is required.\n",
            "type" : "string"
          },
          "source" : {
            "example" : "WORKSPACE",
            "description" : "This describes an enum",
            "type" : "string",
            "enum" : [ "WORKSPACE", "GIT" ],
            "x-databricks-enum-descriptions" : {
              "WORKSPACE" : "Notebook is located in Databricks workspace.",
              "GIT" : "Notebook is located in cloud Git provider."
            }
          }
        },
        "type" : "object"
      },
      "jobs.PipelineParams" : {
        "properties" : {
          "full_refresh" : {
            "description" : "If true, triggers a full refresh on the delta live table.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "jobs.PipelineTask" : {
        "properties" : {
          "full_refresh" : {
            "default" : false,
            "description" : "If true, a full refresh will be triggered on the delta live table.",
            "type" : "boolean"
          },
          "pipeline_id" : {
            "example" : "a12cd3e4-0ab1-1abc-1a2b-1a2bcd3e4fg5",
            "description" : "The full name of the pipeline task to execute.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "jobs.PythonWheelTask" : {
        "properties" : {
          "entry_point" : {
            "description" : "Named entry point to use, if it does not exist in the metadata of the package it executes the function from the package directly using `$packageName.$entryPoint()`",
            "type" : "string"
          },
          "named_parameters" : {
            "example" : {
              "data" : "dbfs:/path/to/data.json",
              "name" : "task"
            },
            "properties" : { },
            "description" : "Command-line parameters passed to Python wheel task in the form of `[\"--name=task\", \"--data=dbfs:/path/to/data.json\"]`. Leave it empty if `parameters` is not null.",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "package_name" : {
            "description" : "Name of the package to execute",
            "type" : "string"
          },
          "parameters" : {
            "example" : [ "--name=task", "one", "two" ],
            "description" : "Command-line parameters passed to Python wheel task. Leave it empty if `named_parameters` is not null.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          }
        },
        "type" : "object"
      },
      "jobs.RepairHistoryItem" : {
        "properties" : {
          "task_run_ids" : {
            "example" : [ 1106460542112844, 988297789683452 ],
            "description" : "The run IDs of the task runs that ran as part of this repair history item.",
            "type" : "array",
            "items" : {
              "format" : "int64",
              "type" : "integer"
            }
          },
          "start_time" : {
            "example" : 1625060460483,
            "format" : "int64",
            "description" : "The start time of the (repaired) run.",
            "type" : "integer"
          },
          "state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.RunState"
          },
          "end_time" : {
            "example" : 1625060863413,
            "format" : "int64",
            "description" : "The end time of the (repaired) run.",
            "type" : "integer"
          },
          "id" : {
            "example" : 734650698524280,
            "format" : "int64",
            "description" : "The ID of the repair. Only returned for the items that represent a repair in `repair_history`.",
            "type" : "integer"
          },
          "type" : {
            "description" : "The repair history item type. Indicates whether a run is the original run or a repair run.",
            "type" : "string",
            "enum" : [ "ORIGINAL", "REPAIR" ]
          }
        },
        "type" : "object"
      },
      "jobs.RepairRun" : {
        "required" : [ "run_id" ],
        "properties" : {
          "pipeline_params" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.PipelineParams"
          },
          "python_params" : {
            "example" : [ "john doe", "35" ],
            "description" : "A list of parameters for jobs with Python tasks, for example `\\\"python_params\\\": [\\\"john doe\\\", \\\"35\\\"]`.\nThe parameters are passed to Python file as command-line parameters. If specified upon `run-now`, it would overwrite\nthe parameters specified in job setting. The JSON representation of this field (for example `{\\\"python_params\\\":[\\\"john doe\\\",\\\"35\\\"]}`)\ncannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nImportant\n\nThese parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error.\nExamples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "rerun_all_failed_tasks" : {
            "default" : false,
            "description" : "If true, repair all failed tasks. Only one of rerun_tasks or rerun_all_failed_tasks can be used.",
            "type" : "boolean"
          },
          "python_named_params" : {
            "example" : {
              "data" : "dbfs:/path/to/data.json",
              "name" : "task"
            },
            "properties" : { },
            "description" : "A map from keys to values for jobs with Python wheel task, for example `\"python_named_params\": {\"name\": \"task\", \"data\": \"dbfs:/path/to/data.json\"}`.",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "notebook_params" : {
            "example" : {
              "age" : "35",
              "name" : "john doe"
            },
            "properties" : { },
            "description" : "A map from keys to values for jobs with notebook task, for example `\\\"notebook_params\\\": {\\\"name\\\": \\\"john doe\\\", \\\"age\\\": \\\"35\\\"}`.\nThe map is passed to the notebook and is accessible through the [dbutils.widgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html) function.\n\nIf not specified upon `run-now`, the triggered run uses the jobs base parameters.\n\nnotebook_params cannot be specified in conjunction with jar_params.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nThe JSON representation of this field (for example `{\\\"notebook_params\\\":{\\\"name\\\":\\\"john doe\\\",\\\"age\\\":\\\"35\\\"}}`) cannot exceed 10,000 bytes.\n",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "jar_params" : {
            "example" : [ "john", "doe", "35" ],
            "description" : "A list of parameters for jobs with Spark JAR tasks, for example `\\\"jar_params\\\": [\\\"john doe\\\", \\\"35\\\"]`.\nThe parameters are used to invoke the main function of the main class specified in the Spark JAR task.\nIf not specified upon `run-now`, it defaults to an empty list.\njar_params cannot be specified in conjunction with notebook_params.\nThe JSON representation of this field (for example `{\\\"jar_params\\\":[\\\"john doe\\\",\\\"35\\\"]}`) cannot exceed 10,000 bytes.\n\nUse [Task parameter variables](/jobs.html\"#parameter-variables\") to set parameters containing information about job runs.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "latest_repair_id" : {
            "example" : 734650698524280,
            "format" : "int64",
            "description" : "The ID of the latest repair. This parameter is not required when repairing a run for the first time, but must be provided on subsequent requests to repair the same run.",
            "type" : "integer"
          },
          "run_id" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "The job run ID of the run to repair. The run must not be in progress.",
            "x-databricks-id" : true,
            "type" : "integer"
          },
          "rerun_tasks" : {
            "example" : [ "task0", "task1" ],
            "description" : "The task keys of the task runs to repair.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "dbt_commands" : {
            "example" : [ "dbt deps", "dbt seed", "dbt run" ],
            "description" : "An array of commands to execute for jobs with the dbt task, for example `\"dbt_commands\": [\"dbt deps\", \"dbt seed\", \"dbt run\"]`",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "spark_submit_params" : {
            "example" : [ "--class", "org.apache.spark.examples.SparkPi" ],
            "description" : "A list of parameters for jobs with spark submit task, for example `\\\"spark_submit_params\\\": [\\\"--class\\\", \\\"org.apache.spark.examples.SparkPi\\\"]`.\nThe parameters are passed to spark-submit script as command-line parameters. If specified upon `run-now`, it would overwrite the\nparameters specified in job setting. The JSON representation of this field (for example `{\\\"python_params\\\":[\\\"john doe\\\",\\\"35\\\"]}`)\ncannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs\n\nImportant\n\nThese parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error.\nExamples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "sql_params" : {
            "example" : {
              "age" : "35",
              "name" : "john doe"
            },
            "properties" : { },
            "description" : "A map from keys to values for jobs with SQL task, for example `\"sql_params\": {\"name\": \"john doe\", \"age\": \"35\"}`. The SQL alert task does not support custom parameters.",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            },
            "x-databricks-not-cloud" : "gcp"
          }
        },
        "type" : "object"
      },
      "jobs.ResetJob" : {
        "required" : [ "job_id", "new_settings" ],
        "properties" : {
          "job_id" : {
            "example" : 11223344,
            "format" : "int64",
            "description" : "The canonical identifier of the job to reset. This field is required.",
            "type" : "integer"
          },
          "new_settings" : {
            "description" : "The new settings of the job. These settings completely replace the old settings.\n\nChanges to the field `JobSettings.timeout_seconds` are applied to active runs. Changes to other fields are applied to future runs only.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.JobSettings"
          }
        },
        "type" : "object"
      },
      "jobs.Run" : {
        "properties" : {
          "continuous" : {
            "description" : "The continuous trigger that triggered this run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.Continuous"
          },
          "setup_duration" : {
            "example" : 0,
            "format" : "int64",
            "description" : "The time in milliseconds it took to set up the cluster. For runs that run on new clusters this is the cluster creation time, for runs that run on existing clusters this time should be very short. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `setup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.",
            "type" : "integer"
          },
          "start_time" : {
            "example" : 1625060460483,
            "format" : "int64",
            "description" : "The time at which this run was started in epoch milliseconds (milliseconds since 1/1/1970 UTC). This may not be the time when the job task starts executing, for example, if the job is scheduled to run on a new cluster, this is the time the cluster creation call is issued.",
            "type" : "integer"
          },
          "job_clusters" : {
            "example" : [ {
              "job_cluster_key" : "auto_scaling_cluster",
              "new_cluster" : {
                "autoscale" : {
                  "max_workers" : 16,
                  "min_workers" : 2
                },
                "node_type_id" : null,
                "spark_conf" : {
                  "spark.speculation" : true
                },
                "spark_version" : "7.3.x-scala2.12"
              }
            } ],
            "description" : "A list of job cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.JobCluster"
            }
          },
          "state" : {
            "description" : "The result and lifecycle states of the run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.RunState"
          },
          "original_attempt_run_id" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "If this run is a retry of a prior run attempt, this field contains the run_id of the original attempt; otherwise, it is the same as the run_id.",
            "type" : "integer"
          },
          "overriding_parameters" : {
            "description" : "The parameters used for this run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.RunParameters"
          },
          "repair_history" : {
            "description" : "The repair history of the run.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.RepairHistoryItem"
            }
          },
          "end_time" : {
            "example" : 1625060863413,
            "format" : "int64",
            "description" : "The time at which this run ended in epoch milliseconds (milliseconds since 1/1/1970 UTC). This field is set to 0 if the job is still running.",
            "type" : "integer"
          },
          "job_id" : {
            "example" : 11223344,
            "format" : "int64",
            "description" : "The canonical identifier of the job that contains this run.",
            "type" : "integer"
          },
          "run_page_url" : {
            "example" : "https://my-workspace.cloud.databricks.com/#job/11223344/run/123",
            "description" : "The URL to the detail page of the run.",
            "type" : "string"
          },
          "number_in_job" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "A unique identifier for this job run. This is set to the same value as `run_id`.",
            "deprecated" : true,
            "type" : "integer"
          },
          "run_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.RunType"
          },
          "run_id" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "The canonical identifier of the run. This ID is unique across all runs of all jobs.",
            "x-databricks-id" : true,
            "type" : "integer"
          },
          "cluster_instance" : {
            "description" : "The cluster used for this run. If the run is specified to use a new cluster, this field is set once the Jobs service has requested a cluster for the run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.ClusterInstance"
          },
          "tasks" : {
            "example" : [ {
              "setup_duration" : 0,
              "start_time" : 1629989929660,
              "task_key" : "Orders_Ingest",
              "state" : {
                "life_cycle_state" : "INTERNAL_ERROR",
                "result_state" : "FAILED",
                "state_message" : "Library installation failed for library due to user error. Error messages:\n'Manage' permissions are required to install libraries on a cluster",
                "user_cancelled_or_timedout" : false
              },
              "description" : "Ingests order data",
              "job_cluster_key" : "auto_scaling_cluster",
              "end_time" : 1629989930171,
              "run_page_url" : "https://my-workspace.cloud.databricks.com/#job/39832/run/20",
              "run_id" : 2112892,
              "cluster_instance" : {
                "cluster_id" : "0923-164208-meows279",
                "spark_context_id" : "4348585301701786933"
              },
              "spark_jar_task" : {
                "main_class_name" : "com.databricks.OrdersIngest"
              },
              "libraries" : [ {
                "jar" : "dbfs:/mnt/databricks/OrderIngest.jar"
              } ],
              "attempt_number" : 0,
              "cleanup_duration" : 0,
              "execution_duration" : 0
            }, {
              "setup_duration" : 0,
              "start_time" : 0,
              "task_key" : "Match",
              "state" : {
                "life_cycle_state" : "SKIPPED",
                "state_message" : "An upstream task failed.",
                "user_cancelled_or_timedout" : false
              },
              "description" : "Matches orders with user sessions",
              "notebook_task" : {
                "notebook_path" : "/Users/user.name@databricks.com/Match",
                "source" : "WORKSPACE"
              },
              "end_time" : 1629989930238,
              "depends_on" : [ {
                "task_key" : "Orders_Ingest"
              }, {
                "task_key" : "Sessionize"
              } ],
              "run_page_url" : "https://my-workspace.cloud.databricks.com/#job/39832/run/21",
              "new_cluster" : {
                "autoscale" : {
                  "max_workers" : 16,
                  "min_workers" : 2
                },
                "node_type_id" : null,
                "spark_conf" : {
                  "spark.speculation" : true
                },
                "spark_version" : "7.3.x-scala2.12"
              },
              "run_id" : 2112897,
              "cluster_instance" : {
                "cluster_id" : "0923-164208-meows279"
              },
              "attempt_number" : 0,
              "cleanup_duration" : 0,
              "execution_duration" : 0
            }, {
              "setup_duration" : 0,
              "start_time" : 1629989929668,
              "task_key" : "Sessionize",
              "state" : {
                "life_cycle_state" : "INTERNAL_ERROR",
                "result_state" : "FAILED",
                "state_message" : "Library installation failed for library due to user error. Error messages:\n'Manage' permissions are required to install libraries on a cluster",
                "user_cancelled_or_timedout" : false
              },
              "description" : "Extracts session data from events",
              "end_time" : 1629989930144,
              "run_page_url" : "https://my-workspace.cloud.databricks.com/#job/39832/run/22",
              "run_id" : 2112902,
              "cluster_instance" : {
                "cluster_id" : "0923-164208-meows279",
                "spark_context_id" : "4348585301701786933"
              },
              "spark_jar_task" : {
                "main_class_name" : "com.databricks.Sessionize"
              },
              "libraries" : [ {
                "jar" : "dbfs:/mnt/databricks/Sessionize.jar"
              } ],
              "attempt_number" : 0,
              "existing_cluster_id" : "0923-164208-meows279",
              "cleanup_duration" : 0,
              "execution_duration" : 0
            } ],
            "description" : "The list of tasks performed by the run. Each task has its own `run_id` which you can use to call `JobsGetOutput` to retrieve the run resutls.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.RunTask"
            }
          },
          "run_name" : {
            "default" : "Untitled",
            "example" : "A multitask job run",
            "description" : "An optional name for the run. The maximum allowed length is 4096 bytes in UTF-8 encoding.",
            "type" : "string"
          },
          "creator_user_name" : {
            "example" : "user.name@databricks.com",
            "description" : "The creator user name. This field wont be included in the response if the user has already been deleted.",
            "type" : "string"
          },
          "schedule" : {
            "description" : "The cron schedule that triggered this run if it was triggered by the periodic scheduler.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.CronSchedule"
          },
          "attempt_number" : {
            "example" : 0,
            "format" : "int32",
            "description" : "The sequence number of this run attempt for a triggered job run. The initial attempt of a run has an attempt_number of 0\\. If the initial run attempt fails, and the job has a retry policy (`max_retries` \\> 0), subsequent runs are created with an `original_attempt_run_id` of the original attempts ID and an incrementing `attempt_number`. Runs are retried only until they succeed, and the maximum `attempt_number` is the same as the `max_retries` value for the job.",
            "type" : "integer"
          },
          "run_duration" : {
            "example" : 110183,
            "description" : "The time in milliseconds it took the job run and all of its repairs to finish.",
            "type" : "integer"
          },
          "git_source" : {
            "example" : {
              "git_branch" : "main",
              "git_provider" : "gitHub",
              "git_url" : "https://github.com/databricks/databricks-cli"
            },
            "description" : "An optional specification for a remote repository containing the notebooks used by this job's notebook tasks.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.GitSource"
          },
          "cleanup_duration" : {
            "example" : 0,
            "format" : "int64",
            "description" : "The time in milliseconds it took to terminate the cluster and clean up any associated artifacts. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `cleanup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.",
            "type" : "integer"
          },
          "execution_duration" : {
            "example" : 0,
            "format" : "int64",
            "description" : "The time in milliseconds it took to execute the commands in the JAR or notebook until they  completed, failed, timed out, were cancelled, or encountered an unexpected error. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the  `cleanup_duration`. The `execution_duration` field is set to 0 for multitask job runs. The total  duration of a multitask job run is the value of the `run_duration` field.",
            "type" : "integer"
          },
          "cluster_spec" : {
            "description" : "A snapshot of the jobs cluster specification when this run was created.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.ClusterSpec"
          },
          "trigger" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.TriggerType"
          }
        },
        "type" : "object"
      },
      "jobs.RunLifeCycleState" : {
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "PENDING", "RUNNING", "TERMINATING", "TERMINATED", "SKIPPED", "INTERNAL_ERROR", "BLOCKED", "WAITING_FOR_RETRY" ],
        "x-databricks-enum-descriptions" : {
          "TERMINATED" : "The task of this run has completed, and the cluster and execution context have been cleaned up. This state is terminal.",
          "WAITING_FOR_RETRY" : "The run is waiting for a retry.",
          "RUNNING" : "The task of this run is being executed.",
          "TERMINATING" : "The task of this run has completed, and the cluster and execution context are being cleaned up.",
          "PENDING" : "The run has been triggered. If there is not already an active run of the same job, the cluster and execution context are being prepared. If there is already an active run of the same job, the run immediately transitions into the `SKIPPED` state without preparing any resources.",
          "INTERNAL_ERROR" : "An exceptional state that indicates a failure in the Jobs service, such as network failure over a long period. If a run on a new cluster ends in the `INTERNAL_ERROR` state, the Jobs service terminates the cluster as soon as possible. This state is terminal.",
          "BLOCKED" : "The run is blocked on an upstream dependency.",
          "SKIPPED" : "This run was aborted because a previous run of the same job was already active. This state is terminal."
        }
      },
      "jobs.RunNow" : {
        "required" : [ "job_id" ],
        "properties" : {
          "pipeline_params" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.PipelineParams"
          },
          "python_params" : {
            "example" : [ "john doe", "35" ],
            "description" : "A list of parameters for jobs with Python tasks, for example `\\\"python_params\\\": [\\\"john doe\\\", \\\"35\\\"]`.\nThe parameters are passed to Python file as command-line parameters. If specified upon `run-now`, it would overwrite\nthe parameters specified in job setting. The JSON representation of this field (for example `{\\\"python_params\\\":[\\\"john doe\\\",\\\"35\\\"]}`)\ncannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nImportant\n\nThese parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error.\nExamples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "idempotency_token" : {
            "example" : "8f018174-4792-40d5-bcbc-3e6a527352c8",
            "description" : "An optional token to guarantee the idempotency of job run requests. If a run with the provided token already exists,\nthe request does not create a new run but returns the ID of the existing run instead. If a run with the provided token is deleted,\nan error is returned.\n\nIf you specify the idempotency token, upon failure you can retry until the request succeeds. Databricks guarantees that exactly one run\nis launched with that idempotency token.\n\nThis token must have at most 64 characters.\n\nFor more information, see [How to ensure idempotency for jobs](\nhttps://kb.databricks.com/jobs/jobs-idempotency.html).",
            "type" : "string"
          },
          "python_named_params" : {
            "example" : {
              "data" : "dbfs:/path/to/data.json",
              "name" : "task"
            },
            "properties" : { },
            "description" : "A map from keys to values for jobs with Python wheel task, for example `\"python_named_params\": {\"name\": \"task\", \"data\": \"dbfs:/path/to/data.json\"}`.",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "notebook_params" : {
            "example" : {
              "age" : "35",
              "name" : "john doe"
            },
            "properties" : { },
            "description" : "A map from keys to values for jobs with notebook task, for example `\\\"notebook_params\\\": {\\\"name\\\": \\\"john doe\\\", \\\"age\\\": \\\"35\\\"}`.\nThe map is passed to the notebook and is accessible through the [dbutils.widgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html) function.\n\nIf not specified upon `run-now`, the triggered run uses the jobs base parameters.\n\nnotebook_params cannot be specified in conjunction with jar_params.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nThe JSON representation of this field (for example `{\\\"notebook_params\\\":{\\\"name\\\":\\\"john doe\\\",\\\"age\\\":\\\"35\\\"}}`) cannot exceed 10,000 bytes.\n",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "jar_params" : {
            "example" : [ "john", "doe", "35" ],
            "description" : "A list of parameters for jobs with Spark JAR tasks, for example `\\\"jar_params\\\": [\\\"john doe\\\", \\\"35\\\"]`.\nThe parameters are used to invoke the main function of the main class specified in the Spark JAR task.\nIf not specified upon `run-now`, it defaults to an empty list.\njar_params cannot be specified in conjunction with notebook_params.\nThe JSON representation of this field (for example `{\\\"jar_params\\\":[\\\"john doe\\\",\\\"35\\\"]}`) cannot exceed 10,000 bytes.\n\nUse [Task parameter variables](/jobs.html\"#parameter-variables\") to set parameters containing information about job runs.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "job_id" : {
            "example" : 11223344,
            "format" : "int64",
            "description" : "The ID of the job to be executed",
            "type" : "integer"
          },
          "dbt_commands" : {
            "example" : [ "dbt deps", "dbt seed", "dbt run" ],
            "description" : "An array of commands to execute for jobs with the dbt task, for example `\"dbt_commands\": [\"dbt deps\", \"dbt seed\", \"dbt run\"]`",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "spark_submit_params" : {
            "example" : [ "--class", "org.apache.spark.examples.SparkPi" ],
            "description" : "A list of parameters for jobs with spark submit task, for example `\\\"spark_submit_params\\\": [\\\"--class\\\", \\\"org.apache.spark.examples.SparkPi\\\"]`.\nThe parameters are passed to spark-submit script as command-line parameters. If specified upon `run-now`, it would overwrite the\nparameters specified in job setting. The JSON representation of this field (for example `{\\\"python_params\\\":[\\\"john doe\\\",\\\"35\\\"]}`)\ncannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs\n\nImportant\n\nThese parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error.\nExamples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "sql_params" : {
            "example" : {
              "age" : "35",
              "name" : "john doe"
            },
            "properties" : { },
            "description" : "A map from keys to values for jobs with SQL task, for example `\"sql_params\": {\"name\": \"john doe\", \"age\": \"35\"}`. The SQL alert task does not support custom parameters.",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            },
            "x-databricks-not-cloud" : "gcp"
          }
        },
        "type" : "object"
      },
      "jobs.RunNowResponse" : {
        "properties" : {
          "number_in_job" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "A unique identifier for this job run. This is set to the same value as `run_id`.",
            "deprecated" : true,
            "type" : "integer"
          },
          "run_id" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "The globally unique ID of the newly triggered run.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "jobs.RunOutput" : {
        "properties" : {
          "error_trace" : {
            "example" : "---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n      1 numerator = 42\n      2 denominator = 0\n----> 3 return numerator / denominator\n\nZeroDivisionError: integer division or modulo by zero",
            "description" : "If there was an error executing the run, this field contains any available stack traces.",
            "type" : "string"
          },
          "error" : {
            "example" : "ZeroDivisionError: integer division or modulo by zero",
            "description" : "An error message indicating why a task failed or why output is not available. The message is unstructured, and its exact format is subject to change.",
            "type" : "string"
          },
          "sql_output" : {
            "example" : "",
            "description" : "The output of a SQL task, if available.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-not-cloud" : "gcp",
            "$ref" : "#/components/schemas/jobs.SqlOutput"
          },
          "metadata" : {
            "description" : "All details of the run except for its output.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.Run"
          },
          "logs" : {
            "example" : "Hello World!",
            "description" : "The output from tasks that write to standard streams (stdout/stderr) such as\nspark_jar_task, spark_python_task, python_wheel_task.\n\nIt's not supported for the notebook_task, pipeline_task or spark_submit_task.\n\nDatabricks restricts this API to return the last 5 MB of these logs.\n",
            "type" : "string"
          },
          "logs_truncated" : {
            "example" : true,
            "description" : "Whether the logs are truncated.",
            "type" : "boolean"
          },
          "notebook_output" : {
            "example" : "",
            "description" : "The output of a notebook task, if available. A notebook task that terminates (either successfully or with a failure)\nwithout calling `dbutils.notebook.exit()` is considered to have an empty output.\nThis field is set but its result value is empty. <Databricks> restricts this API to return the first 5 MB of the output.\nTo return a larger result, use the [ClusterLogConf](/dev-tools/api/latest/clusters.html#clusterlogconf) field to configure log storage\nfor the job cluster.\n",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.NotebookOutput"
          },
          "dbt_output" : {
            "example" : "",
            "description" : "The output of a dbt task, if available.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.DbtOutput"
          }
        },
        "type" : "object"
      },
      "jobs.RunParameters" : {
        "properties" : {
          "pipeline_params" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.PipelineParams"
          },
          "python_params" : {
            "example" : [ "john doe", "35" ],
            "description" : "A list of parameters for jobs with Python tasks, for example `\\\"python_params\\\": [\\\"john doe\\\", \\\"35\\\"]`.\nThe parameters are passed to Python file as command-line parameters. If specified upon `run-now`, it would overwrite\nthe parameters specified in job setting. The JSON representation of this field (for example `{\\\"python_params\\\":[\\\"john doe\\\",\\\"35\\\"]}`)\ncannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nImportant\n\nThese parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error.\nExamples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "python_named_params" : {
            "example" : {
              "data" : "dbfs:/path/to/data.json",
              "name" : "task"
            },
            "properties" : { },
            "description" : "A map from keys to values for jobs with Python wheel task, for example `\"python_named_params\": {\"name\": \"task\", \"data\": \"dbfs:/path/to/data.json\"}`.",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "notebook_params" : {
            "example" : {
              "age" : "35",
              "name" : "john doe"
            },
            "properties" : { },
            "description" : "A map from keys to values for jobs with notebook task, for example `\\\"notebook_params\\\": {\\\"name\\\": \\\"john doe\\\", \\\"age\\\": \\\"35\\\"}`.\nThe map is passed to the notebook and is accessible through the [dbutils.widgets.get](https://docs.databricks.com/dev-tools/databricks-utils.html) function.\n\nIf not specified upon `run-now`, the triggered run uses the jobs base parameters.\n\nnotebook_params cannot be specified in conjunction with jar_params.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n\nThe JSON representation of this field (for example `{\\\"notebook_params\\\":{\\\"name\\\":\\\"john doe\\\",\\\"age\\\":\\\"35\\\"}}`) cannot exceed 10,000 bytes.\n",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "jar_params" : {
            "example" : [ "john", "doe", "35" ],
            "description" : "A list of parameters for jobs with Spark JAR tasks, for example `\\\"jar_params\\\": [\\\"john doe\\\", \\\"35\\\"]`.\nThe parameters are used to invoke the main function of the main class specified in the Spark JAR task.\nIf not specified upon `run-now`, it defaults to an empty list.\njar_params cannot be specified in conjunction with notebook_params.\nThe JSON representation of this field (for example `{\\\"jar_params\\\":[\\\"john doe\\\",\\\"35\\\"]}`) cannot exceed 10,000 bytes.\n\nUse [Task parameter variables](/jobs.html\"#parameter-variables\") to set parameters containing information about job runs.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "dbt_commands" : {
            "example" : [ "dbt deps", "dbt seed", "dbt run" ],
            "description" : "An array of commands to execute for jobs with the dbt task, for example `\"dbt_commands\": [\"dbt deps\", \"dbt seed\", \"dbt run\"]`",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "spark_submit_params" : {
            "example" : [ "--class", "org.apache.spark.examples.SparkPi" ],
            "description" : "A list of parameters for jobs with spark submit task, for example `\\\"spark_submit_params\\\": [\\\"--class\\\", \\\"org.apache.spark.examples.SparkPi\\\"]`.\nThe parameters are passed to spark-submit script as command-line parameters. If specified upon `run-now`, it would overwrite the\nparameters specified in job setting. The JSON representation of this field (for example `{\\\"python_params\\\":[\\\"john doe\\\",\\\"35\\\"]}`)\ncannot exceed 10,000 bytes.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs\n\nImportant\n\nThese parameters accept only Latin characters (ASCII character set). Using non-ASCII characters returns an error.\nExamples of invalid, non-ASCII characters are Chinese, Japanese kanjis, and emojis.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "sql_params" : {
            "example" : {
              "age" : "35",
              "name" : "john doe"
            },
            "properties" : { },
            "description" : "A map from keys to values for jobs with SQL task, for example `\"sql_params\": {\"name\": \"john doe\", \"age\": \"35\"}`. The SQL alert task does not support custom parameters.",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            },
            "x-databricks-not-cloud" : "gcp"
          }
        },
        "type" : "object"
      },
      "jobs.RunResultState" : {
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "SUCCESS", "FAILED", "TIMEDOUT", "CANCELED" ],
        "x-databricks-enum-descriptions" : {
          "SUCCESS" : "The task completed successfully.",
          "FAILED" : "The task completed with an error.",
          "TIMEDOUT" : "The run was stopped after reaching the timeout.",
          "CANCELED" : "The run was canceled at user request."
        }
      },
      "jobs.RunState" : {
        "properties" : {
          "life_cycle_state" : {
            "description" : "A description of a runs current location in the run lifecycle. This field is always available in the response.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.RunLifeCycleState"
          },
          "result_state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.RunResultState"
          },
          "state_message" : {
            "example" : "",
            "description" : "A descriptive message for the current state. This field is unstructured, and its exact format is subject to change.",
            "type" : "string"
          },
          "user_cancelled_or_timedout" : {
            "example" : false,
            "description" : "Whether a run was canceled manually by a user or by the scheduler because the run timed out.",
            "type" : "boolean"
          }
        },
        "description" : "The result and lifecycle state of the run.",
        "type" : "object"
      },
      "jobs.RunSubmitTaskSettings" : {
        "required" : [ "task_key" ],
        "properties" : {
          "spark_python_task" : {
            "description" : "If spark_python_task, indicates that this task must run a Python file.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SparkPythonTask"
          },
          "pipeline_task" : {
            "description" : "If pipeline_task, indicates that this task must execute a Pipeline.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.PipelineTask"
          },
          "task_key" : {
            "example" : "Task_Key",
            "description" : "A unique name for the task. This field is used to refer to this task from other tasks.\nThis field is required and must be unique within its parent job.\nOn Update or Reset, this field is used to reference the tasks to be updated or reset.\nThe maximum length is 100 characters.",
            "type" : "string"
          },
          "notebook_task" : {
            "description" : "If notebook_task, indicates that this task must run a notebook. This field may not be specified in conjunction with spark_jar_task.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.NotebookTask"
          },
          "python_wheel_task" : {
            "description" : "If python_wheel_task, indicates that this job must execute a PythonWheel.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.PythonWheelTask"
          },
          "depends_on" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.TaskDependencies"
          },
          "new_cluster" : {
            "example" : "",
            "description" : "If new_cluster, a description of a cluster that is created for each run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.BaseClusterInfo"
          },
          "timeout_seconds" : {
            "example" : 86400,
            "format" : "int32",
            "description" : "An optional timeout applied to each run of this job task. The default behavior is to have no timeout.",
            "type" : "integer"
          },
          "spark_jar_task" : {
            "description" : "If spark_jar_task, indicates that this task must run a JAR.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SparkJarTask"
          },
          "libraries" : {
            "description" : "An optional list of libraries to be installed on the cluster that executes the task. The default value is an empty list.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/libraries.Library"
            }
          },
          "spark_submit_task" : {
            "description" : "If spark_submit_task, indicates that this task must be launched by the spark submit script. This task can run only on new clusters.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SparkSubmitTask"
          },
          "existing_cluster_id" : {
            "example" : "0923-164208-meows279",
            "description" : "If existing_cluster_id, the ID of an existing cluster that is used for all runs of this task. When running tasks on an existing cluster, you may need to manually restart the cluster if it stops responding. We suggest running jobs on new clusters for greater reliability.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "jobs.RunTask" : {
        "properties" : {
          "dbt_task" : {
            "description" : "If dbt_task, indicates that this must execute a dbt task. It requires both Databricks SQL and the ability to use a serverless or a pro SQL warehouse.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.DbtTask"
          },
          "spark_python_task" : {
            "description" : "If spark_python_task, indicates that this job must run a Python file.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SparkPythonTask"
          },
          "pipeline_task" : {
            "description" : "If pipeline_task, indicates that this job must execute a Pipeline.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.PipelineTask"
          },
          "setup_duration" : {
            "example" : 0,
            "format" : "int64",
            "description" : "The time in milliseconds it took to set up the cluster. For runs that run on new clusters this is the cluster creation time, for runs that run on existing clusters this time should be very short. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `setup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.",
            "type" : "integer"
          },
          "start_time" : {
            "example" : 1625060460483,
            "format" : "int64",
            "description" : "The time at which this run was started in epoch milliseconds (milliseconds since 1/1/1970 UTC). This may not be the time when the job task starts executing, for example, if the job is scheduled to run on a new cluster, this is the time the cluster creation call is issued.",
            "type" : "integer"
          },
          "task_key" : {
            "example" : "Task_Key",
            "description" : "A unique name for the task. This field is used to refer to this task from other tasks.\nThis field is required and must be unique within its parent job.\nOn Update or Reset, this field is used to reference the tasks to be updated or reset.\nThe maximum length is 100 characters.",
            "type" : "string"
          },
          "state" : {
            "description" : "The result and lifecycle states of the run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.RunState"
          },
          "description" : {
            "example" : "This is the description for this task.",
            "description" : "An optional description for this task.\nThe maximum length is 4096 bytes.",
            "type" : "string"
          },
          "notebook_task" : {
            "description" : "If notebook_task, indicates that this job must run a notebook. This field may not be specified in conjunction with spark_jar_task.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.NotebookTask"
          },
          "python_wheel_task" : {
            "description" : "If python_wheel_task, indicates that this job must execute a PythonWheel.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.PythonWheelTask"
          },
          "end_time" : {
            "example" : 1625060863413,
            "format" : "int64",
            "description" : "The time at which this run ended in epoch milliseconds (milliseconds since 1/1/1970 UTC). This field is set to 0 if the job is still running.",
            "type" : "integer"
          },
          "depends_on" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.TaskDependencies"
          },
          "sql_task" : {
            "description" : "If sql_task, indicates that this job must execute a SQL.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-not-cloud" : "gcp",
            "$ref" : "#/components/schemas/jobs.SqlTask"
          },
          "new_cluster" : {
            "description" : "If new_cluster, a description of a new cluster that is created only for this task.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.BaseClusterInfo"
          },
          "run_id" : {
            "example" : 99887766,
            "format" : "int64",
            "description" : "The ID of the task run.",
            "type" : "integer"
          },
          "cluster_instance" : {
            "description" : "The cluster used for this run. If the run is specified to use a new cluster, this field is set once the Jobs service has requested a cluster for the run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.ClusterInstance"
          },
          "spark_jar_task" : {
            "description" : "If spark_jar_task, indicates that this job must run a JAR.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SparkJarTask"
          },
          "libraries" : {
            "description" : "An optional list of libraries to be installed on the cluster that executes the job. The default value is an empty list.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/libraries.Library"
            }
          },
          "spark_submit_task" : {
            "description" : "If spark_submit_task, indicates that this task must be launched by the spark submit script. This task can run only on new clusters",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SparkSubmitTask"
          },
          "attempt_number" : {
            "example" : 0,
            "format" : "int32",
            "description" : "The sequence number of this run attempt for a triggered job run. The initial attempt of a run has an attempt_number of 0\\. If the initial run attempt fails, and the job has a retry policy (`max_retries` \\> 0), subsequent runs are created with an `original_attempt_run_id` of the original attempts ID and an incrementing `attempt_number`. Runs are retried only until they succeed, and the maximum `attempt_number` is the same as the `max_retries` value for the job.",
            "type" : "integer"
          },
          "existing_cluster_id" : {
            "description" : "If existing_cluster_id, the ID of an existing cluster that is used for all runs of this job. When running jobs on an existing cluster, you may need to manually restart the cluster if it stops responding. We suggest running jobs on new clusters for greater reliability.",
            "type" : "string"
          },
          "git_source" : {
            "example" : {
              "git_branch" : "main",
              "git_provider" : "gitHub",
              "git_url" : "https://github.com/databricks/databricks-cli"
            },
            "description" : "An optional specification for a remote repository containing the notebooks used by this job's notebook tasks.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.GitSource"
          },
          "cleanup_duration" : {
            "example" : 0,
            "format" : "int64",
            "description" : "The time in milliseconds it took to terminate the cluster and clean up any associated artifacts. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the `cleanup_duration`. The `cleanup_duration` field is set to 0 for multitask job runs. The total duration of a multitask job run is the value of the `run_duration` field.",
            "type" : "integer"
          },
          "execution_duration" : {
            "example" : 0,
            "format" : "int64",
            "description" : "The time in milliseconds it took to execute the commands in the JAR or notebook until they  completed, failed, timed out, were cancelled, or encountered an unexpected error. The duration of a task run is the sum of the `setup_duration`, `execution_duration`, and the  `cleanup_duration`. The `execution_duration` field is set to 0 for multitask job runs. The total  duration of a multitask job run is the value of the `run_duration` field.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "jobs.RunType" : {
        "example" : "JOB_RUN",
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "JOB_RUN", "WORKFLOW_RUN", "SUBMIT_RUN" ],
        "x-databricks-enum-descriptions" : {
          "JOB_RUN" : "Normal job run. A run created with :method:jobs/runNow.",
          "WORKFLOW_RUN" : "Workflow run. A run created with [dbutils.notebook.run](/dev-tools/databricks-utils.html#dbutils-workflow).",
          "SUBMIT_RUN" : "Submit run. A run created with :method:jobs/submit."
        }
      },
      "jobs.SparkJarTask" : {
        "properties" : {
          "jar_uri" : {
            "description" : "Deprecated since 04/2016\\\\. Provide a `jar` through the `libraries` field instead. For an example, see :method:jobs/create.\n",
            "deprecated" : true,
            "type" : "string"
          },
          "main_class_name" : {
            "example" : "com.databricks.ComputeModels",
            "description" : "The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library.\n\nThe code must use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job fail.",
            "type" : "string"
          },
          "parameters" : {
            "example" : [ "--data", "dbfs:/path/to/data.json" ],
            "description" : "Parameters passed to the main method.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          }
        },
        "type" : "object"
      },
      "jobs.SparkPythonTask" : {
        "required" : [ "python_file" ],
        "properties" : {
          "parameters" : {
            "example" : [ "--data", "dbfs:/path/to/data.json" ],
            "description" : "Command line parameters passed to the Python file.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "python_file" : {
            "example" : "dbfs:/path/to/file.py",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "jobs.SparkSubmitTask" : {
        "properties" : {
          "parameters" : {
            "example" : [ "--class", "org.apache.spark.examples.SparkPi", "dbfs:/path/to/examples.jar", "10" ],
            "description" : "Command-line parameters passed to spark submit.\n\nUse [Task parameter variables](https://docs.databricks.com/jobs.html#parameter-variables) to set parameters containing information about job runs.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          }
        },
        "type" : "object"
      },
      "jobs.SqlAlertOutput" : {
        "properties" : {
          "sql_statements" : {
            "description" : "Information about SQL statements executed in the run.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.SqlStatementOutput"
            }
          },
          "output_link" : {
            "description" : "The link to find the output results.",
            "type" : "string"
          },
          "alert_state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SqlAlertState"
          },
          "query_text" : {
            "description" : "The text of the SQL query. Can Run permission of the SQL query associated with the SQL alert is required to view this field.",
            "type" : "string"
          },
          "warehouse_id" : {
            "description" : "The canonical identifier of the SQL warehouse.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.SqlAlertState" : {
        "description" : "The state of the SQL alert.\n\n* UNKNOWN: alert yet to be evaluated\n* OK: alert evaluated and did not fulfill trigger conditions\n* TRIGGERED: alert evaluated and fulfilled trigger conditions\n",
        "type" : "string",
        "enum" : [ "UNKNOWN", "OK", "TRIGGERED" ]
      },
      "jobs.SqlDashboardOutput" : {
        "properties" : {
          "warehouse_id" : {
            "description" : "The canonical identifier of the SQL warehouse.",
            "type" : "string"
          },
          "widgets" : {
            "description" : "Widgets executed in the run. Only SQL query based widgets are listed.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SqlDashboardWidgetOutput"
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.SqlDashboardWidgetOutput" : {
        "properties" : {
          "start_time" : {
            "format" : "int64",
            "description" : "Time (in epoch milliseconds) when execution of the SQL widget starts.",
            "type" : "integer"
          },
          "widget_id" : {
            "description" : "The canonical identifier of the SQL widget.",
            "type" : "string"
          },
          "widget_title" : {
            "description" : "The title of the SQL widget.",
            "type" : "string"
          },
          "output_link" : {
            "description" : "The link to find the output results.",
            "type" : "string"
          },
          "end_time" : {
            "format" : "int64",
            "description" : "Time (in epoch milliseconds) when execution of the SQL widget ends.",
            "type" : "integer"
          },
          "error" : {
            "description" : "The information about the error when execution fails.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SqlOutputError"
          },
          "status" : {
            "description" : "The execution status of the SQL widget.",
            "type" : "string",
            "enum" : [ "PENDING", "RUNNING", "SUCCESS", "FAILED", "CANCELLED" ]
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.SqlOutput" : {
        "properties" : {
          "alert_output" : {
            "description" : "The output of a SQL alert task, if available.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SqlAlertOutput"
          },
          "dashboard_output" : {
            "description" : "The output of a SQL dashboard task, if available.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SqlDashboardOutput"
          },
          "query_output" : {
            "description" : "The output of a SQL query task, if available.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SqlQueryOutput"
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.SqlOutputError" : {
        "properties" : {
          "message" : {
            "description" : "The error message when execution fails.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.SqlQueryOutput" : {
        "properties" : {
          "output_link" : {
            "description" : "The link to find the output results.",
            "type" : "string"
          },
          "query_text" : {
            "description" : "The text of the SQL query. Can Run permission of the SQL query is required to view this field.",
            "type" : "string"
          },
          "sql_statements" : {
            "description" : "Information about SQL statements executed in the run.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.SqlStatementOutput"
            }
          },
          "warehouse_id" : {
            "description" : "The canonical identifier of the SQL warehouse.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.SqlStatementOutput" : {
        "properties" : {
          "lookup_key" : {
            "description" : "A key that can be used to look up query details.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.SqlTask" : {
        "required" : [ "warehouse_id" ],
        "properties" : {
          "alert" : {
            "description" : "If alert, indicates that this job must refresh a SQL alert.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SqlTaskAlert"
          },
          "dashboard" : {
            "description" : "If dashboard, indicates that this job must refresh a SQL dashboard.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SqlTaskDashboard"
          },
          "query" : {
            "description" : "If query, indicates that this job must execute a SQL query.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.SqlTaskQuery"
          },
          "parameters" : {
            "example" : {
              "age" : "35",
              "name" : "John Doe"
            },
            "properties" : { },
            "description" : "Parameters to be used for each run of this job. The SQL alert task does not support custom parameters.",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "warehouse_id" : {
            "description" : "The canonical identifier of the SQL warehouse. Only serverless and pro SQL warehouses are supported.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.SqlTaskAlert" : {
        "required" : [ "alert_id" ],
        "properties" : {
          "alert_id" : {
            "description" : "The canonical identifier of the SQL alert.",
            "type" : "string"
          },
          "pause_subscriptions" : {
            "description" : "If true, the alert notifications are not sent to subscribers.",
            "type" : "boolean"
          },
          "subscriptions" : {
            "description" : "If specified, alert notifications are sent to subscribers.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.SqlTaskSubscription"
            }
          }
        },
        "type" : "object"
      },
      "jobs.SqlTaskDashboard" : {
        "required" : [ "dashboard_id" ],
        "properties" : {
          "custom_subject" : {
            "description" : "Subject of the email sent to subscribers of this task.",
            "type" : "string"
          },
          "dashboard_id" : {
            "description" : "The canonical identifier of the SQL dashboard.",
            "type" : "string"
          },
          "pause_subscriptions" : {
            "description" : "If true, the dashboard snapshot is not taken, and emails are not sent to subscribers.",
            "type" : "boolean"
          },
          "subscriptions" : {
            "description" : "If specified, dashboard snapshots are sent to subscriptions.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.SqlTaskSubscription"
            }
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.SqlTaskQuery" : {
        "required" : [ "query_id" ],
        "properties" : {
          "query_id" : {
            "description" : "The canonical identifier of the SQL query.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.SqlTaskSubscription" : {
        "properties" : {
          "destination_id" : {
            "description" : "The canonical identifier of the destination to receive email notification.",
            "type" : "string"
          },
          "user_name" : {
            "description" : "The user name to receive the subscription email.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-not-cloud" : "gcp"
      },
      "jobs.SubmitRun" : {
        "properties" : {
          "idempotency_token" : {
            "example" : "8f018174-4792-40d5-bcbc-3e6a527352c8",
            "description" : "An optional token that can be used to guarantee the idempotency of job run requests. If a run with the provided token already exists,\nthe request does not create a new run but returns the ID of the existing run instead. If a run with the provided token is deleted,\nan error is returned.\n\nIf you specify the idempotency token, upon failure you can retry until the request succeeds. Databricks guarantees that exactly\none run is launched with that idempotency token.\n\nThis token must have at most 64 characters.\n\nFor more information, see [How to ensure idempotency for jobs](\nhttps://kb.databricks.com/jobs/jobs-idempotency.html).",
            "type" : "string"
          },
          "timeout_seconds" : {
            "example" : 86400,
            "format" : "int32",
            "description" : "An optional timeout applied to each run of this job. The default behavior is to have no timeout.",
            "type" : "integer"
          },
          "tasks" : {
            "example" : [ {
              "task_key" : "Sessionize",
              "description" : "Extracts session data from events",
              "depends_on" : [ ],
              "timeout_seconds" : 86400,
              "spark_jar_task" : {
                "main_class_name" : "com.databricks.Sessionize",
                "parameters" : [ "--data", "dbfs:/path/to/data.json" ]
              },
              "libraries" : [ {
                "jar" : "dbfs:/mnt/databricks/Sessionize.jar"
              } ],
              "existing_cluster_id" : "0923-164208-meows279"
            }, {
              "task_key" : "Orders_Ingest",
              "description" : "Ingests order data",
              "depends_on" : [ ],
              "timeout_seconds" : 86400,
              "spark_jar_task" : {
                "main_class_name" : "com.databricks.OrdersIngest",
                "parameters" : [ "--data", "dbfs:/path/to/order-data.json" ]
              },
              "libraries" : [ {
                "jar" : "dbfs:/mnt/databricks/OrderIngest.jar"
              } ],
              "existing_cluster_id" : "0923-164208-meows279"
            }, {
              "task_key" : "Match",
              "description" : "Matches orders with user sessions",
              "notebook_task" : {
                "base_parameters" : {
                  "age" : "35",
                  "name" : "John Doe"
                },
                "notebook_path" : "/Users/user.name@databricks.com/Match"
              },
              "depends_on" : [ {
                "task_key" : "Orders_Ingest"
              }, {
                "task_key" : "Sessionize"
              } ],
              "new_cluster" : {
                "autoscale" : {
                  "max_workers" : 16,
                  "min_workers" : 2
                },
                "node_type_id" : null,
                "spark_conf" : {
                  "spark.speculation" : true
                },
                "spark_version" : "7.3.x-scala2.12"
              },
              "timeout_seconds" : 86400
            } ],
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/jobs.RunSubmitTaskSettings"
            }
          },
          "run_name" : {
            "example" : "A multitask job run",
            "description" : "An optional name for the run. The default value is `Untitled`.",
            "type" : "string"
          },
          "webhook_notifications" : {
            "description" : "A collection of system notification IDs to notify when the run begins or completes. The default behavior is to not send any system notifications.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.JobWebhookNotifications"
          },
          "access_control_list" : {
            "description" : "List of permissions to set on the job.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.AccessControlRequest"
            }
          },
          "git_source" : {
            "description" : "An optional specification for a remote repository containing the notebooks used by this job's notebook tasks.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-preview" : "PUBLIC",
            "$ref" : "#/components/schemas/jobs.GitSource"
          }
        },
        "type" : "object"
      },
      "jobs.SubmitRunResponse" : {
        "properties" : {
          "run_id" : {
            "example" : 455644833,
            "format" : "int64",
            "description" : "The canonical identifier for the newly submitted run.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "jobs.TaskDependencies" : {
        "example" : [ {
          "task_key" : "Previous_Task_Key"
        }, {
          "task_key" : "Other_Task_Key"
        } ],
        "description" : "An optional array of objects specifying the dependency graph of the task. All tasks specified in this field must complete successfully before executing this task.\nThe key is `task_key`, and the value is the name assigned to the dependent task.\nThis field is required when a job consists of more than one task.",
        "type" : "array",
        "items" : {
          "properties" : {
            "task_key" : {
              "type" : "string"
            }
          },
          "type" : "object"
        }
      },
      "jobs.TriggerType" : {
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "PERIODIC", "ONE_TIME", "RETRY" ],
        "x-databricks-enum-descriptions" : {
          "PERIODIC" : "Schedules that periodically trigger runs, such as a cron scheduler.",
          "ONE_TIME" : "One time triggers that fire a single run. This occurs you triggered a single run on demand through the UI or the API.",
          "RETRY" : "Indicates a run that is triggered as a retry of a previously failed run. This occurs when you request to re-run the job in case of failures."
        }
      },
      "jobs.UpdateJob" : {
        "required" : [ "job_id" ],
        "properties" : {
          "fields_to_remove" : {
            "example" : [ "libraries", "schedule" ],
            "description" : "Remove top-level fields in the job settings. Removing nested fields is not supported. This field is optional.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "job_id" : {
            "example" : 11223344,
            "format" : "int64",
            "description" : "The canonical identifier of the job to update. This field is required.",
            "type" : "integer"
          },
          "new_settings" : {
            "description" : "The new settings for the job. Any top-level fields specified in `new_settings` are completely replaced. Partially updating nested fields is not supported.\n\nChanges to the field `JobSettings.timeout_seconds` are applied to active runs. Changes to other fields are applied to future runs only.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.JobSettings"
          }
        },
        "type" : "object"
      },
      "jobs.ViewItem" : {
        "properties" : {
          "content" : {
            "description" : "Content of the view.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of the view item. In the case of code view, it would be the notebooks name. In the case of dashboard view, it would be the dashboards name.",
            "type" : "string"
          },
          "type" : {
            "description" : "Type of the view item.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/jobs.ViewType"
          }
        },
        "type" : "object"
      },
      "jobs.ViewType" : {
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "NOTEBOOK", "DASHBOARD" ],
        "x-databricks-enum-descriptions" : {
          "NOTEBOOK" : "Notebook view item.",
          "DASHBOARD" : "Dashboard view item."
        }
      },
      "jobs.ViewsToExport" : {
        "default" : "CODE",
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "CODE", "DASHBOARDS", "ALL" ],
        "x-databricks-enum-descriptions" : {
          "CODE" : "Code view of the notebook.",
          "DASHBOARDS" : "All dashboard views of the notebook.",
          "ALL" : "All views of the notebook."
        }
      },
      "libraries.ClusterLibraryStatuses" : {
        "properties" : {
          "cluster_id" : {
            "description" : "Unique identifier for the cluster.",
            "type" : "string"
          },
          "library_statuses" : {
            "description" : "Status of all libraries on the cluster.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/libraries.LibraryFullStatus"
            }
          }
        },
        "type" : "object"
      },
      "libraries.InstallLibraries" : {
        "required" : [ "cluster_id", "libraries" ],
        "properties" : {
          "cluster_id" : {
            "description" : "Unique identifier for the cluster on which to install these libraries.",
            "type" : "string"
          },
          "libraries" : {
            "description" : "The libraries to install.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/libraries.Library"
            }
          }
        },
        "type" : "object"
      },
      "libraries.InstallLibrariesResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "libraries.Library" : {
        "properties" : {
          "egg" : {
            "description" : "URI of the egg to be installed. Currently only DBFS and S3 URIs are supported.\nFor example: `{ \"egg\": \"dbfs:/my/egg\" }` or\n`{ \"egg\": \"s3://my-bucket/egg\" }`.\nIf S3 is used, please make sure the cluster has read access on the library. You may need to\nlaunch the cluster with an IAM role to access the S3 URI.",
            "type" : "string"
          },
          "pypi" : {
            "description" : "Specification of a PyPi library to be installed. For example:\n`{ \"package\": \"simplejson\" }`",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/libraries.PythonPyPiLibrary"
          },
          "maven" : {
            "description" : "Specification of a maven library to be installed. For example:\n`{ \"coordinates\": \"org.jsoup:jsoup:1.7.2\" }`",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/libraries.MavenLibrary"
          },
          "cran" : {
            "description" : "Specification of a CRAN library to be installed as part of the library",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/libraries.RCranLibrary"
          },
          "whl" : {
            "description" : "URI of the wheel to be installed.\nFor example: `{ \"whl\": \"dbfs:/my/whl\" }` or `{ \"whl\": \"s3://my-bucket/whl\" }`.\nIf S3 is used, please make sure the cluster has read access on the library. You may need to\nlaunch the cluster with an IAM role to access the S3 URI.",
            "type" : "string"
          },
          "jar" : {
            "description" : "URI of the jar to be installed. Currently only DBFS and S3 URIs are supported.\nFor example: `{ \"jar\": \"dbfs:/mnt/databricks/library.jar\" }` or\n`{ \"jar\": \"s3://my-bucket/library.jar\" }`.\nIf S3 is used, please make sure the cluster has read access on the library. You may need to\nlaunch the cluster with an IAM role to access the S3 URI.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "libraries.LibraryFullStatus" : {
        "properties" : {
          "is_library_for_all_clusters" : {
            "description" : "Whether the library was set to be installed on all clusters via the libraries UI.",
            "type" : "boolean"
          },
          "library" : {
            "description" : "Unique identifier for the library.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/libraries.Library"
          },
          "messages" : {
            "description" : "All the info and warning messages that have occurred so far for this library.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "status" : {
            "description" : "Status of installing the library on the cluster.",
            "type" : "string",
            "enum" : [ "PENDING", "RESOLVING", "INSTALLING", "INSTALLED", "FAILED", "UNINSTALL_ON_RESTART", "SKIPPED" ]
          }
        },
        "type" : "object"
      },
      "libraries.ListAllClusterLibraryStatusesResponse" : {
        "properties" : {
          "statuses" : {
            "description" : "A list of cluster statuses.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/libraries.ClusterLibraryStatuses"
            }
          }
        },
        "type" : "object"
      },
      "libraries.MavenLibrary" : {
        "required" : [ "coordinates" ],
        "properties" : {
          "coordinates" : {
            "description" : "Gradle-style maven coordinates. For example: \"org.jsoup:jsoup:1.7.2\".",
            "type" : "string"
          },
          "exclusions" : {
            "description" : "List of dependences to exclude. For example: `[\"slf4j:slf4j\", \"*:hadoop-client\"]`.\n\nMaven dependency exclusions:\nhttps://maven.apache.org/guides/introduction/introduction-to-optional-and-excludes-dependencies.html.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "repo" : {
            "description" : "Maven repo to install the Maven package from. If omitted, both Maven Central Repository\nand Spark Packages are searched.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "libraries.PythonPyPiLibrary" : {
        "required" : [ "package" ],
        "properties" : {
          "package" : {
            "description" : "The name of the pypi package to install. An optional exact version specification is also\nsupported. Examples: \"simplejson\" and \"simplejson==3.8.0\".",
            "type" : "string"
          },
          "repo" : {
            "description" : "The repository where the package can be found. If not specified, the default pip index is\nused.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "libraries.RCranLibrary" : {
        "required" : [ "package" ],
        "properties" : {
          "package" : {
            "description" : "The name of the CRAN package to install.",
            "type" : "string"
          },
          "repo" : {
            "description" : "The repository where the package can be found. If not specified, the default CRAN repo is used.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "libraries.UninstallLibraries" : {
        "required" : [ "cluster_id", "libraries" ],
        "properties" : {
          "cluster_id" : {
            "description" : "Unique identifier for the cluster on which to uninstall these libraries.",
            "type" : "string"
          },
          "libraries" : {
            "description" : "The libraries to uninstall.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/libraries.Library"
            }
          }
        },
        "type" : "object"
      },
      "libraries.UninstallLibrariesResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.Activity" : {
        "properties" : {
          "activity_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.ActivityType"
          },
          "user_id" : {
            "example" : "jane.doe@example.com",
            "format" : "email",
            "description" : "The username of the user that created the object.",
            "type" : "string"
          },
          "creation_timestamp" : {
            "example" : 1594437249910,
            "format" : "int64",
            "description" : "Creation time of the object, as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "to_stage" : {
            "description" : "Target stage of the transition (if the activity is stage transition related). Valid values are:\n\n* `None`: The initial stage of a model version.\n\n* `Staging`: Staging or pre-production stage.\n\n* `Production`: Production stage.\n\n* `Archived`: Archived stage.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.to_stage"
          },
          "id" : {
            "example" : "6fc74c92704341aaa49e74dcc6031057",
            "format" : "uuid",
            "description" : "Unique identifier for the object.",
            "type" : "string"
          },
          "comment" : {
            "example" : "This version is great!",
            "description" : "User-provided comment associated with the activity.",
            "type" : "string"
          },
          "system_comment" : {
            "description" : "Comment made by system, for example explaining an activity of type `SYSTEM_TRANSITION`. It usually describes a side effect, such as a version being archived as part of another version's stage transition, and may not be returned for some activity types.",
            "type" : "string"
          },
          "last_updated_timestamp" : {
            "example" : 1594437549910,
            "format" : "int64",
            "description" : "Time of the object at last update, as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "from_stage" : {
            "description" : "Source stage of the transition (if the activity is stage transition related). Valid values are:\n\n* `None`: The initial stage of a model version.\n\n* `Staging`: Staging or pre-production stage.\n\n* `Production`: Production stage.\n\n* `Archived`: Archived stage.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.stage"
          }
        },
        "description" : "Activity recorded for the action.",
        "type" : "object"
      },
      "mlflow.ActivityAction" : {
        "example" : "CANCEL_TRANSITION_REQUEST",
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "APPROVE_TRANSITION_REQUEST", "REJECT_TRANSITION_REQUEST", "CANCEL_TRANSITION_REQUEST" ],
        "x-databricks-enum-descriptions" : {
          "APPROVE_TRANSITION_REQUEST" : "Approve a transition request",
          "REJECT_TRANSITION_REQUEST" : "Reject a transition request",
          "CANCEL_TRANSITION_REQUEST" : "Cancel (delete) a transition request"
        }
      },
      "mlflow.ActivityType" : {
        "example" : "REQUESTED_TRANSITION",
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "APPLIED_TRANSITION", "REQUESTED_TRANSITION", "CANCELLED_REQUEST", "APPROVED_REQUEST", "REJECTED_REQUEST", "NEW_COMMENT", "SYSTEM_TRANSITION" ],
        "x-databricks-enum-descriptions" : {
          "CANCELLED_REQUEST" : "User cancelled an existing transition request.",
          "APPLIED_TRANSITION" : "User applied the corresponding stage transition.",
          "REJECTED_REQUEST" : "User rejected the coressponding stage transition.",
          "APPROVED_REQUEST" : "User approved the corresponding stage transition.",
          "REQUESTED_TRANSITION" : "User requested the corresponding stage transition.",
          "SYSTEM_TRANSITION" : "For events performed as a side effect, such as archiving existing model versions in a stage."
        }
      },
      "mlflow.ApproveTransitionRequest" : {
        "required" : [ "name", "version", "stage", "archive_existing_versions" ],
        "properties" : {
          "stage" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.to_stage"
          },
          "name" : {
            "example" : "search_ads_model",
            "description" : "Name of the model.",
            "type" : "string"
          },
          "archive_existing_versions" : {
            "example" : true,
            "description" : "Specifies whether to archive all current model versions in the target stage.",
            "type" : "boolean"
          },
          "version" : {
            "example" : "1",
            "description" : "Version of the model.",
            "type" : "string"
          },
          "comment" : {
            "example" : "This version is great!",
            "description" : "User-provided comment on the action.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.CommentActivityAction" : {
        "example" : "EDIT_COMMENT",
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "EDIT_COMMENT", "DELETE_COMMENT" ],
        "x-databricks-enum-descriptions" : {
          "EDIT_COMMENT" : "Edit the comment",
          "DELETE_COMMENT" : "Delete the comment"
        }
      },
      "mlflow.CommentObject" : {
        "properties" : {
          "available_actions" : {
            "description" : "Array of actions on the activity allowed for the current viewer.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.CommentActivityAction"
            }
          },
          "user_id" : {
            "example" : "jane.doe@example.com",
            "format" : "email",
            "description" : "The username of the user that created the object.",
            "type" : "string"
          },
          "creation_timestamp" : {
            "example" : 1594437249910,
            "format" : "int64",
            "description" : "Creation time of the object, as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "comment" : {
            "example" : "This version is great!",
            "description" : "User-provided comment on the action.",
            "type" : "string"
          },
          "last_updated_timestamp" : {
            "example" : 1594437549910,
            "format" : "int64",
            "description" : "Time of the object at last update, as a Unix timestamp in milliseconds.",
            "type" : "integer"
          }
        },
        "description" : "Comment details.",
        "type" : "object"
      },
      "mlflow.CreateComment" : {
        "required" : [ "name", "version", "comment" ],
        "properties" : {
          "comment" : {
            "example" : "This version is great!",
            "description" : "User-provided comment on the action.",
            "type" : "string"
          },
          "name" : {
            "example" : "search_ads_model",
            "description" : "Name of the model.",
            "type" : "string"
          },
          "version" : {
            "example" : "1",
            "description" : "Version of the model.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.CreateExperiment" : {
        "required" : [ "name" ],
        "properties" : {
          "artifact_location" : {
            "description" : "Location where all artifacts for the experiment are stored.\nIf not provided, the remote server will select an appropriate default.",
            "type" : "string"
          },
          "name" : {
            "description" : "Experiment name.",
            "type" : "string"
          },
          "tags" : {
            "description" : "A collection of tags to set on the experiment. Maximum tag size and number of tags per request\ndepends on the storage backend. All storage backends are guaranteed to support tag keys up\nto 250 bytes in size and tag values up to 5000 bytes in size. All storage backends are also\nguaranteed to support up to 20 tags per request.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.ExperimentTag"
            }
          }
        },
        "type" : "object"
      },
      "mlflow.CreateExperimentResponse" : {
        "properties" : {
          "experiment_id" : {
            "description" : "Unique identifier for the experiment.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.CreateModelVersionRequest" : {
        "required" : [ "name", "source" ],
        "properties" : {
          "name" : {
            "description" : "Register model under this name",
            "type" : "string"
          },
          "source" : {
            "description" : "URI indicating the location of the model artifacts.",
            "type" : "string"
          },
          "description" : {
            "description" : "Optional description for model version.",
            "type" : "string"
          },
          "tags" : {
            "description" : "Additional metadata for model version.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.ModelVersionTag"
            }
          },
          "run_id" : {
            "description" : "MLflow run ID for correlation, if `source` was generated by an experiment run in\nMLflow tracking server",
            "type" : "string"
          },
          "run_link" : {
            "description" : "MLflow run link - this is the exact link of the run that generated this model version,\npotentially hosted at another instance of MLflow.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.CreateModelVersionResponse" : {
        "properties" : {
          "model_version" : {
            "description" : "Return new version number generated for this model in registry.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.ModelVersion"
          }
        },
        "type" : "object"
      },
      "mlflow.CreateRegisteredModelRequest" : {
        "required" : [ "name" ],
        "properties" : {
          "description" : {
            "description" : "Optional description for registered model.",
            "type" : "string"
          },
          "name" : {
            "description" : "Register models under this name",
            "type" : "string"
          },
          "tags" : {
            "description" : "Additional metadata for registered model.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.RegisteredModelTag"
            }
          }
        },
        "type" : "object"
      },
      "mlflow.CreateRegisteredModelResponse" : {
        "properties" : {
          "registered_model" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RegisteredModel"
          }
        },
        "type" : "object"
      },
      "mlflow.CreateRegistryWebhook" : {
        "required" : [ "events" ],
        "properties" : {
          "job_spec" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.JobSpec"
          },
          "description" : {
            "example" : "Webhook for comment creation",
            "description" : "User-specified description for the webhook.",
            "type" : "string"
          },
          "http_url_spec" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.HttpUrlSpec"
          },
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RegistryWebhookStatus"
          },
          "model_name" : {
            "example" : "registered-model-1",
            "description" : "Name of the model whose events would trigger this webhook.",
            "type" : "string"
          },
          "events" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RegistryWebhookEvents"
          }
        },
        "type" : "object"
      },
      "mlflow.CreateRun" : {
        "properties" : {
          "experiment_id" : {
            "description" : "ID of the associated experiment.",
            "type" : "string"
          },
          "start_time" : {
            "format" : "int64",
            "description" : "Unix timestamp in milliseconds of when the run started.",
            "type" : "integer"
          },
          "tags" : {
            "description" : "Additional metadata for run.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.RunTag"
            }
          },
          "user_id" : {
            "description" : "ID of the user executing the run.\nThis field is deprecated as of MLflow 1.0, and will be removed in a future\nMLflow release. Use 'mlflow.user' tag instead.",
            "deprecated" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.CreateRunResponse" : {
        "properties" : {
          "run" : {
            "description" : "The newly created run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.Run"
          }
        },
        "type" : "object"
      },
      "mlflow.CreateTransitionRequest" : {
        "required" : [ "name", "version", "stage" ],
        "properties" : {
          "comment" : {
            "example" : "This version is great!",
            "description" : "User-provided comment on the action.",
            "type" : "string"
          },
          "name" : {
            "example" : "search_ads_model",
            "description" : "Name of the model.",
            "type" : "string"
          },
          "stage" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.to_stage"
          },
          "version" : {
            "example" : "1",
            "description" : "Version of the model.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.DeleteExperiment" : {
        "required" : [ "experiment_id" ],
        "properties" : {
          "experiment_id" : {
            "description" : "ID of the associated experiment.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.DeleteExperimentResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.DeleteRun" : {
        "required" : [ "run_id" ],
        "properties" : {
          "run_id" : {
            "description" : "ID of the run to delete.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.DeleteRunResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.DeleteTag" : {
        "required" : [ "run_id", "key" ],
        "properties" : {
          "key" : {
            "description" : "Name of the tag. Maximum size is 255 bytes. Must be provided.",
            "type" : "string"
          },
          "run_id" : {
            "description" : "ID of the run that the tag was logged under. Must be provided.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.DeleteTagResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.Experiment" : {
        "properties" : {
          "artifact_location" : {
            "description" : "Location where artifacts for the experiment are stored.",
            "type" : "string"
          },
          "name" : {
            "description" : "Human readable name that identifies the experiment.",
            "type" : "string"
          },
          "creation_time" : {
            "format" : "int64",
            "description" : "Creation time",
            "type" : "integer"
          },
          "tags" : {
            "description" : "Tags: Additional metadata key-value pairs.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.ExperimentTag"
            }
          },
          "last_update_time" : {
            "format" : "int64",
            "description" : "Last update time",
            "type" : "integer"
          },
          "lifecycle_stage" : {
            "description" : "Current life cycle stage of the experiment: \"active\" or \"deleted\".\nDeleted experiments are not returned by APIs.",
            "type" : "string"
          },
          "experiment_id" : {
            "description" : "Unique identifier for the experiment.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.ExperimentTag" : {
        "properties" : {
          "key" : {
            "description" : "The tag key.",
            "type" : "string"
          },
          "value" : {
            "description" : "The tag value.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.FileInfo" : {
        "properties" : {
          "file_size" : {
            "format" : "int64",
            "description" : "Size in bytes. Unset for directories.",
            "type" : "integer"
          },
          "is_dir" : {
            "description" : "Whether the path is a directory.",
            "type" : "boolean"
          },
          "path" : {
            "description" : "Path relative to the root artifact directory run.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.GetExperimentByNameResponse" : {
        "properties" : {
          "experiment" : {
            "description" : "Experiment details.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.Experiment"
          }
        },
        "type" : "object"
      },
      "mlflow.GetLatestVersionsRequest" : {
        "required" : [ "name" ],
        "properties" : {
          "name" : {
            "description" : "Registered model unique name identifier.",
            "type" : "string"
          },
          "stages" : {
            "description" : "List of stages.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          }
        },
        "type" : "object"
      },
      "mlflow.GetLatestVersionsResponse" : {
        "properties" : {
          "model_versions" : {
            "description" : "Latest version models for each requests stage. Only return models with current `READY` status.\nIf no `stages` provided, returns the latest version for each stage, including `\"None\"`.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.ModelVersion"
            }
          }
        },
        "type" : "object"
      },
      "mlflow.GetMetricHistoryResponse" : {
        "properties" : {
          "metrics" : {
            "description" : "All logged values for this metric.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Metric"
            }
          },
          "next_page_token" : {
            "description" : "Token that can be used to retrieve the next page of metric history results",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.GetModelVersionDownloadUriResponse" : {
        "properties" : {
          "artifact_uri" : {
            "description" : "URI corresponding to where artifacts for this model version are stored.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.GetModelVersionResponse" : {
        "properties" : {
          "model_version" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.ModelVersion"
          }
        },
        "type" : "object"
      },
      "mlflow.GetRegisteredModelResponse" : {
        "properties" : {
          "registered_model" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RegisteredModel"
          }
        },
        "type" : "object"
      },
      "mlflow.GetRunResponse" : {
        "properties" : {
          "run" : {
            "description" : "Run metadata (name, start time, etc) and data (metrics, params, and tags).",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.Run"
          }
        },
        "type" : "object"
      },
      "mlflow.HttpUrlSpec" : {
        "required" : [ "url" ],
        "properties" : {
          "authorization" : {
            "example" : "Bearer <access_token>",
            "description" : "Value of the authorization header that should be sent in the request sent by the wehbook. It should be of the form `\"<auth type> <credentials>\"`. If set to an empty string, no authorization header will be included in the request.",
            "type" : "string"
          },
          "enable_ssl_verification" : {
            "description" : "Enable/disable SSL certificate validation. Default is true. For self-signed certificates, this field must be false AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host.",
            "type" : "boolean"
          },
          "secret" : {
            "example" : "anyRandomString",
            "description" : "Shared secret required for HMAC encoding payload. The HMAC-encoded payload will be sent in the header as: { \"X-Databricks-Signature\": $encoded_payload }.",
            "type" : "string"
          },
          "url" : {
            "example" : "https://hooks.slack.com/services/...",
            "description" : "External HTTPS URL called on event trigger (by using a POST request).",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.HttpUrlSpecWithoutSecret" : {
        "properties" : {
          "enable_ssl_verification" : {
            "description" : "Enable/disable SSL certificate validation. Default is true. For self-signed certificates, this field must be false AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host.",
            "type" : "boolean"
          },
          "url" : {
            "example" : "https://hooks.slack.com/services/...",
            "description" : "External HTTPS URL called on event trigger (by using a POST request).",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.JobSpec" : {
        "required" : [ "job_id", "access_token" ],
        "properties" : {
          "access_token" : {
            "example" : "dapi12345678935845824",
            "description" : "The personal access token used to authorize webhook's job runs.",
            "type" : "string"
          },
          "job_id" : {
            "example" : "1",
            "description" : "ID of the job that the webhook runs.",
            "type" : "string"
          },
          "workspace_url" : {
            "description" : "URL of the workspace containing the job that this webhook runs. If not specified, the jobs workspace URL is assumed to be the same as the workspace where the webhook is created.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.JobSpecWithoutSecret" : {
        "properties" : {
          "job_id" : {
            "example" : "1",
            "description" : "ID of the job that the webhook runs.",
            "type" : "string"
          },
          "workspace_url" : {
            "description" : "URL of the workspace containing the job that this webhook runs. Defaults to the workspace URL in which the webhook is created. If not specified, the jobs workspace is assumed to be the same as the webhooks.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.ListArtifactsResponse" : {
        "properties" : {
          "files" : {
            "description" : "File location and metadata for artifacts.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.FileInfo"
            }
          },
          "next_page_token" : {
            "description" : "Token that can be used to retrieve the next page of artifact results",
            "type" : "string"
          },
          "root_uri" : {
            "description" : "Root artifact directory for the run.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.ListExperimentsResponse" : {
        "properties" : {
          "experiments" : {
            "description" : "Paginated Experiments beginning with the first item on the requested page.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Experiment"
            }
          },
          "next_page_token" : {
            "description" : "Token that can be used to retrieve the next page of experiments.\nEmpty token means no more experiment is available for retrieval.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.ListRegisteredModelsResponse" : {
        "properties" : {
          "next_page_token" : {
            "description" : "Pagination token to request next page of models for the same query.",
            "type" : "string"
          },
          "registered_models" : {
            "description" : "",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.RegisteredModel"
            }
          }
        },
        "type" : "object"
      },
      "mlflow.ListRegistryWebhooks" : {
        "properties" : {
          "next_page_token" : {
            "description" : "Token that can be used to retrieve the next page of artifact results",
            "type" : "string"
          },
          "webhooks" : {
            "description" : "Array of registry webhooks.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.RegistryWebhook"
            }
          }
        },
        "type" : "object"
      },
      "mlflow.LogBatch" : {
        "example" : {
          "metrics" : [ {
            "key" : "mae",
            "timestamp" : 1552550804,
            "value" : 2.5
          }, {
            "key" : "rmse",
            "timestamp" : 1552550804,
            "value" : 2.7
          } ],
          "params" : [ {
            "key" : "model_class",
            "value" : "LogisticRegression"
          } ],
          "run_id" : "2a14ed5c6a87499199e0106c3501eab8"
        },
        "properties" : {
          "metrics" : {
            "description" : "Metrics to log. A single request can contain up to 1000 metrics, and up to 1000\nmetrics, params, and tags in total.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Metric"
            }
          },
          "params" : {
            "description" : "Params to log. A single request can contain up to 100 params, and up to 1000\nmetrics, params, and tags in total.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Param"
            }
          },
          "run_id" : {
            "description" : "ID of the run to log under",
            "type" : "string"
          },
          "tags" : {
            "description" : "Tags to log. A single request can contain up to 100 tags, and up to 1000\nmetrics, params, and tags in total.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.RunTag"
            }
          }
        },
        "type" : "object"
      },
      "mlflow.LogBatchResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.LogMetric" : {
        "required" : [ "key", "value", "timestamp" ],
        "properties" : {
          "step" : {
            "default" : "0",
            "format" : "int64",
            "description" : "Step at which to log the metric",
            "type" : "integer"
          },
          "run_uuid" : {
            "description" : "[Deprecated, use run_id instead] ID of the run under which to log the metric. This field will\nbe removed in a future MLflow version.",
            "deprecated" : true,
            "type" : "string"
          },
          "timestamp" : {
            "format" : "int64",
            "description" : "Unix timestamp in milliseconds at the time metric was logged.",
            "type" : "integer"
          },
          "key" : {
            "description" : "Name of the metric.",
            "type" : "string"
          },
          "run_id" : {
            "description" : "ID of the run under which to log the metric. Must be provided.",
            "type" : "string"
          },
          "value" : {
            "format" : "double",
            "description" : "Double value of the metric being logged.",
            "type" : "number"
          }
        },
        "type" : "object"
      },
      "mlflow.LogMetricResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.LogModel" : {
        "properties" : {
          "model_json" : {
            "description" : "MLmodel file in json format.",
            "type" : "string"
          },
          "run_id" : {
            "description" : "ID of the run to log under",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.LogModelResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.LogParam" : {
        "required" : [ "key", "value" ],
        "properties" : {
          "key" : {
            "description" : "Name of the param. Maximum size is 255 bytes.",
            "type" : "string"
          },
          "run_id" : {
            "description" : "ID of the run under which to log the param. Must be provided.",
            "type" : "string"
          },
          "run_uuid" : {
            "description" : "[Deprecated, use run_id instead] ID of the run under which to log the param. This field will\nbe removed in a future MLflow version.",
            "deprecated" : true,
            "type" : "string"
          },
          "value" : {
            "description" : "String value of the param being logged. Maximum size is 500 bytes.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.LogParamResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.Metric" : {
        "properties" : {
          "key" : {
            "description" : "Key identifying this metric.",
            "type" : "string"
          },
          "step" : {
            "default" : "0",
            "format" : "int64",
            "description" : "Step at which to log the metric.",
            "type" : "integer"
          },
          "timestamp" : {
            "format" : "int64",
            "description" : "The timestamp at which this metric was recorded.",
            "type" : "integer"
          },
          "value" : {
            "format" : "double",
            "description" : "Value associated with this metric.",
            "type" : "number"
          }
        },
        "type" : "object"
      },
      "mlflow.ModelVersion" : {
        "properties" : {
          "name" : {
            "description" : "Unique name of the model",
            "type" : "string"
          },
          "source" : {
            "description" : "URI indicating the location of the source model artifacts, used when creating `model_version`",
            "type" : "string"
          },
          "description" : {
            "description" : "Description of this `model_version`.",
            "type" : "string"
          },
          "tags" : {
            "description" : "Tags: Additional metadata key-value pairs for this `model_version`.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.ModelVersionTag"
            }
          },
          "user_id" : {
            "description" : "User that created this `model_version`.",
            "type" : "string"
          },
          "creation_timestamp" : {
            "format" : "int64",
            "description" : "Timestamp recorded when this `model_version` was created.",
            "type" : "integer"
          },
          "current_stage" : {
            "description" : "Current stage for this `model_version`.",
            "type" : "string"
          },
          "run_id" : {
            "description" : "MLflow run ID used when creating `model_version`, if `source` was generated by an\nexperiment run stored in MLflow tracking server.",
            "type" : "string"
          },
          "version" : {
            "description" : "Model's version number.",
            "type" : "string"
          },
          "status" : {
            "description" : "Current status of `model_version`",
            "type" : "string",
            "enum" : [ "PENDING_REGISTRATION", "FAILED_REGISTRATION", "READY" ]
          },
          "status_message" : {
            "description" : "Details on current `status`, if it is pending or failed.",
            "type" : "string"
          },
          "last_updated_timestamp" : {
            "format" : "int64",
            "description" : "Timestamp recorded when metadata for this `model_version` was last updated.",
            "type" : "integer"
          },
          "run_link" : {
            "description" : "Run Link: Direct link to the run that generated this version",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.ModelVersionDatabricks" : {
        "properties" : {
          "name" : {
            "example" : "search_ads_model",
            "description" : "Name of the model.",
            "type" : "string"
          },
          "source" : {
            "example" : "dbfs:/databricks/mlflow-tracking/run123/model",
            "description" : "URI that indicates the location of the source model artifacts. This is used when creating the model version.",
            "type" : "string"
          },
          "description" : {
            "example" : "Here's some extra info.",
            "description" : "User-specified description for the object.",
            "type" : "string"
          },
          "permission_level" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.PermissionLevel"
          },
          "tags" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.version_tags"
          },
          "user_id" : {
            "example" : "jane.doe@example.com",
            "format" : "email",
            "description" : "The username of the user that created the object.",
            "type" : "string"
          },
          "creation_timestamp" : {
            "example" : 1594437249910,
            "format" : "int64",
            "description" : "Creation time of the object, as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "current_stage" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.stage"
          },
          "run_id" : {
            "example" : "6fc74c92704341aaa49e74dcc6031057",
            "format" : "uuid",
            "description" : "Unique identifier for the MLflow tracking run associated with the source model artifacts.",
            "type" : "string"
          },
          "version" : {
            "example" : "1",
            "description" : "Version of the model.",
            "type" : "string"
          },
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.status"
          },
          "status_message" : {
            "example" : "Source could not be found.",
            "description" : "Details on the current status, for example why registration failed.",
            "type" : "string"
          },
          "last_updated_timestamp" : {
            "example" : 1594437549910,
            "format" : "int64",
            "description" : "Time of the object at last update, as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "run_link" : {
            "example" : "https://<databricks-instance>/#mlflow/experiments/28774367/runs/6fc74c92704341aaa49e74dcc6031057",
            "description" : "URL of the run associated with the model artifacts. This field is set at model version creation time only for model versions whose source run is from a tracking server that is different from the registry server.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.ModelVersionTag" : {
        "properties" : {
          "key" : {
            "description" : "The tag key.",
            "type" : "string"
          },
          "value" : {
            "description" : "The tag value.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.Param" : {
        "properties" : {
          "key" : {
            "description" : "Key identifying this param.",
            "type" : "string"
          },
          "value" : {
            "description" : "Value associated with this param.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.PermissionLevel" : {
        "example" : "CAN_READ",
        "description" : "Permission level of the requesting user on the object. For what is allowed at each level, see [MLflow Model permissions](..).",
        "type" : "string",
        "enum" : [ "CAN_MANAGE", "CAN_EDIT", "CAN_READ", "CAN_MANAGE_STAGING_VERSIONS", "CAN_MANAGE_PRODUCTION_VERSIONS" ]
      },
      "mlflow.RegisteredModel" : {
        "properties" : {
          "name" : {
            "description" : "Unique name for the model.",
            "type" : "string"
          },
          "description" : {
            "description" : "Description of this `registered_model`.",
            "type" : "string"
          },
          "tags" : {
            "description" : "Tags: Additional metadata key-value pairs for this `registered_model`.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.RegisteredModelTag"
            }
          },
          "latest_versions" : {
            "description" : "Collection of latest model versions for each stage.\nOnly contains models with current `READY` status.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.ModelVersion"
            }
          },
          "user_id" : {
            "description" : "User that created this `registered_model`",
            "type" : "string"
          },
          "creation_timestamp" : {
            "format" : "int64",
            "description" : "Timestamp recorded when this `registered_model` was created.",
            "type" : "integer"
          },
          "last_updated_timestamp" : {
            "format" : "int64",
            "description" : "Timestamp recorded when metadata for this `registered_model` was last updated.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "mlflow.RegisteredModelDatabricks" : {
        "properties" : {
          "name" : {
            "example" : "search_ads_model",
            "description" : "Name of the model.",
            "type" : "string"
          },
          "description" : {
            "example" : "Here's some extra info.",
            "description" : "User-specified description for the object.",
            "type" : "string"
          },
          "permission_level" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.PermissionLevel"
          },
          "tags" : {
            "description" : "Array of tags associated with the model.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.RegisteredModelTag"
            }
          },
          "latest_versions" : {
            "description" : "Array of model versions, each the latest version for its stage.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.ModelVersion"
            }
          },
          "user_id" : {
            "example" : "jane.doe@example.com",
            "format" : "email",
            "description" : "The username of the user that created the object.",
            "type" : "string"
          },
          "creation_timestamp" : {
            "example" : 1594437249910,
            "format" : "int64",
            "description" : "Creation time of the object, as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "id" : {
            "example" : "6fc74c92704341aaa49e74dcc6031057",
            "format" : "uuid",
            "description" : "Unique identifier for the object.",
            "type" : "string"
          },
          "last_updated_timestamp" : {
            "example" : 1594437549910,
            "format" : "int64",
            "description" : "Time of the object at last update, as a Unix timestamp in milliseconds.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "mlflow.RegisteredModelTag" : {
        "properties" : {
          "key" : {
            "description" : "The tag key.",
            "type" : "string"
          },
          "value" : {
            "description" : "The tag value.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.RegistryWebhook" : {
        "properties" : {
          "job_spec" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.JobSpecWithoutSecret"
          },
          "description" : {
            "example" : "Webhook for comment creation",
            "description" : "User-specified description for the webhook.",
            "type" : "string"
          },
          "http_url_spec" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.HttpUrlSpecWithoutSecret"
          },
          "creation_timestamp" : {
            "example" : 1594437249910,
            "format" : "int64",
            "description" : "Creation time of the object, as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "id" : {
            "example" : "124323",
            "description" : "Webhook ID",
            "type" : "string"
          },
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RegistryWebhookStatus"
          },
          "last_updated_timestamp" : {
            "example" : 1594437549910,
            "format" : "int64",
            "description" : "Time of the object at last update, as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "model_name" : {
            "example" : "registered-model-1",
            "description" : "Name of the model whose events would trigger this webhook.",
            "type" : "string"
          },
          "events" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RegistryWebhookEvents"
          }
        },
        "type" : "object"
      },
      "mlflow.RegistryWebhookEvent" : {
        "example" : "MODEL_VERSION_CREATED",
        "type" : "string",
        "enum" : [ "MODEL_VERSION_CREATED", "MODEL_VERSION_TRANSITIONED_STAGE", "TRANSITION_REQUEST_CREATED", "COMMENT_CREATED", "REGISTERED_MODEL_CREATED", "MODEL_VERSION_TAG_SET", "MODEL_VERSION_TRANSITIONED_TO_STAGING", "MODEL_VERSION_TRANSITIONED_TO_PRODUCTION", "MODEL_VERSION_TRANSITIONED_TO_ARCHIVED", "TRANSITION_REQUEST_TO_STAGING_CREATED", "TRANSITION_REQUEST_TO_PRODUCTION_CREATED", "TRANSITION_REQUEST_TO_ARCHIVED_CREATED" ]
      },
      "mlflow.RegistryWebhookEvents" : {
        "example" : [ "MODEL_VERSION_CREATED", "MODEL_VERSION_TRANSITIONED_TO_STAGING", "COMMENT_CREATED" ],
        "description" : "Events that can trigger a registry webhook:\n* `MODEL_VERSION_CREATED`: A new model version was created for the associated model.\n\n* `MODEL_VERSION_TRANSITIONED_STAGE`: A model versions stage was changed.\n\n* `TRANSITION_REQUEST_CREATED`: A user requested a model versions stage be transitioned.\n\n* `COMMENT_CREATED`: A user wrote a comment on a registered model.\n\n* `REGISTERED_MODEL_CREATED`: A new registered model was created. This event type can only be specified for a registry-wide webhook, which can be created by not specifying a model name in the create request.\n\n* `MODEL_VERSION_TAG_SET`: A user set a tag on the model version.\n\n* `MODEL_VERSION_TRANSITIONED_TO_STAGING`: A model version was transitioned to staging.\n\n* `MODEL_VERSION_TRANSITIONED_TO_PRODUCTION`: A model version was transitioned to production.\n\n* `MODEL_VERSION_TRANSITIONED_TO_ARCHIVED`: A model version was archived.\n\n* `TRANSITION_REQUEST_TO_STAGING_CREATED`: A user requested a model version be transitioned to staging.\n\n* `TRANSITION_REQUEST_TO_PRODUCTION_CREATED`: A user requested a model version be transitioned to production.\n\n* `TRANSITION_REQUEST_TO_ARCHIVED_CREATED`: A user requested a model version be archived.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/mlflow.RegistryWebhookEvent"
        }
      },
      "mlflow.RegistryWebhookStatus" : {
        "example" : "ACTIVE",
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "ACTIVE", "DISABLED", "TEST_MODE" ],
        "x-databricks-enum-descriptions" : {
          "ACTIVE" : "Webhook is triggered when an associated event happens.",
          "DISABLED" : "Webhook is not triggered.",
          "TEST_MODE" : "Webhook can be triggered through the test endpoint, but is not triggered on a real event."
        }
      },
      "mlflow.RejectTransitionRequest" : {
        "required" : [ "name", "version", "stage" ],
        "properties" : {
          "comment" : {
            "example" : "This version is great!",
            "description" : "User-provided comment on the action.",
            "type" : "string"
          },
          "name" : {
            "example" : "search_ads_model",
            "description" : "Name of the model.",
            "type" : "string"
          },
          "stage" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.to_stage"
          },
          "version" : {
            "example" : "1",
            "description" : "Version of the model.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.RenameRegisteredModelRequest" : {
        "required" : [ "name" ],
        "properties" : {
          "name" : {
            "description" : "Registered model unique name identifier.",
            "type" : "string"
          },
          "new_name" : {
            "description" : "If provided, updates the name for this `registered_model`.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.RenameRegisteredModelResponse" : {
        "properties" : {
          "registered_model" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RegisteredModel"
          }
        },
        "type" : "object"
      },
      "mlflow.RestoreExperiment" : {
        "required" : [ "experiment_id" ],
        "properties" : {
          "experiment_id" : {
            "description" : "ID of the associated experiment.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.RestoreExperimentResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.RestoreRun" : {
        "required" : [ "run_id" ],
        "properties" : {
          "run_id" : {
            "description" : "ID of the run to restore.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.RestoreRunResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.Run" : {
        "properties" : {
          "data" : {
            "description" : "Run data.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RunData"
          },
          "info" : {
            "description" : "Run metadata.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RunInfo"
          }
        },
        "type" : "object"
      },
      "mlflow.RunData" : {
        "properties" : {
          "metrics" : {
            "description" : "Run metrics.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Metric"
            }
          },
          "params" : {
            "description" : "Run parameters.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Param"
            }
          },
          "tags" : {
            "description" : "Additional metadata key-value pairs.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.RunTag"
            }
          }
        },
        "type" : "object"
      },
      "mlflow.RunInfo" : {
        "properties" : {
          "start_time" : {
            "format" : "int64",
            "description" : "Unix timestamp of when the run started in milliseconds.",
            "type" : "integer"
          },
          "run_uuid" : {
            "description" : "[Deprecated, use run_id instead] Unique identifier for the run. This field will\nbe removed in a future MLflow version.",
            "deprecated" : true,
            "type" : "string"
          },
          "artifact_uri" : {
            "description" : "URI of the directory where artifacts should be uploaded.\nThis can be a local path (starting with \"/\"), or a distributed file system (DFS)\npath, like `s3://bucket/directory` or `dbfs:/my/directory`.\nIf not set, the local `./mlruns` directory is  chosen.",
            "type" : "string"
          },
          "end_time" : {
            "format" : "int64",
            "description" : "Unix timestamp of when the run ended in milliseconds.",
            "type" : "integer"
          },
          "user_id" : {
            "description" : "User who initiated the run.\nThis field is deprecated as of MLflow 1.0, and will be removed in a future\nMLflow release. Use 'mlflow.user' tag instead.",
            "deprecated" : true,
            "type" : "string"
          },
          "run_id" : {
            "description" : "Unique identifier for the run.",
            "type" : "string"
          },
          "status" : {
            "description" : "Current status of the run.",
            "type" : "string",
            "enum" : [ "RUNNING", "SCHEDULED", "FINISHED", "FAILED", "KILLED" ]
          },
          "lifecycle_stage" : {
            "description" : "Current life cycle stage of the experiment : OneOf(\"active\", \"deleted\")",
            "type" : "string"
          },
          "experiment_id" : {
            "description" : "The experiment ID.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.RunTag" : {
        "properties" : {
          "key" : {
            "description" : "The tag key.",
            "type" : "string"
          },
          "value" : {
            "description" : "The tag value.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.SearchExperiments" : {
        "properties" : {
          "view_type" : {
            "description" : "Qualifier for type of experiments to be returned.\nIf unspecified, return only active experiments.",
            "type" : "string",
            "enum" : [ "ACTIVE_ONLY", "DELETED_ONLY", "ALL" ]
          },
          "page_token" : {
            "description" : "Token indicating the page of experiments to fetch",
            "type" : "string"
          },
          "filter" : {
            "description" : "String representing a SQL filter condition (e.g. \"name ILIKE 'my-experiment%'\")",
            "type" : "string"
          },
          "order_by" : {
            "description" : "List of columns for ordering search results, which can include experiment name and last updated\ntimestamp with an optional \"DESC\" or \"ASC\" annotation, where \"ASC\" is the default.\nTiebreaks are done by experiment id DESC.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "max_results" : {
            "format" : "int64",
            "description" : "Maximum number of experiments desired. Max threshold is 3000.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "mlflow.SearchExperimentsResponse" : {
        "properties" : {
          "experiments" : {
            "description" : "Experiments that match the search criteria",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Experiment"
            }
          },
          "next_page_token" : {
            "description" : "Token that can be used to retrieve the next page of experiments.\nAn empty token means that no more experiments are available for retrieval.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.SearchModelVersionsResponse" : {
        "properties" : {
          "model_versions" : {
            "description" : "Models that match the search criteria",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.ModelVersion"
            }
          },
          "next_page_token" : {
            "description" : "Pagination token to request next page of models for the same search query.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.SearchRegisteredModelsResponse" : {
        "properties" : {
          "next_page_token" : {
            "description" : "Pagination token to request the next page of models.",
            "type" : "string"
          },
          "registered_models" : {
            "description" : "Registered Models that match the search criteria.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.RegisteredModel"
            }
          }
        },
        "type" : "object"
      },
      "mlflow.SearchRuns" : {
        "properties" : {
          "experiment_ids" : {
            "description" : "List of experiment IDs to search over.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "page_token" : {
            "description" : "Token for the current page of runs.",
            "type" : "string"
          },
          "filter" : {
            "description" : "A filter expression over params, metrics, and tags, that allows returning a subset of\nruns. The syntax is a subset of SQL that supports ANDing together binary operations\nbetween a param, metric, or tag and a constant.\n\nExample: `metrics.rmse < 1 and params.model_class = 'LogisticRegression'`\n\nYou can select columns with special characters (hyphen, space, period, etc.) by using double quotes:\n`metrics.\"model class\" = 'LinearRegression' and tags.\"user-name\" = 'Tomas'`\n\nSupported operators are `=`, `!=`, `>`, `>=`, `<`, and `<=`.",
            "type" : "string"
          },
          "run_view_type" : {
            "default" : "ACTIVE_ONLY",
            "description" : "Whether to display only active, only deleted, or all runs.\nDefaults to only active runs.",
            "type" : "string",
            "enum" : [ "ACTIVE_ONLY", "DELETED_ONLY", "ALL" ]
          },
          "order_by" : {
            "description" : "List of columns to be ordered by, including attributes, params, metrics, and tags with an\noptional \"DESC\" or \"ASC\" annotation, where \"ASC\" is the default.\nExample: [\"params.input DESC\", \"metrics.alpha ASC\", \"metrics.rmse\"]\nTiebreaks are done by start_time DESC followed by run_id for runs with the same start time\n(and this is the default ordering criterion if order_by is not provided).",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "max_results" : {
            "default" : "1000",
            "format" : "int32",
            "description" : "Maximum number of runs desired. Max threshold is 50000",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "mlflow.SearchRunsResponse" : {
        "properties" : {
          "next_page_token" : {
            "description" : "Token for the next page of runs.",
            "type" : "string"
          },
          "runs" : {
            "description" : "Runs that match the search criteria.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.Run"
            }
          }
        },
        "type" : "object"
      },
      "mlflow.SetExperimentTag" : {
        "required" : [ "experiment_id", "key", "value" ],
        "properties" : {
          "experiment_id" : {
            "description" : "ID of the experiment under which to log the tag. Must be provided.",
            "type" : "string"
          },
          "key" : {
            "description" : "Name of the tag. Maximum size depends on storage backend.\nAll storage backends are guaranteed to support key values up to 250 bytes in size.",
            "type" : "string"
          },
          "value" : {
            "description" : "String value of the tag being logged. Maximum size depends on storage backend.\nAll storage backends are guaranteed to support key values up to 5000 bytes in size.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.SetExperimentTagResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.SetModelVersionTagRequest" : {
        "required" : [ "name", "version", "key", "value" ],
        "properties" : {
          "key" : {
            "description" : "Name of the tag. Maximum size depends on storage backend.\nIf a tag with this name already exists, its preexisting value will be replaced by the specified `value`.\nAll storage backends are guaranteed to support key values up to 250 bytes in size.",
            "type" : "string"
          },
          "name" : {
            "description" : "Unique name of the model.",
            "type" : "string"
          },
          "value" : {
            "description" : "String value of the tag being logged. Maximum size depends on storage backend.\nAll storage backends are guaranteed to support key values up to 5000 bytes in size.",
            "type" : "string"
          },
          "version" : {
            "description" : "Model version number.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.SetModelVersionTagResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.SetRegisteredModelTagRequest" : {
        "required" : [ "name", "key", "value" ],
        "properties" : {
          "key" : {
            "description" : "Name of the tag. Maximum size depends on storage backend.\nIf a tag with this name already exists, its preexisting value will be replaced by the specified `value`.\nAll storage backends are guaranteed to support key values up to 250 bytes in size.",
            "type" : "string"
          },
          "name" : {
            "description" : "Unique name of the model.",
            "type" : "string"
          },
          "value" : {
            "description" : "String value of the tag being logged. Maximum size depends on storage backend.\nAll storage backends are guaranteed to support key values up to 5000 bytes in size.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.SetRegisteredModelTagResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.SetTag" : {
        "required" : [ "key", "value" ],
        "properties" : {
          "key" : {
            "description" : "Name of the tag. Maximum size depends on storage backend.\nAll storage backends are guaranteed to support key values up to 250 bytes in size.",
            "type" : "string"
          },
          "run_id" : {
            "description" : "ID of the run under which to log the tag. Must be provided.",
            "type" : "string"
          },
          "run_uuid" : {
            "description" : "[Deprecated, use run_id instead] ID of the run under which to log the tag. This field will\nbe removed in a future MLflow version.",
            "deprecated" : true,
            "type" : "string"
          },
          "value" : {
            "description" : "String value of the tag being logged. Maximum size depends on storage backend.\nAll storage backends are guaranteed to support key values up to 5000 bytes in size.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.SetTagResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.TestRegistryWebhook" : {
        "properties" : {
          "body" : {
            "example" : "OK",
            "description" : "Body of the response from the webhook URL",
            "type" : "string"
          },
          "status_code" : {
            "example" : 200,
            "format" : "int32",
            "description" : "Status code returned by the webhook URL",
            "type" : "integer"
          }
        },
        "description" : "Test webhook response object.",
        "type" : "object"
      },
      "mlflow.TestRegistryWebhookRequest" : {
        "required" : [ "id" ],
        "properties" : {
          "event" : {
            "description" : "If `event` is specified, the test trigger uses the specified event. If `event` is not specified, the test trigger uses a randomly chosen event associated with the webhook.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RegistryWebhookEvent"
          },
          "id" : {
            "example" : "124323",
            "description" : "Webhook ID",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.TransitionModelVersionStage" : {
        "required" : [ "name", "version", "stage", "archive_existing_versions" ],
        "properties" : {
          "archive_existing_versions" : {
            "description" : "When transitioning a model version to a particular stage, this flag dictates whether all\nexisting model versions in that stage should be atomically moved to the \"archived\" stage.\nThis ensures that at-most-one model version exists in the target stage.\nThis field is *required* when transitioning a model versions's stage",
            "type" : "boolean"
          },
          "name" : {
            "description" : "Name of the registered model",
            "type" : "string"
          },
          "stage" : {
            "description" : "Transition `model_version` to new stage.",
            "type" : "string"
          },
          "version" : {
            "description" : "Model version number",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.TransitionModelVersionStageDatabricks" : {
        "required" : [ "name", "version", "stage", "archive_existing_versions" ],
        "properties" : {
          "stage" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.to_stage"
          },
          "name" : {
            "example" : "search_ads_model",
            "description" : "Name of the model.",
            "type" : "string"
          },
          "archive_existing_versions" : {
            "example" : true,
            "description" : "Specifies whether to archive all current model versions in the target stage.",
            "type" : "boolean"
          },
          "version" : {
            "example" : "1",
            "description" : "Version of the model.",
            "type" : "string"
          },
          "comment" : {
            "example" : "This version is great!",
            "description" : "User-provided comment on the action.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.TransitionModelVersionStageResponse" : {
        "properties" : {
          "model_version" : {
            "description" : "Updated model version",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.ModelVersion"
          }
        },
        "type" : "object"
      },
      "mlflow.TransitionRequest" : {
        "properties" : {
          "available_actions" : {
            "description" : "Array of actions on the activity allowed for the current viewer.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/mlflow.ActivityAction"
            }
          },
          "user_id" : {
            "example" : "jane.doe@example.com",
            "format" : "email",
            "description" : "The username of the user that created the object.",
            "type" : "string"
          },
          "creation_timestamp" : {
            "example" : 1594437249910,
            "format" : "int64",
            "description" : "Creation time of the object, as a Unix timestamp in milliseconds.",
            "type" : "integer"
          },
          "to_stage" : {
            "description" : "Target stage of the transition (if the activity is stage transition related). Valid values are:\n\n* `None`: The initial stage of a model version.\n\n* `Staging`: Staging or pre-production stage.\n\n* `Production`: Production stage.\n\n* `Archived`: Archived stage.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.to_stage"
          },
          "comment" : {
            "example" : "This version is great!",
            "description" : "User-provided comment associated with the transition request.",
            "type" : "string"
          }
        },
        "description" : "Transition request details.",
        "type" : "object"
      },
      "mlflow.UpdateComment" : {
        "required" : [ "id", "comment" ],
        "properties" : {
          "comment" : {
            "example" : "This version is great!",
            "description" : "User-provided comment on the action.",
            "type" : "string"
          },
          "id" : {
            "example" : "6fc74c92704341aaa49e74dcc6031057",
            "format" : "uuid",
            "description" : "Unique identifier of an activity",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.UpdateExperiment" : {
        "required" : [ "experiment_id" ],
        "properties" : {
          "experiment_id" : {
            "description" : "ID of the associated experiment.",
            "type" : "string"
          },
          "new_name" : {
            "description" : "If provided, the experiment's name is changed to the new name. The new name must be unique.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.UpdateExperimentResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "mlflow.UpdateModelVersionRequest" : {
        "required" : [ "name", "version" ],
        "properties" : {
          "description" : {
            "description" : "If provided, updates the description for this `registered_model`.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of the registered model",
            "type" : "string"
          },
          "version" : {
            "description" : "Model version number",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.UpdateRegisteredModelRequest" : {
        "required" : [ "name" ],
        "properties" : {
          "description" : {
            "description" : "If provided, updates the description for this `registered_model`.",
            "type" : "string"
          },
          "name" : {
            "description" : "Registered model unique name identifier.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "mlflow.UpdateRegistryWebhook" : {
        "required" : [ "id" ],
        "properties" : {
          "job_spec" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.JobSpec"
          },
          "description" : {
            "example" : "Webhook for comment creation",
            "description" : "User-specified description for the webhook.",
            "type" : "string"
          },
          "http_url_spec" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.HttpUrlSpec"
          },
          "id" : {
            "example" : "124323",
            "description" : "Webhook ID",
            "type" : "string"
          },
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RegistryWebhookStatus"
          },
          "events" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RegistryWebhookEvents"
          }
        },
        "type" : "object"
      },
      "mlflow.UpdateRun" : {
        "properties" : {
          "end_time" : {
            "format" : "int64",
            "description" : "Unix timestamp in milliseconds of when the run ended.",
            "type" : "integer"
          },
          "run_id" : {
            "description" : "ID of the run to update. Must be provided.",
            "type" : "string"
          },
          "run_uuid" : {
            "description" : "[Deprecated, use run_id instead] ID of the run to update.. This field will\nbe removed in a future MLflow version.",
            "deprecated" : true,
            "type" : "string"
          },
          "status" : {
            "description" : "Updated status of the run.",
            "type" : "string",
            "enum" : [ "RUNNING", "SCHEDULED", "FINISHED", "FAILED", "KILLED" ]
          }
        },
        "type" : "object"
      },
      "mlflow.UpdateRunResponse" : {
        "properties" : {
          "run_info" : {
            "description" : "Updated metadata of the run.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/mlflow.RunInfo"
          }
        },
        "type" : "object"
      },
      "mlflow.activity_id" : {
        "example" : "6fc74c92704341aaa49e74dcc6031057",
        "format" : "uuid",
        "description" : "Unique identifier of an activity",
        "type" : "string"
      },
      "mlflow.model_name" : {
        "example" : "registered-model-1",
        "description" : "Name of the model whose events would trigger this webhook.",
        "type" : "string"
      },
      "mlflow.stage" : {
        "example" : "Staging",
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "None", "Staging", "Production", "Archived" ],
        "x-databricks-enum-descriptions" : {
          "None" : "The initial stage of a model version.",
          "Staging" : "Staging or pre-production stage.",
          "Production" : "Production stage.",
          "Archived" : "Archived stage."
        }
      },
      "mlflow.status" : {
        "example" : "PENDING_REGISTRATION",
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "PENDING_REGISTRATION", "FAILED_REGISTRATION", "READY" ],
        "x-databricks-enum-descriptions" : {
          "PENDING_REGISTRATION" : "Request to register a new model version is pending as server performs background tasks.",
          "FAILED_REGISTRATION" : "Request to register a new model version has failed.",
          "READY" : "Model version is ready for use."
        }
      },
      "mlflow.to_stage" : {
        "description" : "Target stage of the transition. Valid values are:\n\n* `None`: The initial stage of a model version.\n\n* `Staging`: Staging or pre-production stage.\n\n* `Production`: Production stage.\n\n* `Archived`: Archived stage.",
        "extRef" : true,
        "ref" : true,
        "$ref" : "#/components/schemas/mlflow.stage"
      },
      "mlflow.version_tags" : {
        "description" : "Array of tags that are associated with the model version.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/mlflow.ModelVersionTag"
        }
      },
      "mlflow.webhook_id" : {
        "example" : "124323",
        "description" : "Webhook ID",
        "type" : "string"
      },
      "permissions.AccessControlRequest" : {
        "properties" : {
          "group_name" : {
            "description" : "name of the group",
            "type" : "string"
          },
          "permission_level" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/permissions.PermissionLevel"
          },
          "service_principal_name" : {
            "description" : "name of the service principal",
            "type" : "string"
          },
          "user_name" : {
            "description" : "name of the user",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "permissions.AccessControlResponse" : {
        "properties" : {
          "all_permissions" : {
            "description" : "All permissions.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.Permission"
            }
          },
          "group_name" : {
            "description" : "name of the group",
            "type" : "string"
          },
          "service_principal_name" : {
            "description" : "name of the service principal",
            "type" : "string"
          },
          "user_name" : {
            "description" : "name of the user",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "permissions.CreateWorkspaceAssignments" : {
        "required" : [ "permission_assignments" ],
        "properties" : {
          "permission_assignments" : {
            "description" : "Array of permissions assignments to apply to a workspace.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.PermissionAssignmentInput"
            }
          }
        },
        "type" : "object"
      },
      "permissions.DeleteWorkspaceAssignments" : {
        "properties" : { },
        "type" : "object"
      },
      "permissions.GetPermissionLevelsResponse" : {
        "properties" : {
          "permission_levels" : {
            "description" : "Specific permission levels",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.PermissionsDescription"
            }
          }
        },
        "type" : "object"
      },
      "permissions.ObjectPermissions" : {
        "properties" : {
          "access_control_list" : {
            "description" : "",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.AccessControlResponse"
            }
          },
          "object_id" : {
            "description" : "",
            "type" : "string"
          },
          "object_type" : {
            "description" : "",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "permissions.Permission" : {
        "properties" : {
          "inherited" : {
            "description" : "",
            "type" : "boolean"
          },
          "inherited_from_object" : {
            "description" : "",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "permission_level" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/permissions.PermissionLevel"
          }
        },
        "type" : "object"
      },
      "permissions.PermissionAssignment" : {
        "properties" : {
          "error" : {
            "description" : "Error response associated with a workspace permission assignment, if any.",
            "type" : "string"
          },
          "permissions" : {
            "description" : "The permissions level of the service principal.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.WorkspacePermission"
            }
          },
          "principal" : {
            "description" : "Information about the service principal assigned for the workspace.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/permissions.PrincipalOutput"
          }
        },
        "type" : "object"
      },
      "permissions.PermissionAssignmentInput" : {
        "required" : [ "permissions" ],
        "properties" : {
          "group_name" : {
            "description" : "The group name for the service principal.",
            "type" : "string"
          },
          "permissions" : {
            "description" : "Array of permissions to apply to the workspace for the service principal.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.WorkspacePermission"
            }
          },
          "service_principal_name" : {
            "description" : "The name of the service principal.",
            "type" : "string"
          },
          "user_name" : {
            "description" : "The username of the owner of the service principal.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "permissions.PermissionAssignments" : {
        "properties" : {
          "permission_assignments" : {
            "description" : "Array of permissions assignments defined for a workspace.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.PermissionAssignment"
            }
          }
        },
        "type" : "object"
      },
      "permissions.PermissionLevel" : {
        "description" : "Permission level",
        "type" : "string",
        "enum" : [ "CAN_MANAGE", "CAN_RESTART", "CAN_ATTACH_TO", "IS_OWNER", "CAN_MANAGE_RUN", "CAN_VIEW", "CAN_READ", "CAN_RUN", "CAN_EDIT", "CAN_USE", "CAN_MANAGE_STAGING_VERSIONS", "CAN_MANAGE_PRODUCTION_VERSIONS", "CAN_EDIT_METADATA", "CAN_VIEW_METADATA", "CAN_BIND" ]
      },
      "permissions.PermissionOutput" : {
        "properties" : {
          "description" : {
            "description" : "The results of a permissions query.",
            "type" : "string"
          },
          "permission_level" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/permissions.WorkspacePermission"
          }
        },
        "type" : "object"
      },
      "permissions.PermissionsDescription" : {
        "properties" : {
          "description" : {
            "description" : "",
            "type" : "string"
          },
          "permission_level" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/permissions.PermissionLevel"
          }
        },
        "type" : "object"
      },
      "permissions.PermissionsRequest" : {
        "properties" : {
          "access_control_list" : {
            "description" : "",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.AccessControlRequest"
            }
          }
        },
        "type" : "object"
      },
      "permissions.PrincipalOutput" : {
        "properties" : {
          "display_name" : {
            "description" : "The display name of the service principal.",
            "type" : "string"
          },
          "principal_id" : {
            "format" : "int64",
            "description" : "The unique, opaque id of the principal.",
            "type" : "integer"
          },
          "group_name" : {
            "description" : "The group name for the service principal.",
            "type" : "string"
          },
          "service_principal_name" : {
            "description" : "The name of the service principal.",
            "type" : "string"
          },
          "user_name" : {
            "description" : "The username of the owner of the service principal.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "permissions.UpdateWorkspaceAssignments" : {
        "required" : [ "permissions" ],
        "properties" : {
          "permissions" : {
            "description" : "Array of permissions assignments to update on the workspace.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.WorkspacePermission"
            }
          }
        },
        "type" : "object"
      },
      "permissions.WorkspaceAssignmentsCreated" : {
        "properties" : {
          "permission_assignments" : {
            "description" : "Array of permissions assignments applied to a workspace.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.PermissionAssignment"
            }
          }
        },
        "type" : "object"
      },
      "permissions.WorkspaceAssignmentsUpdated" : {
        "properties" : { },
        "type" : "object"
      },
      "permissions.WorkspacePermission" : {
        "type" : "string",
        "enum" : [ "UNKNOWN", "USER", "ADMIN" ]
      },
      "permissions.WorkspacePermissions" : {
        "properties" : {
          "permissions" : {
            "description" : "Array of permissions defined for a workspace.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/permissions.PermissionOutput"
            }
          }
        },
        "type" : "object"
      },
      "pipelines.CreatePipeline" : {
        "properties" : {
          "continuous" : {
            "description" : "Whether the pipeline is continuous or triggered. This replaces `trigger`.",
            "type" : "boolean"
          },
          "name" : {
            "description" : "Friendly identifier for this pipeline.",
            "type" : "string"
          },
          "channel" : {
            "description" : "DLT Release Channel that specifies which version to use.",
            "type" : "string"
          },
          "catalog" : {
            "description" : "Catalog in UC to add tables to. If target is specified, tables in this pipeline will be\npublished to a \"target\" schema inside catalog (i.e. <catalog>.<target>.<table>).",
            "type" : "string"
          },
          "clusters" : {
            "description" : "Cluster settings for this pipeline deployment.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/pipelines.PipelineCluster"
            }
          },
          "allow_duplicate_names" : {
            "default" : "false",
            "description" : "If false, deployment will fail if name conflicts with that of another pipeline.",
            "type" : "boolean"
          },
          "photon" : {
            "description" : "Whether Photon is enabled for this pipeline.",
            "type" : "boolean"
          },
          "edition" : {
            "description" : "Pipeline product edition.",
            "type" : "string"
          },
          "id" : {
            "description" : "Unique identifier for this pipeline.",
            "type" : "string"
          },
          "filters" : {
            "description" : "Filters on which Pipeline packages to include in the deployed graph.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.Filters"
          },
          "dry_run" : {
            "description" : "",
            "type" : "boolean"
          },
          "libraries" : {
            "description" : "Libraries or code needed by this deployment.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/pipelines.PipelineLibrary"
            }
          },
          "configuration" : {
            "properties" : { },
            "description" : "String-String configuration for this pipeline execution.",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "storage" : {
            "description" : "DBFS root directory for storing checkpoints and tables.",
            "type" : "string"
          },
          "target" : {
            "description" : "Target schema (database) to add tables in this pipeline to.",
            "type" : "string"
          },
          "development" : {
            "description" : "Whether the pipeline is in Development mode. Defaults to false.",
            "type" : "boolean"
          },
          "trigger" : {
            "description" : "Which pipeline trigger to use. Deprecated: Use `continuous` instead.",
            "deprecated" : true,
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.PipelineTrigger"
          }
        },
        "type" : "object"
      },
      "pipelines.CreatePipelineResponse" : {
        "properties" : {
          "effective_settings" : {
            "description" : "Only returned when dry_run is true",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.PipelineSpec"
          },
          "pipeline_id" : {
            "description" : "Only returned when dry_run is false",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "pipelines.CronTrigger" : {
        "properties" : {
          "quartz_cron_schedule" : {
            "description" : "",
            "type" : "string"
          },
          "timezone_id" : {
            "description" : "",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "pipelines.DeletePipelineResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "pipelines.EditPipeline" : {
        "properties" : {
          "continuous" : {
            "description" : "Whether the pipeline is continuous or triggered. This replaces `trigger`.",
            "type" : "boolean"
          },
          "name" : {
            "description" : "Friendly identifier for this pipeline.",
            "type" : "string"
          },
          "channel" : {
            "description" : "DLT Release Channel that specifies which version to use.",
            "type" : "string"
          },
          "catalog" : {
            "description" : "Catalog in UC to add tables to. If target is specified, tables in this pipeline will be\npublished to a \"target\" schema inside catalog (i.e. <catalog>.<target>.<table>).",
            "type" : "string"
          },
          "pipeline_id" : {
            "description" : "Unique identifier for this pipeline.",
            "type" : "string"
          },
          "clusters" : {
            "description" : "Cluster settings for this pipeline deployment.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/pipelines.PipelineCluster"
            }
          },
          "allow_duplicate_names" : {
            "default" : "false",
            "description" : "If false, deployment will fail if name has changed and conflicts the name of another pipeline.",
            "type" : "boolean"
          },
          "expected_last_modified" : {
            "format" : "int64",
            "description" : "If present, the last-modified time of the pipeline settings before the edit.\nIf the settings were modified after that time, then the request will fail with\na conflict.",
            "type" : "integer"
          },
          "photon" : {
            "description" : "Whether Photon is enabled for this pipeline.",
            "type" : "boolean"
          },
          "edition" : {
            "description" : "Pipeline product edition.",
            "type" : "string"
          },
          "id" : {
            "description" : "Unique identifier for this pipeline.",
            "type" : "string"
          },
          "filters" : {
            "description" : "Filters on which Pipeline packages to include in the deployed graph.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.Filters"
          },
          "libraries" : {
            "description" : "Libraries or code needed by this deployment.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/pipelines.PipelineLibrary"
            }
          },
          "configuration" : {
            "properties" : { },
            "description" : "String-String configuration for this pipeline execution.",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "storage" : {
            "description" : "DBFS root directory for storing checkpoints and tables.",
            "type" : "string"
          },
          "target" : {
            "description" : "Target schema (database) to add tables in this pipeline to.",
            "type" : "string"
          },
          "development" : {
            "description" : "Whether the pipeline is in Development mode. Defaults to false.",
            "type" : "boolean"
          },
          "trigger" : {
            "description" : "Which pipeline trigger to use. Deprecated: Use `continuous` instead.",
            "deprecated" : true,
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.PipelineTrigger"
          }
        },
        "type" : "object"
      },
      "pipelines.EditPipelineResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "pipelines.Filters" : {
        "properties" : {
          "exclude" : {
            "description" : "Paths to exclude.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "include" : {
            "description" : "Paths to include.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          }
        },
        "type" : "object"
      },
      "pipelines.GetPipelineResponse" : {
        "properties" : {
          "health" : {
            "description" : "The health of a pipeline.",
            "type" : "string",
            "enum" : [ "HEALTHY", "UNHEALTHY" ]
          },
          "name" : {
            "description" : "A human friendly identifier for the pipeline, taken from the `spec`.",
            "type" : "string"
          },
          "pipeline_id" : {
            "description" : "The ID of the pipeline.",
            "type" : "string"
          },
          "state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.PipelineState"
          },
          "latest_updates" : {
            "description" : "Status of the latest updates for the pipeline. Ordered with the newest update first.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/pipelines.UpdateStateInfo"
            }
          },
          "last_modified" : {
            "format" : "int64",
            "description" : "The last time the pipeline settings were modified or created.",
            "type" : "integer"
          },
          "creator_user_name" : {
            "description" : "The username of the pipeline creator.",
            "type" : "string"
          },
          "cause" : {
            "description" : "An optional message detailing the cause of the pipeline state.",
            "type" : "string"
          },
          "spec" : {
            "description" : "The pipeline specification. This field is not returned when called by `ListPipelines`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.PipelineSpec"
          },
          "run_as_user_name" : {
            "description" : "Username of the user that the pipeline will run on behalf of.",
            "type" : "string"
          },
          "cluster_id" : {
            "description" : "The ID of the cluster that the pipeline is running on.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "pipelines.GetUpdateResponse" : {
        "properties" : {
          "update" : {
            "description" : "The current update info.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.UpdateInfo"
          }
        },
        "type" : "object"
      },
      "pipelines.ListPipelinesResponse" : {
        "properties" : {
          "next_page_token" : {
            "description" : "If present, a token to fetch the next page of events.",
            "type" : "string"
          },
          "statuses" : {
            "description" : "The list of events matching the request criteria.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/pipelines.PipelineStateInfo"
            }
          }
        },
        "type" : "object"
      },
      "pipelines.ListUpdatesResponse" : {
        "properties" : {
          "next_page_token" : {
            "description" : "If present, then there are more results, and this a token to be used in a subsequent request\nto fetch the next page.",
            "type" : "string"
          },
          "prev_page_token" : {
            "description" : "If present, then this token can be used in a subsequent request to fetch the previous page.",
            "type" : "string"
          },
          "updates" : {
            "description" : "",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/pipelines.UpdateInfo"
            }
          }
        },
        "type" : "object"
      },
      "pipelines.ManualTrigger" : {
        "properties" : { },
        "type" : "object"
      },
      "pipelines.NotebookLibrary" : {
        "properties" : {
          "path" : {
            "description" : "The absolute path of the notebook.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "pipelines.PipelineCluster" : {
        "properties" : {
          "ssh_public_keys" : {
            "description" : "SSH public key contents that will be added to each Spark node in this cluster. The\ncorresponding private keys can be used to login with the user name `ubuntu` on port `2200`.\nUp to 10 keys can be specified.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "policy_id" : {
            "description" : "The ID of the cluster policy used to create the cluster if applicable.",
            "type" : "string"
          },
          "node_type_id" : {
            "description" : "This field encodes, through a single value, the resources available to each of\nthe Spark nodes in this cluster. For example, the Spark nodes can be provisioned\nand optimized for memory or compute intensive workloads. A list of available node\ntypes can be retrieved by using the :method:clusters/listNodeTypes API call.\n",
            "type" : "string"
          },
          "num_workers" : {
            "format" : "int32",
            "description" : "Number of worker nodes that this cluster should have. A cluster has one Spark Driver\nand `num_workers` Executors for a total of `num_workers` + 1 Spark nodes.\n\nNote: When reading the properties of a cluster, this field reflects the desired number\nof workers rather than the actual current number of workers. For instance, if a cluster\nis resized from 5 to 10 workers, this field will immediately be updated to reflect\nthe target size of 10 workers, whereas the workers listed in `spark_info` will gradually\nincrease from 5 to 10 as the new nodes are provisioned.",
            "type" : "integer"
          },
          "label" : {
            "description" : "Cluster label",
            "type" : "string"
          },
          "driver_instance_pool_id" : {
            "description" : "The optional ID of the instance pool for the driver of the cluster belongs.\nThe pool cluster uses the instance pool with id (instance_pool_id) if the driver pool is not\nassigned.",
            "type" : "string"
          },
          "apply_policy_default_values" : {
            "description" : "Note: This field won't be persisted. Only API users will check this field.",
            "type" : "boolean"
          },
          "custom_tags" : {
            "properties" : { },
            "description" : "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS\ninstances and EBS volumes) with these tags in addition to `default_tags`. Notes:\n\n- Currently, Databricks allows at most 45 custom tags\n\n- Clusters can only reuse cloud resources if the resources' tags are a subset of the cluster tags",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "autoscale" : {
            "description" : "Parameters needed in order to automatically scale clusters up and down based on load.\nNote: autoscaling works best with DB runtime versions 3.0 or later.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.AutoScale"
          },
          "spark_conf" : {
            "example" : {
              "spark.speculation" : true,
              "spark.streaming.ui.retainedBatches" : 5
            },
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified Spark configuration key-value pairs.\nSee :method:clusters/create for more details.\n",
            "type" : "object",
            "additionalProperties" : {
              "type" : "string"
            }
          },
          "driver_node_type_id" : {
            "description" : "The node type of the Spark driver.\nNote that this field is optional; if unset, the driver node type will be set as the same value\nas `node_type_id` defined above.",
            "type" : "string"
          },
          "instance_pool_id" : {
            "description" : "The optional ID of the instance pool to which the cluster belongs.",
            "type" : "string"
          },
          "gcp_attributes" : {
            "description" : "Attributes related to clusters running on Google Cloud Platform.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/clusters.GcpAttributes"
          },
          "azure_attributes" : {
            "description" : "Attributes related to clusters running on Amazon Web Services.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/clusters.AzureAttributes"
          },
          "aws_attributes" : {
            "description" : "Attributes related to clusters running on Amazon Web Services.\nIf not specified at cluster creation, a set of default values will be used.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/clusters.AwsAttributes"
          },
          "spark_env_vars" : {
            "properties" : { },
            "description" : "An object containing a set of optional, user-specified environment variable key-value pairs.\nPlease note that key-value pair of the form (X,Y) will be exported as is (i.e.,\n`export X='Y'`) while launching the driver and workers.\n\nIn order to specify an additional set of `SPARK_DAEMON_JAVA_OPTS`, we recommend appending\nthem to `$SPARK_DAEMON_JAVA_OPTS` as shown in the example below. This ensures that all\ndefault databricks managed environmental variables are included as well.\n\nExample Spark environment variables:\n`{\"SPARK_WORKER_MEMORY\": \"28000m\", \"SPARK_LOCAL_DIRS\": \"/local_disk0\"}` or\n`{\"SPARK_DAEMON_JAVA_OPTS\": \"$SPARK_DAEMON_JAVA_OPTS -Dspark.shuffle.service.enabled=true\"}`",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "cluster_log_conf" : {
            "description" : "The configuration for delivering spark logs to a long-term storage destination.\nTwo kinds of destinations (dbfs and s3) are supported. Only one destination can be specified\nfor one cluster. If the conf is given, the logs will be delivered to the destination every\n`5 mins`. The destination of driver logs is `$destination/$clusterId/driver`, while\nthe destination of executor logs is `$destination/$clusterId/executor`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/clusters.ClusterLogConf"
          }
        },
        "type" : "object"
      },
      "pipelines.PipelineLibrary" : {
        "properties" : {
          "jar" : {
            "description" : "URI of the jar to be installed. Currently only DBFS and S3 URIs are supported.\nFor example: `{ \"jar\": \"dbfs:/mnt/databricks/library.jar\" }` or\n`{ \"jar\": \"s3://my-bucket/library.jar\" }`.\nIf S3 is used, please make sure the cluster has read access on the library. You may need to\nlaunch the cluster with an IAM role to access the S3 URI.",
            "type" : "string"
          },
          "maven" : {
            "description" : "Specification of a maven library to be installed. For example:\n`{ \"coordinates\": \"org.jsoup:jsoup:1.7.2\" }`",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/libraries.MavenLibrary"
          },
          "notebook" : {
            "description" : "The path to a notebook that defines a pipeline and is stored in the Databricks workspace.\nFor example: `{ \"notebook\" : { \"path\" : \"/my-pipeline-notebook-path\" } }`.\nCurrently, only Scala notebooks are supported, and pipelines must be defined in a package\ncell.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.NotebookLibrary"
          },
          "whl" : {
            "description" : "URI of the wheel to be installed.\nFor example: `{ \"whl\": \"dbfs:/my/whl\" }` or `{ \"whl\": \"s3://my-bucket/whl\" }`.\nIf S3 is used, please make sure the cluster has read access on the library. You may need to\nlaunch the cluster with an IAM role to access the S3 URI.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "pipelines.PipelineSpec" : {
        "properties" : {
          "continuous" : {
            "description" : "Whether the pipeline is continuous or triggered. This replaces `trigger`.",
            "type" : "boolean"
          },
          "name" : {
            "description" : "Friendly identifier for this pipeline.",
            "type" : "string"
          },
          "channel" : {
            "description" : "DLT Release Channel that specifies which version to use.",
            "type" : "string"
          },
          "catalog" : {
            "description" : "Catalog in UC to add tables to. If target is specified, tables in this pipeline will be\npublished to a \"target\" schema inside catalog (i.e. <catalog>.<target>.<table>).",
            "type" : "string"
          },
          "clusters" : {
            "description" : "Cluster settings for this pipeline deployment.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/pipelines.PipelineCluster"
            }
          },
          "photon" : {
            "description" : "Whether Photon is enabled for this pipeline.",
            "type" : "boolean"
          },
          "edition" : {
            "description" : "Pipeline product edition.",
            "type" : "string"
          },
          "id" : {
            "description" : "Unique identifier for this pipeline.",
            "type" : "string"
          },
          "filters" : {
            "description" : "Filters on which Pipeline packages to include in the deployed graph.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.Filters"
          },
          "libraries" : {
            "description" : "Libraries or code needed by this deployment.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/pipelines.PipelineLibrary"
            }
          },
          "configuration" : {
            "properties" : { },
            "description" : "String-String configuration for this pipeline execution.",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "storage" : {
            "description" : "DBFS root directory for storing checkpoints and tables.",
            "type" : "string"
          },
          "target" : {
            "description" : "Target schema (database) to add tables in this pipeline to.",
            "type" : "string"
          },
          "development" : {
            "description" : "Whether the pipeline is in Development mode. Defaults to false.",
            "type" : "boolean"
          },
          "trigger" : {
            "description" : "Which pipeline trigger to use. Deprecated: Use `continuous` instead.",
            "deprecated" : true,
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.PipelineTrigger"
          }
        },
        "type" : "object"
      },
      "pipelines.PipelineState" : {
        "description" : "The pipeline state.",
        "type" : "string",
        "enum" : [ "DEPLOYING", "STARTING", "RUNNING", "STOPPING", "DELETED", "RECOVERING", "FAILED", "RESETTING", "IDLE" ]
      },
      "pipelines.PipelineStateInfo" : {
        "properties" : {
          "name" : {
            "description" : "The user-friendly name of the pipeline.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "pipeline_id" : {
            "description" : "The unique identifier of the pipeline.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.PipelineState"
          },
          "latest_updates" : {
            "description" : "Status of the latest updates for the pipeline. Ordered with the newest update first.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/pipelines.UpdateStateInfo"
            }
          },
          "creator_user_name" : {
            "description" : "The username of the pipeline creator.",
            "type" : "string"
          },
          "run_as_user_name" : {
            "description" : "The username that the pipeline runs as. This is a read only value derived from the pipeline owner.",
            "type" : "string"
          },
          "cluster_id" : {
            "description" : "The unique identifier of the cluster running the pipeline.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "pipelines.PipelineTrigger" : {
        "properties" : {
          "cron" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.CronTrigger"
          },
          "manual" : {
            "description" : "",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.ManualTrigger"
          }
        },
        "type" : "object"
      },
      "pipelines.ResetPipelineResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "pipelines.StartUpdate" : {
        "properties" : {
          "cause" : {
            "description" : "",
            "type" : "string",
            "enum" : [ "API_CALL", "RETRY_ON_FAILURE", "SERVICE_UPGRADE", "SCHEMA_CHANGE", "JOB_TASK", "USER_ACTION" ]
          },
          "full_refresh" : {
            "description" : "If true, this update will reset all tables before running.",
            "type" : "boolean"
          },
          "full_refresh_selection" : {
            "description" : "A list of tables to update with fullRefresh. If both refresh_selection and\nfull_refresh_selection are empty, this is a full graph update. Full Refresh on a table means\nthat the states of the table will be reset before the refresh.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "refresh_selection" : {
            "description" : "A list of tables to update without fullRefresh. If both refresh_selection and\nfull_refresh_selection are empty, this is a full graph update. Full Refresh on a table means\nthat the states of the table will be reset before the refresh.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          }
        },
        "type" : "object"
      },
      "pipelines.StartUpdateResponse" : {
        "properties" : {
          "update_id" : {
            "description" : "",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "pipelines.StopPipelineResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "pipelines.UpdateInfo" : {
        "properties" : {
          "creation_time" : {
            "format" : "int64",
            "description" : "The time when this update was created.",
            "type" : "integer"
          },
          "pipeline_id" : {
            "description" : "The ID of the pipeline.",
            "type" : "string"
          },
          "state" : {
            "description" : "The update state.",
            "type" : "string",
            "enum" : [ "QUEUED", "CREATED", "WAITING_FOR_RESOURCES", "INITIALIZING", "RESETTING", "SETTING_UP_TABLES", "RUNNING", "STOPPING", "COMPLETED", "FAILED", "CANCELED" ]
          },
          "refresh_selection" : {
            "description" : "A list of tables to update without fullRefresh. If both refresh_selection and\nfull_refresh_selection are empty, this is a full graph update. Full Refresh on a table means\nthat the states of the table will be reset before the refresh.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "update_id" : {
            "description" : "The ID of this update.",
            "type" : "string"
          },
          "config" : {
            "description" : "The pipeline configuration with system defaults applied where unspecified by the user.\nNot returned by ListUpdates.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/pipelines.PipelineSpec"
          },
          "full_refresh_selection" : {
            "description" : "A list of tables to update with fullRefresh. If both refresh_selection and\nfull_refresh_selection are empty, this is a full graph update. Full Refresh on a table means\nthat the states of the table will be reset before the refresh.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "full_refresh" : {
            "description" : "If true, this update will reset all tables before running.",
            "type" : "boolean"
          },
          "cause" : {
            "description" : "What triggered this update.",
            "type" : "string",
            "enum" : [ "API_CALL", "RETRY_ON_FAILURE", "SERVICE_UPGRADE", "SCHEMA_CHANGE", "JOB_TASK", "USER_ACTION" ]
          },
          "cluster_id" : {
            "description" : "The ID of the cluster that the update is running on.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "pipelines.UpdateStateInfo" : {
        "properties" : {
          "creation_time" : {
            "description" : "",
            "type" : "string"
          },
          "state" : {
            "description" : "",
            "type" : "string",
            "enum" : [ "QUEUED", "CREATED", "WAITING_FOR_RESOURCES", "INITIALIZING", "RESETTING", "SETTING_UP_TABLES", "RUNNING", "STOPPING", "COMPLETED", "FAILED", "CANCELED" ]
          },
          "update_id" : {
            "description" : "",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "repos.CreateRepo" : {
        "required" : [ "url", "provider" ],
        "properties" : {
          "path" : {
            "example" : "/Repos/Production/testrepo",
            "description" : "Desired path for the repo in the workspace. Must be in the format /Repos/{folder}/{repo-name}.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "provider" : {
            "example" : "gitHub",
            "description" : "Git provider. This field is case-insensitive. The available Git providers are gitHub, bitbucketCloud, gitLab, azureDevOpsServices, gitHubEnterprise, bitbucketServer, gitLabEnterpriseEdition and awsCodeCommit.",
            "type" : "string"
          },
          "sparse_checkout" : {
            "description" : "If specified, the repo will be created with sparse checkout enabled. You cannot enable/disable sparse checkout after the repo is created.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/repos.SparseCheckout"
          },
          "url" : {
            "example" : "https://github.com/jsmith/test",
            "description" : "URL of the Git repository to be linked.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "repos.ListReposResponse" : {
        "properties" : {
          "next_page_token" : {
            "example" : "eyJyZXBvX3RyZWVub2RlX2lkIjo1MjQ5NjA4ODE0NTA5Mjc5fQ==",
            "description" : "Token that can be specified as a query parameter to the GET /repos endpoint to retrieve the next page of results.",
            "type" : "string"
          },
          "repos" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/repos.RepoInfo"
            }
          }
        },
        "type" : "object"
      },
      "repos.RepoInfo" : {
        "properties" : {
          "path" : {
            "example" : "/Repos/Production/testrepo",
            "description" : "Desired path for the repo in the workspace. Must be in the format /Repos/{folder}/{repo-name}.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "provider" : {
            "example" : "gitHub",
            "description" : "Git provider. This field is case-insensitive. The available Git providers are gitHub, bitbucketCloud, gitLab, azureDevOpsServices, gitHubEnterprise, bitbucketServer, gitLabEnterpriseEdition and awsCodeCommit.",
            "type" : "string"
          },
          "url" : {
            "example" : "https://github.com/jsmith/test",
            "description" : "URL of the Git repository to be linked.",
            "type" : "string"
          },
          "branch" : {
            "example" : "main",
            "description" : "Branch that the local version of the repo is checked out to.",
            "type" : "string"
          },
          "head_commit_id" : {
            "example" : "7e0847ede61f07adede22e2bcce6050216489171",
            "description" : "SHA-1 hash representing the commit ID of the current HEAD of the repo.",
            "type" : "string"
          },
          "id" : {
            "example" : 5249608814509279,
            "format" : "int64",
            "description" : "ID of the repo object in the workspace.",
            "x-databricks-id" : true,
            "type" : "integer"
          },
          "sparse_checkout" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/repos.SparseCheckout"
          }
        },
        "type" : "object"
      },
      "repos.SparseCheckout" : {
        "properties" : {
          "patterns" : {
            "description" : "List of patterns to include for sparse checkout.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/repos.sparse_checkout_pattern"
            }
          }
        },
        "type" : "object"
      },
      "repos.SparseCheckoutUpdate" : {
        "properties" : {
          "patterns" : {
            "description" : "List of patterns to include for sparse checkout.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/repos.sparse_checkout_pattern"
            }
          }
        },
        "type" : "object"
      },
      "repos.UpdateRepo" : {
        "properties" : {
          "branch" : {
            "example" : "main",
            "description" : "Branch that the local version of the repo is checked out to.",
            "type" : "string"
          },
          "sparse_checkout" : {
            "description" : "If specified, update the sparse checkout settings. The update will fail if sparse checkout is not enabled for the repo.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/repos.SparseCheckoutUpdate"
          },
          "tag" : {
            "example" : "v1.0",
            "description" : "Tag that the local version of the repo is checked out to. Updating the repo to a tag puts the repo in a detached HEAD state. Before committing new changes, you must update the repo to a branch instead of the detached HEAD.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "repos.sparse_checkout_pattern" : {
        "example" : "parent-folder/child-folder",
        "description" : "Sparse checkout cone pattern, see [cone mode handling](https://git-scm.com/docs/git-sparse-checkout#_internalscone_mode_handling) for details.",
        "type" : "string"
      },
      "scim.ComplexValue" : {
        "properties" : {
          "$ref" : {
            "type" : "string"
          },
          "type" : {
            "type" : "string"
          },
          "value" : {
            "type" : "string"
          },
          "primary" : {
            "type" : "boolean"
          },
          "display" : {
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "scim.Group" : {
        "properties" : {
          "displayName" : {
            "description" : "String that represents a human-readable group name",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "externalId" : {
            "type" : "string"
          },
          "groups" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ComplexValue"
            }
          },
          "id" : {
            "format" : "int64",
            "description" : "Databricks group ID",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "roles" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ComplexValue"
            }
          },
          "entitlements" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ComplexValue"
            }
          },
          "members" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ComplexValue"
            }
          }
        },
        "type" : "object"
      },
      "scim.ListGroupsResponse" : {
        "properties" : {
          "Resources" : {
            "description" : "User objects returned in the response.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.Group"
            }
          },
          "itemsPerPage" : {
            "format" : "int64",
            "description" : "Total results returned in the response.",
            "type" : "integer"
          },
          "startIndex" : {
            "default" : 1,
            "format" : "int64",
            "description" : "Starting index of all the results that matched the request filters. First item is number 1.",
            "type" : "integer"
          },
          "totalResults" : {
            "format" : "int64",
            "description" : "Total results that match the request filters.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "scim.ListServicePrincipalResponse" : {
        "properties" : {
          "Resources" : {
            "description" : "User objects returned in the response.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ServicePrincipal"
            }
          },
          "itemsPerPage" : {
            "format" : "int64",
            "description" : "Total results returned in the response.",
            "type" : "integer"
          },
          "startIndex" : {
            "default" : 1,
            "format" : "int64",
            "description" : "Starting index of all the results that matched the request filters. First item is number 1.",
            "type" : "integer"
          },
          "totalResults" : {
            "format" : "int64",
            "description" : "Total results that match the request filters.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "scim.ListUsersResponse" : {
        "properties" : {
          "Resources" : {
            "description" : "User objects returned in the response.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.User"
            }
          },
          "itemsPerPage" : {
            "format" : "int64",
            "description" : "Total results returned in the response.",
            "type" : "integer"
          },
          "startIndex" : {
            "default" : 1,
            "format" : "int64",
            "description" : "Starting index of all the results that matched the request filters. First item is number 1.",
            "type" : "integer"
          },
          "totalResults" : {
            "default" : 10,
            "format" : "int64",
            "description" : "Total results that match the request filters.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "scim.Name" : {
        "properties" : {
          "familyName" : {
            "description" : "Family name of the Databricks user.",
            "type" : "string"
          },
          "givenName" : {
            "description" : "Given name of the Databricks user.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "scim.PartialUpdate" : {
        "properties" : {
          "operations" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.Patch"
            }
          }
        },
        "type" : "object"
      },
      "scim.Patch" : {
        "properties" : {
          "op" : {
            "description" : "Type of patch operation.",
            "type" : "string",
            "enum" : [ "add", "remove", "replace" ]
          },
          "path" : {
            "description" : "Selection of patch operation",
            "type" : "string"
          },
          "value" : {
            "description" : "Value to modify",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "scim.ServicePrincipal" : {
        "properties" : {
          "displayName" : {
            "example" : "etl-service",
            "description" : "String that represents a concatenation of given and family names.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "externalId" : {
            "type" : "string"
          },
          "applicationId" : {
            "format" : "uuid",
            "description" : "UUID relating to the service principal",
            "type" : "string"
          },
          "groups" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ComplexValue"
            }
          },
          "id" : {
            "format" : "int64",
            "description" : "Databricks service principal ID.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "roles" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ComplexValue"
            }
          },
          "entitlements" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ComplexValue"
            }
          },
          "active" : {
            "description" : "If this user is active",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "scim.User" : {
        "properties" : {
          "displayName" : {
            "description" : "String that represents a concatenation of given and family names. For example `John Smith`.",
            "type" : "string"
          },
          "name" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/scim.Name"
          },
          "externalId" : {
            "type" : "string"
          },
          "groups" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ComplexValue"
            }
          },
          "id" : {
            "format" : "int64",
            "description" : "Databricks user ID.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "roles" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ComplexValue"
            }
          },
          "entitlements" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ComplexValue"
            }
          },
          "emails" : {
            "description" : "All the emails associated with the Databricks user.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/scim.ComplexValue"
            }
          },
          "userName" : {
            "format" : "email",
            "description" : "Email address of the Databricks user.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "active" : {
            "description" : "If this user is active",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "secrets.AclItem" : {
        "required" : [ "principal", "permission" ],
        "properties" : {
          "permission" : {
            "description" : "The permission level applied to the principal.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/secrets.AclPermission"
          },
          "principal" : {
            "description" : "The principal in which the permission is applied.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "secrets.AclPermission" : {
        "type" : "string",
        "enum" : [ "READ", "WRITE", "MANAGE" ]
      },
      "secrets.AzureKeyVaultSecretScopeMetadata" : {
        "required" : [ "resource_id", "dns_name" ],
        "properties" : {
          "dns_name" : {
            "description" : "The DNS of the KeyVault",
            "type" : "string"
          },
          "resource_id" : {
            "description" : "The resource id of the azure KeyVault that user wants to associate the scope with.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "secrets.CreateScope" : {
        "required" : [ "scope" ],
        "properties" : {
          "initial_manage_principal" : {
            "description" : "The principal that is initially granted `MANAGE` permission to the created scope.",
            "type" : "string"
          },
          "keyvault_metadata" : {
            "description" : "The metadata for the secret scope if the type is `AZURE_KEYVAULT`",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/secrets.AzureKeyVaultSecretScopeMetadata"
          },
          "scope" : {
            "description" : "Scope name requested by the user. Scope names are unique.",
            "type" : "string"
          },
          "scope_backend_type" : {
            "description" : "The backend type the scope will be created with. If not specified, will default to `DATABRICKS`",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/secrets.ScopeBackendType"
          }
        },
        "type" : "object"
      },
      "secrets.DeleteAcl" : {
        "required" : [ "scope", "principal" ],
        "properties" : {
          "principal" : {
            "description" : "The principal to remove an existing ACL from.",
            "type" : "string"
          },
          "scope" : {
            "description" : "The name of the scope to remove permissions from.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "secrets.DeleteScope" : {
        "required" : [ "scope" ],
        "properties" : {
          "scope" : {
            "description" : "Name of the scope to delete.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "secrets.DeleteSecret" : {
        "required" : [ "scope", "key" ],
        "properties" : {
          "key" : {
            "description" : "Name of the secret to delete.",
            "type" : "string"
          },
          "scope" : {
            "description" : "The name of the scope that contains the secret to delete.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "secrets.DeleteSecretResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "secrets.ListAclsResponse" : {
        "properties" : {
          "items" : {
            "description" : "The associated ACLs rule applied to principals in the given scope.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/secrets.AclItem"
            }
          }
        },
        "type" : "object"
      },
      "secrets.ListScopesResponse" : {
        "properties" : {
          "scopes" : {
            "description" : "The available secret scopes.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/secrets.SecretScope"
            }
          }
        },
        "type" : "object"
      },
      "secrets.ListSecretsResponse" : {
        "properties" : {
          "secrets" : {
            "description" : "Metadata information of all secrets contained within the given scope.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/secrets.SecretMetadata"
            }
          }
        },
        "type" : "object"
      },
      "secrets.PutAcl" : {
        "required" : [ "scope", "principal", "permission" ],
        "properties" : {
          "permission" : {
            "description" : "The permission level applied to the principal.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/secrets.AclPermission"
          },
          "principal" : {
            "description" : "The principal in which the permission is applied.",
            "type" : "string"
          },
          "scope" : {
            "description" : "The name of the scope to apply permissions to.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "secrets.PutSecret" : {
        "required" : [ "scope", "key" ],
        "properties" : {
          "bytes_value" : {
            "format" : "string",
            "description" : "If specified, value will be stored as bytes.",
            "type" : "string"
          },
          "key" : {
            "description" : "A unique name to identify the secret.",
            "type" : "string"
          },
          "scope" : {
            "description" : "The name of the scope to which the secret will be associated with.",
            "type" : "string"
          },
          "string_value" : {
            "description" : "If specified, note that the value will be stored in UTF-8 (MB4) form.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "secrets.ScopeBackendType" : {
        "type" : "string",
        "enum" : [ "DATABRICKS", "AZURE_KEYVAULT" ]
      },
      "secrets.SecretMetadata" : {
        "properties" : {
          "key" : {
            "description" : "A unique name to identify the secret.",
            "type" : "string"
          },
          "last_updated_timestamp" : {
            "format" : "int64",
            "description" : "The last updated timestamp (in milliseconds) for the secret.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "secrets.SecretScope" : {
        "properties" : {
          "backend_type" : {
            "description" : "The type of secret scope backend.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/secrets.ScopeBackendType"
          },
          "keyvault_metadata" : {
            "description" : "The metadata for the secret scope if the type is `AZURE_KEYVAULT`",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/secrets.AzureKeyVaultSecretScopeMetadata"
          },
          "name" : {
            "description" : "A unique name to identify the secret scope.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.AccessControl" : {
        "properties" : {
          "group_name" : {
            "example" : "admins",
            "type" : "string"
          },
          "permission_level" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.PermissionLevel"
          },
          "user_name" : {
            "format" : "email",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.Alert" : {
        "properties" : {
          "last_triggered_at" : {
            "format" : "date-time",
            "description" : "Timestamp when the alert was last triggered.",
            "type" : "string"
          },
          "parent" : {
            "example" : "folders/2025532471912059",
            "description" : "The identifier of the parent folder containing the alert. Available for alerts in workspace.",
            "type" : "string"
          },
          "name" : {
            "example" : "Total Trips > 300",
            "description" : "Name of the alert.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.alert_state"
          },
          "query" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Query"
          },
          "options" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.AlertOptions"
          },
          "id" : {
            "example" : "4e443c27-9f61-4f2e-a12d-ea5668460bf1",
            "format" : "UUID",
            "description" : "ID of the alert.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "rearm" : {
            "description" : "Number of seconds after being triggered before the alert rearms itself and can be triggered again. If `null`, alert will never be triggered again.",
            "type" : "integer"
          },
          "created_at" : {
            "format" : "date-time",
            "description" : "Timestamp when the alert was created.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "date-time",
            "description" : "Timestamp when the alert was last updated.",
            "type" : "string"
          },
          "user" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.User"
          }
        },
        "type" : "object"
      },
      "sql.AlertOptions" : {
        "required" : [ "column", "op", "value" ],
        "properties" : {
          "custom_subject" : {
            "example" : "The total trips has exceeded 300.",
            "description" : "Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See [here](https://docs.databricks.com/sql/user/alerts/index.html) for custom templating instructions.",
            "type" : "string"
          },
          "column" : {
            "example" : "total_trips",
            "description" : "Name of column in the query result to compare in alert evaluation.",
            "type" : "string"
          },
          "custom_body" : {
            "example" : "Total trips exceeded",
            "description" : "Custom body of alert notification, if it exists. See [here](https://docs.databricks.com/sql/user/alerts/index.html) for custom templating instructions.",
            "type" : "string"
          },
          "value" : {
            "example" : "300",
            "description" : "Value used to compare in alert evaluation.",
            "type" : "string"
          },
          "muted" : {
            "default" : false,
            "example" : false,
            "description" : "Whether or not the alert is muted. If an alert is muted, it will not notify users and alert destinations when triggered.",
            "type" : "boolean"
          },
          "op" : {
            "example" : ">",
            "description" : "Operator used to compare in alert evaluation: `>`, `>=`, `<`, `<=`, `==`, `!=`",
            "type" : "string"
          },
          "schedule_failures" : {
            "example" : 0,
            "description" : "Number of failures encountered during alert refresh. This counter is used for sending aggregated alert failure email notifications.",
            "type" : "integer"
          }
        },
        "description" : "Alert configuration options.",
        "type" : "object"
      },
      "sql.Channel" : {
        "properties" : {
          "dbsql_version" : {
            "description" : "",
            "type" : "string"
          },
          "name" : {
            "description" : "",
            "type" : "string",
            "enum" : [ "CHANNEL_NAME_UNSPECIFIED", "CHANNEL_NAME_PREVIEW", "CHANNEL_NAME_CURRENT", "CHANNEL_NAME_PREVIOUS", "CHANNEL_NAME_CUSTOM" ]
          }
        },
        "type" : "object"
      },
      "sql.ChannelInfo" : {
        "properties" : {
          "dbsql_version" : {
            "example" : "2022.30",
            "description" : "DBSQL Version the channel is mapped to",
            "type" : "string"
          },
          "name" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.ChannelName"
          }
        },
        "description" : "Channel information for the SQL warehouse at the time of query execution",
        "type" : "object"
      },
      "sql.ChannelName" : {
        "example" : "CHANNEL_NAME_CURRENT",
        "description" : "Name of the channel",
        "type" : "string",
        "enum" : [ "CHANNEL_NAME_PREVIEW", "CHANNEL_NAME_CURRENT", "CHANNEL_NAME_PREVIOUS", "CHANNEL_NAME_CUSTOM", "CHANNEL_NAME_UNSPECIFIED" ]
      },
      "sql.ChunkInfo" : {
        "properties" : {
          "byte_count" : {
            "format" : "int64",
            "description" : "Number of bytes in the result chunk.",
            "type" : "integer"
          },
          "next_chunk_internal_link" : {
            "description" : "When fetching, gives `internal_link` for the _next_ chunk; if absent, indicates there are no more chunks.",
            "type" : "string"
          },
          "row_count" : {
            "format" : "int64",
            "description" : "Number of rows within the result chunk.",
            "type" : "integer"
          },
          "next_chunk_index" : {
            "description" : "When fetching, gives `chunk_index` for the _next_ chunk; if absent, indicates there are no more chunks.",
            "type" : "integer"
          },
          "chunk_index" : {
            "description" : "Position within the sequence of result set chunks.",
            "type" : "integer"
          },
          "row_offset" : {
            "format" : "int64",
            "description" : "Starting row offset within the result set.",
            "type" : "integer"
          }
        },
        "description" : "Describes metadata for a particular chunk, within a result set; this structure is used both within a manifest,\nand when fetching individual chunk data or links.\n",
        "type" : "object"
      },
      "sql.ColumnInfo" : {
        "properties" : {
          "name" : {
            "description" : "Name of Column.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "type_interval_type" : {
            "description" : "Format of interval type.",
            "type" : "string"
          },
          "type_scale" : {
            "format" : "int32",
            "description" : "Digits to right of decimal.",
            "type" : "integer"
          },
          "type_text" : {
            "description" : "Full data type spec, SQL/catalogString text.",
            "type" : "string"
          },
          "position" : {
            "format" : "int32",
            "description" : "Ordinal position of column (starting at position 0).",
            "type" : "integer"
          },
          "type_name" : {
            "description" : "Name of type (INT, STRUCT, MAP, and so on)",
            "type" : "string",
            "enum" : [ "BOOLEAN", "BYTE", "SHORT", "INT", "LONG", "FLOAT", "DOUBLE", "DATE", "TIMESTAMP", "STRING", "BINARY", "DECIMAL", "INTERVAL", "ARRAY", "STRUCT", "MAP", "CHAR", "NULL", "USER_DEFINED_TYPE" ]
          },
          "type_precision" : {
            "format" : "int32",
            "description" : "Digits of precision.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "sql.CreateAlert" : {
        "required" : [ "name", "options", "query_id" ],
        "properties" : {
          "parent" : {
            "default" : "folders/HOME",
            "example" : "folders/2025532471912059",
            "description" : "The identifier of the workspace folder containing the alert. The default is ther user's home folder.",
            "type" : "string"
          },
          "name" : {
            "example" : "Total Trips > 300",
            "description" : "Name of the alert.",
            "type" : "string"
          },
          "options" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.AlertOptions"
          },
          "rearm" : {
            "description" : "Number of seconds after being triggered before the alert rearms itself and can be triggered again. If `null`, alert will never be triggered again.",
            "type" : "integer"
          },
          "query_id" : {
            "example" : "dee5cca8-1c79-4b5e-a711-e7f9d241bdf6",
            "format" : "UUID",
            "description" : "ID of the query evaluated by the alert.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.CreateRefreshSchedule" : {
        "required" : [ "cron" ],
        "properties" : {
          "cron" : {
            "example" : "0 0 1 * *",
            "format" : "cron",
            "description" : "Cron string representing the refresh schedule.",
            "type" : "string"
          },
          "data_source_id" : {
            "example" : "f7df1dfd-565d-4506-accb-8a1e0f8fad09",
            "format" : "UUID",
            "description" : "ID of the SQL warehouse to refresh with. If `null`, query's SQL warehouse will be used to refresh.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.CreateSubscription" : {
        "required" : [ "alert_id" ],
        "properties" : {
          "alert_id" : {
            "example" : "4e443c27-9f61-4f2e-a12d-ea5668460bf1",
            "format" : "UUID",
            "description" : "ID of the alert.",
            "type" : "string"
          },
          "destination_id" : {
            "format" : "UUID",
            "description" : "ID of the alert subscriber (if subscribing an alert destination). Alert destinations can be configured by admins through the UI. See [here](/sql/admin/alert-destinations.html).",
            "type" : "string"
          },
          "user_id" : {
            "example" : 899619779028913,
            "format" : "int64",
            "description" : "ID of the alert subscriber (if subscribing a user).",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "sql.CreateWarehouseRequest" : {
        "properties" : {
          "enable_serverless_compute" : {
            "description" : "Configures whether the endpoint should use Serverless Compute (aka Nephos)\n\nDefaults to value in global endpoint settings",
            "type" : "boolean"
          },
          "spot_instance_policy" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.SpotInstancePolicy"
          },
          "name" : {
            "description" : "Logical name for the cluster.\n\nSupported values:\n  - Must be unique within an org.\n  - Must be less than 100 characters.",
            "type" : "string"
          },
          "channel" : {
            "description" : "Channel Details",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Channel"
          },
          "max_num_clusters" : {
            "format" : "int32",
            "description" : "Maximum number of clusters that the autoscaler will create to handle concurrent queries.\n\nSupported values:\n  - Must be >= min_num_clusters\n  - Must be <= 30.\n\nDefaults to min_clusters if unset.",
            "type" : "integer"
          },
          "tags" : {
            "description" : "A set of key-value pairs that will be tagged on all resources (e.g., AWS instances and EBS volumes) associated\nwith this SQL Endpoints.\n\nSupported values:\n  - Number of tags < 45.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.EndpointTags"
          },
          "min_num_clusters" : {
            "default" : "1",
            "format" : "int32",
            "description" : "Minimum number of available clusters that will be maintained for this SQL Endpoint.\nIncreasing this will ensure that a larger number of clusters are always running and therefore may reduce\nthe cold start time for new queries. This is similar to reserved vs. revocable cores in a resource manager.\n\nSupported values:\n  - Must be > 0\n  - Must be <= min(max_num_clusters, 30)\n\nDefaults to 1",
            "type" : "integer"
          },
          "instance_profile_arn" : {
            "description" : "Deprecated. Instance profile used to pass IAM role to the cluster",
            "deprecated" : true,
            "type" : "string"
          },
          "warehouse_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.WarehouseType"
          },
          "auto_stop_mins" : {
            "default" : "120",
            "format" : "int32",
            "description" : "The amount of time in minutes that a SQL Endpoint must be idle (i.e., no RUNNING queries) before\nit is automatically stopped.\n\nSupported values:\n  - Must be == 0 or >= 10 mins\n  - 0 indicates no autostop.\n\nDefaults to 120 mins",
            "type" : "integer"
          },
          "cluster_size" : {
            "description" : "Size of the clusters allocated for this endpoint.\nIncreasing the size of a spark cluster allows you to run larger queries on it.\nIf you want to increase the number of concurrent queries, please tune max_num_clusters.\n\nSupported values:\n  - 2X-Small\n  - X-Small\n  - Small\n  - Medium\n  - Large\n  - X-Large\n  - 2X-Large\n  - 3X-Large\n  - 4X-Large\n",
            "type" : "string"
          },
          "creator_name" : {
            "description" : "endpoint creator name",
            "type" : "string"
          },
          "enable_photon" : {
            "description" : "Configures whether the endpoint should use Photon optimized clusters.\n\nDefaults to false.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "sql.CreateWarehouseResponse" : {
        "properties" : {
          "id" : {
            "description" : "Id for the SQL warehouse.\nThis value is unique across all SQL warehouses.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.Dashboard" : {
        "properties" : {
          "parent" : {
            "example" : "folders/2025532471912059",
            "description" : "The identifier of the parent folder containing the dashboard. Available for dashboards in workspace.",
            "type" : "string"
          },
          "name" : {
            "example" : "Sales Dashboard",
            "description" : "The title of the dashboard that appears in list views and at the top of the dashboard page.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "permission_tier" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.PermissionLevel"
          },
          "tags" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.TagString"
            }
          },
          "is_favorite" : {
            "description" : "Indicates whether this query object appears in the current user's favorites list. This flag determines whether the star icon for favorites is selected.",
            "type" : "boolean"
          },
          "slug" : {
            "example" : "sales-dashboard",
            "description" : "URL slug. Usually mirrors the query name with dashes (`-`) instead of spaces. Appears in the URL for this query.",
            "type" : "string"
          },
          "user_id" : {
            "example" : 7878537523656153,
            "description" : "The ID of the user that created and owns this dashboard.",
            "type" : "integer"
          },
          "options" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.DashboardOptions"
          },
          "dashboard_filters_enabled" : {
            "default" : false,
            "example" : false,
            "description" : "In the web application, query filters that share a name are coupled to a single selection box if this value is `true`.",
            "type" : "boolean"
          },
          "id" : {
            "example" : "4e443c27-9f61-4f2e-a12d-ea5668460bf1",
            "format" : "UUID",
            "description" : "The ID for this dashboard.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "is_draft" : {
            "description" : "Whether a dashboard is a draft. Draft dashboards only appear in list views for their owners.",
            "type" : "boolean"
          },
          "created_at" : {
            "format" : "date-time",
            "description" : "Timestamp when this dashboard was created.",
            "type" : "string"
          },
          "can_edit" : {
            "example" : true,
            "description" : "Whether the authenticated user can edit the query definition.",
            "type" : "boolean"
          },
          "widgets" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.Widget"
            }
          },
          "updated_at" : {
            "format" : "date-time",
            "description" : "Timestamp when this dashboard was last updated.",
            "type" : "string"
          },
          "user" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.User"
          },
          "is_archived" : {
            "description" : "Indicates whether a dashboard is trashed. Trashed dashboards won't appear in list views.  If this boolean is `true`, the `options` property for this dashboard includes a `moved_to_trash_at` timestamp. Items in trash are permanently deleted after 30 days.",
            "type" : "boolean"
          }
        },
        "description" : "A JSON representing a dashboard containing widgets of visualizations and text boxes.",
        "type" : "object"
      },
      "sql.DashboardOptions" : {
        "properties" : {
          "moved_to_trash_at" : {
            "example" : "2020-12-15T18:39:15.837Z",
            "format" : "date-time",
            "description" : "The timestamp when this dashboard was moved to trash. Only present when the `is_archived` property is `true`. Trashed items are deleted after thirty days.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.DataSource" : {
        "properties" : {
          "name" : {
            "example" : "starter-warehouse",
            "description" : "The string name of this data source / SQL warehouse as it appears in the Databricks SQL web application.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "supports_auto_limit" : {
            "example" : true,
            "description" : "Reserved for internal use.",
            "type" : "boolean"
          },
          "view_only" : {
            "example" : false,
            "description" : "Reserved for internal use.",
            "type" : "boolean"
          },
          "syntax" : {
            "example" : "sql",
            "description" : "Reserved for internal use.",
            "type" : "string"
          },
          "id" : {
            "example" : "f7df1dfd-565d-4506-accb-8a1e0f8fad09",
            "format" : "UUID",
            "description" : "The unique identifier for this data source / SQL warehouse. Can be used when creating / modifying queries and dashboards.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "paused" : {
            "example" : 0,
            "description" : "Reserved for internal use.",
            "type" : "integer"
          },
          "type" : {
            "example" : "databricks_internal",
            "description" : "The type of data source. For SQL warehouses, this will be `databricks_internal`.",
            "type" : "string"
          },
          "pause_reason" : {
            "description" : "Reserved for internal use.",
            "type" : "string"
          },
          "warehouse_id" : {
            "example" : "3d939b0cc668be06",
            "description" : "The ID of the associated SQL warehouse, if this data source is backed by a SQL warehouse.",
            "type" : "string"
          }
        },
        "description" : "A JSON object representing a DBSQL data source / SQL warehouse.",
        "type" : "object"
      },
      "sql.DeleteWarehouseResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "sql.Destination" : {
        "properties" : {
          "id" : {
            "example" : "1c2836cc-7838-4eca-b560-ac7f494d3c92",
            "format" : "UUID",
            "description" : "ID of the alert destination.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "name" : {
            "example" : "Slack Notifier",
            "description" : "Name of the alert destination.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "type" : {
            "example" : "slack",
            "description" : "Type of the alert destination.",
            "type" : "string",
            "enum" : [ "email", "slack", "webhook", "microsoft_teams", "pagerduty", "hangouts_chat", "mattermost" ]
          }
        },
        "description" : "Alert destination subscribed to the alert, if it exists. Alert destinations can be configured by admins through the UI. See [here](https://docs.databricks.com/sql/admin/alert-destinations.html).",
        "type" : "object"
      },
      "sql.Disposition" : {
        "description" : "The fetch disposition provides for two modes of fetching results: INLINE, and EXTERNAL_LINKS.\n\nStatements executed with INLINE disposition will return result data inline, in JSON_ARRAY format, in a series\nof chunks. INLINE disposition result sets are constrained to 4 MiB (megabytes) of total\ndata, and will typically be split into chunks of <= 4 MiB per chunk. If a given\nstatement produces a result set with a size larger than 16 MiB, that statement\nexecution is aborted, and no result set will be available.\n\n**NOTE**\nByte limits are computed based upon internal representations of the result set data, and may not match the\nsizes visible in JSON responses.\n\nStatements executed with EXTERNAL_LINKS disposition will return result data as external links: URLs that point\nto cloud storage within the workspace's configured DBFS. Using EXTERNAL_LINKS disposition allows statements to\ngenerate arbitrarily sized result sets for fetching. The resulting links have two important properties:\n\n1. They point to resources _external_ to the Databricks compute; therefore any associated authentication\n   information (typically a PAT token, OAuth token, or similar) _must be removed_ when fetching from these links.\n\n2. These are presigned URLs with a specific expiration, indicated in the response. The behavior when\n   attempting to use an expired link is cloud specific.\n",
        "type" : "string",
        "enum" : [ "INLINE", "EXTERNAL_LINKS" ]
      },
      "sql.EditAlert" : {
        "required" : [ "name", "options", "query_id" ],
        "properties" : {
          "name" : {
            "example" : "Total Trips > 300",
            "description" : "Name of the alert.",
            "type" : "string"
          },
          "options" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.AlertOptions"
          },
          "query_id" : {
            "example" : "dee5cca8-1c79-4b5e-a711-e7f9d241bdf6",
            "format" : "UUID",
            "description" : "ID of the query evaluated by the alert.",
            "type" : "string"
          },
          "rearm" : {
            "description" : "Number of seconds after being triggered before the alert rearms itself and can be triggered again. If `null`, alert will never be triggered again.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "sql.EditWarehouseRequest" : {
        "properties" : {
          "enable_databricks_compute" : {
            "description" : "Configures whether the endpoint should use Databricks Compute (aka Nephos)\n\nDeprecated: Use enable_serverless_compute\n",
            "deprecated" : true,
            "type" : "boolean"
          },
          "enable_serverless_compute" : {
            "description" : "Configures whether the endpoint should use Serverless Compute (aka Nephos)\n\nDefaults to value in global endpoint settings",
            "type" : "boolean"
          },
          "spot_instance_policy" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.SpotInstancePolicy"
          },
          "name" : {
            "description" : "Logical name for the cluster.\n\nSupported values:\n  - Must be unique within an org.\n  - Must be less than 100 characters.",
            "type" : "string"
          },
          "channel" : {
            "description" : "Channel Details",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Channel"
          },
          "max_num_clusters" : {
            "format" : "int32",
            "description" : "Maximum number of clusters that the autoscaler will create to handle concurrent queries.\n\nSupported values:\n  - Must be >= min_num_clusters\n  - Must be <= 30.\n\nDefaults to min_clusters if unset.",
            "type" : "integer"
          },
          "tags" : {
            "description" : "A set of key-value pairs that will be tagged on all resources (e.g., AWS instances and EBS volumes) associated\nwith this SQL Endpoints.\n\nSupported values:\n  - Number of tags < 45.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.EndpointTags"
          },
          "min_num_clusters" : {
            "default" : "1",
            "format" : "int32",
            "description" : "Minimum number of available clusters that will be maintained for this SQL Endpoint.\nIncreasing this will ensure that a larger number of clusters are always running and therefore may reduce\nthe cold start time for new queries. This is similar to reserved vs. revocable cores in a resource manager.\n\nSupported values:\n  - Must be > 0\n  - Must be <= min(max_num_clusters, 30)\n\nDefaults to 1",
            "type" : "integer"
          },
          "instance_profile_arn" : {
            "description" : "Deprecated. Instance profile used to pass IAM role to the cluster",
            "deprecated" : true,
            "type" : "string"
          },
          "warehouse_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.WarehouseType"
          },
          "auto_stop_mins" : {
            "default" : "120",
            "format" : "int32",
            "description" : "The amount of time in minutes that a SQL Endpoint must be idle (i.e., no RUNNING queries) before\nit is automatically stopped.\n\nSupported values:\n  - Must be == 0 or >= 10 mins\n  - 0 indicates no autostop.\n\nDefaults to 120 mins",
            "type" : "integer"
          },
          "cluster_size" : {
            "description" : "Size of the clusters allocated for this endpoint.\nIncreasing the size of a spark cluster allows you to run larger queries on it.\nIf you want to increase the number of concurrent queries, please tune max_num_clusters.\n\nSupported values:\n- 2X-Small\n- X-Small\n- Small\n- Medium\n- Large\n- X-Large\n- 2X-Large\n- 3X-Large\n- 4X-Large",
            "type" : "string"
          },
          "creator_name" : {
            "description" : "endpoint creator name",
            "type" : "string"
          },
          "enable_photon" : {
            "description" : "Configures whether the endpoint should use Photon optimized clusters.\n\nDefaults to false.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "sql.EditWarehouseResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "sql.EndpointConfPair" : {
        "properties" : {
          "key" : {
            "description" : "",
            "type" : "string"
          },
          "value" : {
            "description" : "",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.EndpointHealth" : {
        "properties" : {
          "failure_reason" : {
            "description" : "The reason for failure to bring up clusters for this endpoint. This is available when\nstatus is 'FAILED' and sometimes when it is DEGRADED.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.TerminationReason"
          },
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Status"
          },
          "details" : {
            "description" : "Details about errors that are causing current degraded/failed status.",
            "type" : "string"
          },
          "message" : {
            "description" : "Deprecated. split into summary and details for security",
            "deprecated" : true,
            "type" : "string"
          },
          "summary" : {
            "description" : "A short summary of the health status in case of degraded/failed endpoints.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.EndpointInfo" : {
        "properties" : {
          "health" : {
            "description" : "Optional health status. Assume the endpoint is healthy if this field is not set.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.EndpointHealth"
          },
          "enable_databricks_compute" : {
            "description" : "Configures whether the endpoint should use Databricks Compute (aka Nephos)\n\nDeprecated: Use enable_serverless_compute\n",
            "deprecated" : true,
            "type" : "boolean"
          },
          "enable_serverless_compute" : {
            "description" : "Configures whether the endpoint should use Serverless Compute (aka Nephos)\n\nDefaults to value in global endpoint settings",
            "type" : "boolean"
          },
          "spot_instance_policy" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.SpotInstancePolicy"
          },
          "name" : {
            "description" : "Logical name for the cluster.\n\nSupported values:\n  - Must be unique within an org.\n  - Must be less than 100 characters.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "channel" : {
            "description" : "Channel Details",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Channel"
          },
          "num_clusters" : {
            "format" : "int32",
            "description" : "current number of clusters running for the service",
            "type" : "integer"
          },
          "state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.State"
          },
          "jdbc_url" : {
            "description" : "the jdbc connection string for this endpoint",
            "type" : "string"
          },
          "max_num_clusters" : {
            "format" : "int32",
            "description" : "Maximum number of clusters that the autoscaler will create to handle concurrent queries.\n\nSupported values:\n  - Must be >= min_num_clusters\n  - Must be <= 30.\n\nDefaults to min_clusters if unset.",
            "type" : "integer"
          },
          "tags" : {
            "description" : "A set of key-value pairs that will be tagged on all resources (e.g., AWS instances and EBS volumes) associated\nwith this SQL Endpoints.\n\nSupported values:\n  - Number of tags < 45.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.EndpointTags"
          },
          "min_num_clusters" : {
            "default" : "1",
            "format" : "int32",
            "description" : "Minimum number of available clusters that will be maintained for this SQL Endpoint.\nIncreasing this will ensure that a larger number of clusters are always running and therefore may reduce\nthe cold start time for new queries. This is similar to reserved vs. revocable cores in a resource manager.\n\nSupported values:\n  - Must be > 0\n  - Must be <= min(max_num_clusters, 30)\n\nDefaults to 1",
            "type" : "integer"
          },
          "instance_profile_arn" : {
            "description" : "Deprecated. Instance profile used to pass IAM role to the cluster",
            "deprecated" : true,
            "type" : "string"
          },
          "warehouse_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.WarehouseType"
          },
          "id" : {
            "description" : "unique identifier for endpoint",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "auto_stop_mins" : {
            "default" : "120",
            "format" : "int32",
            "description" : "The amount of time in minutes that a SQL Endpoint must be idle (i.e., no RUNNING queries) before\nit is automatically stopped.\n\nSupported values:\n  - Must be == 0 or >= 10 mins\n  - 0 indicates no autostop.\n\nDefaults to 120 mins",
            "type" : "integer"
          },
          "cluster_size" : {
            "description" : "Size of the clusters allocated for this endpoint.\nIncreasing the size of a spark cluster allows you to run larger queries on it.\nIf you want to increase the number of concurrent queries, please tune max_num_clusters.\n\nSupported values:\n- 2X-Small\n- X-Small\n- Small\n- Medium\n- Large\n- X-Large\n- 2X-Large\n- 3X-Large\n- 4X-Large",
            "type" : "string"
          },
          "num_active_sessions" : {
            "format" : "int64",
            "description" : "current number of active sessions for the endpoint",
            "type" : "integer"
          },
          "creator_name" : {
            "description" : "endpoint creator name",
            "type" : "string"
          },
          "enable_photon" : {
            "description" : "Configures whether the endpoint should use Photon optimized clusters.\n\nDefaults to false.",
            "type" : "boolean"
          },
          "odbc_params" : {
            "description" : "ODBC parameters for the sql endpoint",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.OdbcParams"
          }
        },
        "type" : "object"
      },
      "sql.EndpointTagPair" : {
        "properties" : {
          "key" : {
            "description" : "",
            "type" : "string"
          },
          "value" : {
            "description" : "",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.EndpointTags" : {
        "properties" : {
          "custom_tags" : {
            "description" : "",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.EndpointTagPair"
            }
          }
        },
        "type" : "object"
      },
      "sql.ExecuteStatementRequest" : {
        "properties" : {
          "format" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Format"
          },
          "catalog" : {
            "description" : "Sets default catalog for statement execution, similar to [`USE CATALOG`](https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-use-catalog.html) in SQL.\n",
            "type" : "string"
          },
          "wait_timeout" : {
            "description" : "Time that API service will wait statement result, in format '{N}s'. N may be '0s' for asynchronous, or may\nwait between 5-50 seconds.\"\n",
            "type" : "string"
          },
          "disposition" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Disposition"
          },
          "statement" : {
            "example" : "SELECT * FROM range(10)",
            "description" : "SQL statement to execute",
            "type" : "string"
          },
          "on_wait_timeout" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.TimeoutAction"
          },
          "schema" : {
            "description" : "Sets default schema for statement execution, similar to [`USE SCHEMA`](https://docs.databricks.com/sql/language-manual/sql-ref-syntax-ddl-use-schema.html) in SQL.\n",
            "type" : "string"
          },
          "warehouse_id" : {
            "description" : "Warehouse upon which to execute a statement. See also [What are SQL warehouses?](/sql/admin/warehouse-type.html)\n",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.ExternalLink" : {
        "properties" : {
          "byte_count" : {
            "format" : "int64",
            "description" : "Number of bytes in the result chunk.",
            "type" : "integer"
          },
          "external_link" : {
            "description" : "Pre-signed URL pointing to a chunk of result data, hosted by an external service, with a short expiration\ntime (< 1 hour).\n",
            "type" : "string"
          },
          "next_chunk_internal_link" : {
            "description" : "When fetching, gives `internal_link` for the _next_ chunk; if absent, indicates there are no more chunks.",
            "type" : "string"
          },
          "row_count" : {
            "format" : "int64",
            "description" : "Number of rows within the result chunk.",
            "type" : "integer"
          },
          "next_chunk_index" : {
            "description" : "When fetching, gives `chunk_index` for the _next_ chunk; if absent, indicates there are no more chunks.",
            "type" : "integer"
          },
          "expiration" : {
            "format" : "date-time",
            "description" : "Indicates date-time that the given external link will expire and become invalid, after which point a new\nexternal_link must be requested.\n",
            "type" : "string"
          },
          "chunk_index" : {
            "description" : "Position within the sequence of result set chunks.",
            "type" : "integer"
          },
          "row_offset" : {
            "format" : "int64",
            "description" : "Starting row offset within the result set.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "sql.Format" : {
        "description" : "Statement execution supports two result formats: `JSON_ARRAY` (default), and `ARROW_STREAM`.\n\n**NOTE**\n\nCurrently `JSON_ARRAY` is only available for requests with `disposition=INLINE`, and `ARROW_STREAM` is only\navailable for requests with `disposition=EXTERNAL_LINKS`.\n\nWhen specifying `format=JSON_ARRAY`, result data will be formatted as arrays of arrays of values, where each\nvalue is either the *string representation* of a value, or `null`. For example, the output of `SELECT\nconcat('id-', id) AS strId, id AS intId FROM range(3)` would look like this:\n\n```\n[\n  [ \"id-1\", \"1\" ],\n  [ \"id-2\", \"2\" ],\n  [ \"id-3\", \"3\" ],\n]\n```\n\nINLINE JSON_ARRAY data can be found within `StatementResponse.result.chunk.data_array` or\n`ResultData.chunk.data_array`.\n\nWhen specifying `format=ARROW_STREAM`, results fetched through `ResultData.external_links` will be chunks of\nresult data, formatted as Apache Arrow Stream. See\n[https://arrow.apache.org/docs/format/Columnar.html#ipc-streaming-format] for more details.\n",
        "type" : "string",
        "enum" : [ "JSON_ARRAY", "ARROW_STREAM" ]
      },
      "sql.GetWarehouseResponse" : {
        "properties" : {
          "health" : {
            "description" : "Optional health status. Assume the endpoint is healthy if this field is not set.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.EndpointHealth"
          },
          "enable_databricks_compute" : {
            "description" : "Configures whether the endpoint should use Databricks Compute (aka Nephos)\n\nDeprecated: Use enable_serverless_compute\n",
            "deprecated" : true,
            "type" : "boolean"
          },
          "enable_serverless_compute" : {
            "description" : "Configures whether the endpoint should use Serverless Compute (aka Nephos)\n\nDefaults to value in global endpoint settings",
            "type" : "boolean"
          },
          "spot_instance_policy" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.SpotInstancePolicy"
          },
          "name" : {
            "description" : "Logical name for the cluster.\n\nSupported values:\n  - Must be unique within an org.\n  - Must be less than 100 characters.",
            "type" : "string"
          },
          "channel" : {
            "description" : "Channel Details",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Channel"
          },
          "num_clusters" : {
            "format" : "int32",
            "description" : "current number of clusters running for the service",
            "type" : "integer"
          },
          "state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.State"
          },
          "jdbc_url" : {
            "description" : "the jdbc connection string for this endpoint",
            "type" : "string"
          },
          "max_num_clusters" : {
            "format" : "int32",
            "description" : "Maximum number of clusters that the autoscaler will create to handle concurrent queries.\n\nSupported values:\n  - Must be >= min_num_clusters\n  - Must be <= 30.\n\nDefaults to min_clusters if unset.",
            "type" : "integer"
          },
          "tags" : {
            "description" : "A set of key-value pairs that will be tagged on all resources (e.g., AWS instances and EBS volumes) associated\nwith this SQL Endpoints.\n\nSupported values:\n  - Number of tags < 45.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.EndpointTags"
          },
          "min_num_clusters" : {
            "default" : "1",
            "format" : "int32",
            "description" : "Minimum number of available clusters that will be maintained for this SQL Endpoint.\nIncreasing this will ensure that a larger number of clusters are always running and therefore may reduce\nthe cold start time for new queries. This is similar to reserved vs. revocable cores in a resource manager.\n\nSupported values:\n  - Must be > 0\n  - Must be <= min(max_num_clusters, 30)\n\nDefaults to 1",
            "type" : "integer"
          },
          "instance_profile_arn" : {
            "description" : "Deprecated. Instance profile used to pass IAM role to the cluster",
            "deprecated" : true,
            "type" : "string"
          },
          "warehouse_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.WarehouseType"
          },
          "id" : {
            "description" : "unique identifier for endpoint",
            "type" : "string"
          },
          "auto_stop_mins" : {
            "default" : "120",
            "format" : "int32",
            "description" : "The amount of time in minutes that a SQL Endpoint must be idle (i.e., no RUNNING queries) before\nit is automatically stopped.\n\nSupported values:\n  - Must be == 0 or >= 10 mins\n  - 0 indicates no autostop.\n\nDefaults to 120 mins",
            "type" : "integer"
          },
          "cluster_size" : {
            "description" : "Size of the clusters allocated for this endpoint.\nIncreasing the size of a spark cluster allows you to run larger queries on it.\nIf you want to increase the number of concurrent queries, please tune max_num_clusters.\n\nSupported values:\n- 2X-Small\n- X-Small\n- Small\n- Medium\n- Large\n- X-Large\n- 2X-Large\n- 3X-Large\n- 4X-Large",
            "type" : "string"
          },
          "num_active_sessions" : {
            "format" : "int64",
            "description" : "current number of active sessions for the endpoint",
            "type" : "integer"
          },
          "creator_name" : {
            "description" : "endpoint creator name",
            "type" : "string"
          },
          "enable_photon" : {
            "description" : "Configures whether the endpoint should use Photon optimized clusters.\n\nDefaults to false.",
            "type" : "boolean"
          },
          "odbc_params" : {
            "description" : "ODBC parameters for the sql endpoint",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.OdbcParams"
          }
        },
        "type" : "object"
      },
      "sql.GetWorkspaceWarehouseConfigResponse" : {
        "properties" : {
          "enable_databricks_compute" : {
            "description" : "Enable Serverless compute for SQL Endpoints\n\nDeprecated: Use enable_serverless_compute\n",
            "deprecated" : true,
            "type" : "boolean"
          },
          "enable_serverless_compute" : {
            "description" : "Enable Serverless compute for SQL Endpoints",
            "type" : "boolean"
          },
          "security_policy" : {
            "description" : "Security policy for endpoints",
            "type" : "string",
            "enum" : [ "NONE", "DATA_ACCESS_CONTROL", "PASSTHROUGH" ]
          },
          "channel" : {
            "description" : "Optional: Channel selection details",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Channel"
          },
          "google_service_account" : {
            "description" : "GCP only: Google Service Account used to pass to cluster to access Google Cloud Storage",
            "type" : "string"
          },
          "enabled_warehouse_types" : {
            "description" : "List of Warehouse Types allowed in this workspace (limits allowed value of the\ntype field in CreateWarehouse and EditWarehouse).\nNote: Some types cannot be disabled, they don't need to be specified\n    in SetWorkspaceWarehouseConfig.\nNote: Disabling a type may cause existing warehouses to be converted to another type.\nUsed by frontend to save specific type availability in the warehouse create and edit form UI.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.WarehouseTypePair"
            }
          },
          "data_access_config" : {
            "description" : "Spark confs for external hive metastore configuration\nJSON serialized size must be less than <= 512K",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.EndpointConfPair"
            }
          },
          "global_param" : {
            "description" : "Deprecated: Use sql_configuration_parameters",
            "deprecated" : true,
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.RepeatedEndpointConfPairs"
          },
          "config_param" : {
            "description" : "Deprecated: Use sql_configuration_parameters",
            "deprecated" : true,
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.RepeatedEndpointConfPairs"
          },
          "sql_configuration_parameters" : {
            "description" : "SQL configuration parameters",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.RepeatedEndpointConfPairs"
          },
          "instance_profile_arn" : {
            "description" : "AWS Only: Instance profile used to pass IAM role to the cluster",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.JsonArray" : {
        "description" : "JSON_ARRAY format is an array of arrays of values, where each non-null value is formatted as a string. Null\nvalues are encoded as JSON `null`.\n",
        "type" : "array",
        "items" : {
          "type" : "array",
          "items" : {
            "description" : "Non-null values are string encoded, null values are encoded as `null`.",
            "type" : "string"
          }
        }
      },
      "sql.ListQueriesResponse" : {
        "properties" : {
          "has_next_page" : {
            "example" : true,
            "description" : "Whether there is another page of results.",
            "type" : "boolean"
          },
          "next_page_token" : {
            "example" : "Ci0KJDU4NjEwZjY5LTgzNzUtNDdiMS04YTg1LWYxNTU5ODI5MDYyMhDdobu",
            "description" : "A token that can be used to get the next page of results.",
            "type" : "string"
          },
          "res" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.QueryInfo"
            }
          }
        },
        "type" : "object"
      },
      "sql.ListWarehousesResponse" : {
        "properties" : {
          "warehouses" : {
            "description" : "A list of warehouses and their configurations.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.EndpointInfo"
            }
          }
        },
        "type" : "object"
      },
      "sql.OdbcParams" : {
        "properties" : {
          "hostname" : {
            "description" : "",
            "type" : "string"
          },
          "path" : {
            "description" : "",
            "type" : "string"
          },
          "port" : {
            "format" : "int32",
            "description" : "",
            "type" : "integer"
          },
          "protocol" : {
            "description" : "",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.Parameter" : {
        "properties" : {
          "name" : {
            "description" : "The literal parameter marker that appears between double curly braces in the query text.",
            "type" : "string"
          },
          "title" : {
            "description" : "The text displayed in a parameter picking widget.",
            "type" : "string"
          },
          "type" : {
            "description" : "Parameters can have several different types.",
            "type" : "string",
            "enum" : [ "text", "datetime", "number" ]
          },
          "value" : {
            "properties" : { },
            "description" : "The default value for this parameter.",
            "type" : "object"
          }
        },
        "type" : "object"
      },
      "sql.PermissionLevel" : {
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "CAN_VIEW", "CAN_RUN", "CAN_MANAGE" ],
        "x-databricks-enum-descriptions" : {
          "CAN_VIEW" : "Can view the query",
          "CAN_RUN" : "Can run the query",
          "CAN_MANAGE" : "Can manage the query"
        }
      },
      "sql.PlansState" : {
        "example" : "EXISTS",
        "description" : "Whether plans exist for the execution, or the reason why they are missing",
        "type" : "string",
        "enum" : [ "IGNORED_SMALL_DURATION", "IGNORED_LARGE_PLANS_SIZE", "EXISTS", "UNKNOWN", "EMPTY", "IGNORED_SPARK_PLAN_TYPE" ]
      },
      "sql.Query" : {
        "properties" : {
          "query_hash" : {
            "example" : "08314a3361c4795528acd1139a2b7c58",
            "description" : "A SHA-256 hash of the query text along with the authenticated user ID.",
            "type" : "string"
          },
          "parent" : {
            "example" : "folders/2025532471912059",
            "description" : "The identifier of the parent folder containing the query. Available for queries in workspace.",
            "type" : "string"
          },
          "name" : {
            "example" : "Orders by month by customer",
            "description" : "The title of this query that appears in list views, widget headings, and on the query page.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "permission_tier" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.PermissionLevel"
          },
          "description" : {
            "example" : "Summarizes total order dollars for customers in the Europe/Asia region.",
            "description" : "General description that conveys additional information about this query such as usage notes.",
            "type" : "string"
          },
          "tags" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.TagString"
            }
          },
          "last_modified_by" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.User"
          },
          "visualizations" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.Visualization"
            }
          },
          "latest_query_data_id" : {
            "example" : "ab281b1a-527a-4789-bcf7-cfc3ba08d910",
            "format" : "uuid",
            "description" : "If there is a cached result for this query and user, this field includes the query result ID. If this query uses parameters, this field is always null.",
            "type" : "string"
          },
          "is_favorite" : {
            "example" : true,
            "description" : "Whether this query object appears in the current user's favorites list. This flag determines whether the star icon for favorites is selected.",
            "type" : "boolean"
          },
          "query" : {
            "example" : "SELECT field FROM table WHERE field = {{ param }}",
            "description" : "The text of the query to be run.",
            "type" : "string"
          },
          "data_source_id" : {
            "example" : "0c205e24-5db2-4940-adb1-fb13c7ce960b",
            "format" : "UUID",
            "description" : "Data Source ID. The UUID that uniquely identifies this data source / SQL warehouse across the API.",
            "type" : "string"
          },
          "user_id" : {
            "example" : 899619779028913,
            "description" : "The ID of the user who created this query.",
            "type" : "integer"
          },
          "options" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.QueryOptions"
          },
          "id" : {
            "example" : "dee5cca8-1c79-4b5e-a711-e7f9d241bdf6",
            "format" : "uuid",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "is_draft" : {
            "description" : "Whether the query is a draft. Draft queries only appear in list views for their owners. Visualizations from draft queries cannot appear on dashboards.",
            "type" : "boolean"
          },
          "is_safe" : {
            "example" : false,
            "description" : "Text parameter types are not safe from SQL injection for all types of data source. Set this Boolean parameter to `true` if a query either does not use any text type parameters or uses a data source type where text type parameters are handled safely.",
            "type" : "boolean"
          },
          "schedule" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.QueryInterval"
          },
          "created_at" : {
            "example" : "2020-12-15T18:39:15.837Z",
            "format" : "date-time",
            "description" : "The timestamp when this query was created.",
            "type" : "string"
          },
          "can_edit" : {
            "example" : true,
            "description" : "Describes whether the authenticated user is allowed to edit the definition of this query.",
            "type" : "boolean"
          },
          "updated_at" : {
            "example" : "2021-01-07T20:29:24.289Z",
            "format" : "date-time",
            "description" : "The timestamp at which this query was last updated.",
            "type" : "string"
          },
          "user" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.User"
          },
          "is_archived" : {
            "description" : "Indicates whether the query is trashed. Trashed queries can't be used in dashboards, or appear in search results. If this boolean is `true`, the `options` property for this query includes a `moved_to_trash_at` timestamp. Trashed queries are permanently deleted after 30 days.",
            "type" : "boolean"
          },
          "last_modified_by_id" : {
            "example" : 899619779028913,
            "description" : "The ID of the user who last saved changes to this query.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "sql.QueryEditContent" : {
        "properties" : {
          "name" : {
            "example" : "Orders by month by customer",
            "description" : "The name or title of this query to display in list views.",
            "type" : "string"
          },
          "description" : {
            "example" : "Summarizes total order dollars for customers in the Europe/Asia region.",
            "description" : "General description that can convey additional information about this query such as usage notes.",
            "type" : "string"
          },
          "query" : {
            "example" : "SELECT field FROM table WHERE field = {{ param }}",
            "description" : "The text of the query.",
            "type" : "string"
          },
          "data_source_id" : {
            "example" : "2cca1687-60ff-4886-a445-0230578c864d",
            "format" : "UUID",
            "description" : "The ID of the data source / SQL warehouse where this query will run.",
            "type" : "string"
          },
          "options" : {
            "example" : {
              "parameters" : [ {
                "name" : "param",
                "title" : "customer",
                "type" : "text",
                "value" : "acme"
              } ]
            },
            "properties" : { },
            "description" : "Exclusively used for storing a list parameter definitions. A parameter is an object with `title`, `name`, `type`, and `value` properties. The `value` field here is the default value. It can be overridden at runtime.",
            "type" : "object"
          },
          "schedule" : {
            "example" : {
              "day_of_week" : "Wednesday",
              "interval" : 86400,
              "time" : "06:15",
              "until" : "1991-08-03"
            },
            "description" : "JSON object that describes the scheduled execution frequency. A schedule object includes `interval`, `time`, `day_of_week`, and `until` fields. If a scheduled is supplied, then only `interval` is required. All other field can be `null`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.QueryInterval"
          }
        },
        "type" : "object"
      },
      "sql.QueryFilter" : {
        "properties" : {
          "query_start_time_range" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.TimeRange"
          },
          "statuses" : {
            "example" : [ "FINISHED", "FAILED" ],
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.QueryStatus"
            }
          },
          "user_ids" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.user_ids"
          },
          "warehouse_ids" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.warehouse_ids"
          }
        },
        "description" : "A filter to limit query history results. This field is optional.",
        "type" : "object"
      },
      "sql.QueryInfo" : {
        "properties" : {
          "channel_used" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.ChannelInfo"
          },
          "rows_produced" : {
            "example" : 100,
            "description" : "The number of results returned by the query.",
            "type" : "integer"
          },
          "duration" : {
            "example" : 1000,
            "description" : "Total execution time of the query from the clients point of view, in milliseconds.",
            "type" : "integer"
          },
          "query_end_time_ms" : {
            "example" : 1595357087200,
            "description" : "The time the query ended.",
            "type" : "integer"
          },
          "lookup_key" : {
            "example" : "CiQ3OGFkYmQ2Zi00ZGUwLTRlNTYtOTkxZC05Y2I5OTNlZTViYjcQ4N6r/dguGhBlM2VlYTVlOTExMjFkMzNjILPbh9OK6uoL",
            "description" : "A key that can be used to look up query details.",
            "type" : "string"
          },
          "execution_end_time_ms" : {
            "example" : 1595357086373,
            "description" : "The time execution of the query ended.",
            "type" : "integer"
          },
          "plans_state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.PlansState"
          },
          "user_id" : {
            "example" : 1234567890123456,
            "description" : "The ID of the user who ran the query.",
            "type" : "integer"
          },
          "query_id" : {
            "example" : "f996b47c-6672-4763-9668-d491a82099f5",
            "description" : "The query ID.",
            "type" : "string"
          },
          "status" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.QueryStatus"
          },
          "executed_as_user_id" : {
            "example" : 1234567890123456,
            "description" : "The ID of the user whose credentials were used to run the query.",
            "type" : "integer"
          },
          "is_final" : {
            "example" : true,
            "description" : "Whether more updates for the query are expected.",
            "type" : "boolean"
          },
          "spark_ui_url" : {
            "example" : "https://<databricks-instance>/sparkui/1234-567890-test123/driver-1234567890123456789/SQL/execution/?id=0",
            "description" : "URL to the query plan.",
            "type" : "string"
          },
          "error_message" : {
            "example" : "Table or view not found: customers; line 1 pos 14;\n'GlobalLimit 1000\n+- 'LocalLimit 1000\n   +- 'Project [*]\n      +- 'UnresolvedRelation [sales]\n",
            "description" : "Message describing why the query could not complete.",
            "type" : "string"
          },
          "metrics" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.QueryMetrics"
          },
          "query_start_time_ms" : {
            "example" : 1595357086200,
            "description" : "The time the query started.",
            "type" : "integer"
          },
          "query_text" : {
            "example" : "SELECT * FROM customers;",
            "description" : "The text of the query.",
            "type" : "string"
          },
          "user_name" : {
            "example" : "user@example.com",
            "description" : "The email address or username of the user who ran the query.",
            "type" : "string"
          },
          "endpoint_id" : {
            "example" : "098765321fedcba",
            "description" : "Alias for `warehouse_id`.",
            "deprecated" : true,
            "type" : "string"
          },
          "warehouse_id" : {
            "example" : "098765321fedcba",
            "description" : "Warehouse ID.",
            "type" : "string"
          },
          "executed_as_user_name" : {
            "example" : "user@example.com",
            "description" : "The email address or username of the user whose credentials were used to run the query.",
            "type" : "string"
          },
          "statement_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.QueryStatementType"
          }
        },
        "type" : "object"
      },
      "sql.QueryInterval" : {
        "properties" : {
          "day_of_week" : {
            "example" : "Wednesday",
            "description" : "For weekly runs, the day of the week to start the run.",
            "type" : "string"
          },
          "interval" : {
            "example" : 900,
            "description" : "Integer number of seconds between runs.",
            "type" : "integer"
          },
          "time" : {
            "example" : "00:15",
            "description" : "For daily, weekly, and monthly runs, the time of day to start the run.",
            "type" : "string"
          },
          "until" : {
            "example" : "2021-01-07",
            "description" : "A date after which this schedule no longer applies.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.QueryList" : {
        "properties" : {
          "count" : {
            "example" : 1,
            "description" : "The total number of queries.",
            "type" : "integer"
          },
          "page" : {
            "example" : 1,
            "description" : "The page number that is currently displayed.",
            "type" : "integer"
          },
          "page_size" : {
            "example" : 25,
            "description" : "The number of queries per page.",
            "type" : "integer"
          },
          "results" : {
            "description" : "List of queries returned.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.Query"
            }
          }
        },
        "type" : "object"
      },
      "sql.QueryMetrics" : {
        "properties" : {
          "read_bytes" : {
            "example" : 1024,
            "description" : "Total size of data read by the query, in bytes.",
            "type" : "integer"
          },
          "total_files_count" : {
            "example" : 10,
            "description" : "Number of files that would have been read without pruning.",
            "type" : "integer"
          },
          "queued_provisioning_time_ms" : {
            "example" : 1000,
            "description" : "Time waiting for compute resources to be provisioned for the SQL warehouse, in milliseconds.",
            "type" : "integer"
          },
          "write_remote_bytes" : {
            "example" : 1024,
            "description" : "Size pf persistent data written to cloud object storage in your cloud tenant, in bytes.",
            "type" : "integer"
          },
          "result_from_cache" : {
            "example" : false,
            "description" : "true if the query result was fetched from cache, false otherwise.",
            "type" : "boolean"
          },
          "execution_time_ms" : {
            "example" : 1000,
            "description" : "Time spent executing the query, in milliseconds.",
            "type" : "integer"
          },
          "total_time_ms" : {
            "example" : 1000,
            "description" : "Total execution time of the query from the clients point of view, in milliseconds.",
            "type" : "integer"
          },
          "rows_produced_count" : {
            "example" : 100000,
            "description" : "Total number of rows returned by the query.",
            "type" : "integer"
          },
          "photon_total_time_ms" : {
            "example" : 1000,
            "description" : "Total execution time for all individual Photon query engine tasks in the query, in milliseconds.",
            "type" : "integer"
          },
          "rows_read_count" : {
            "example" : 10000,
            "description" : "Total number of rows read by the query.",
            "type" : "integer"
          },
          "task_total_time_ms" : {
            "example" : 100000,
            "description" : "Sum of execution time for all of the querys tasks, in milliseconds.",
            "type" : "integer"
          },
          "read_remote_bytes" : {
            "example" : 1024,
            "description" : "Size of persistent data read from cloud object storage on your cloud tenant, in bytes.",
            "type" : "integer"
          },
          "read_cache_bytes" : {
            "example" : 1024,
            "description" : "Size of persistent data read from the cache, in bytes.",
            "type" : "integer"
          },
          "queued_overload_time_ms" : {
            "example" : 1000,
            "description" : "Time spent waiting to execute the query because the SQL warehouse is already running the maximum number of concurrent queries, in milliseconds.",
            "type" : "integer"
          },
          "result_fetch_time_ms" : {
            "example" : 100000,
            "description" : "Time spent fetching the query results after the execution finished, in milliseconds.",
            "type" : "integer"
          },
          "spill_to_disk_bytes" : {
            "example" : 1024,
            "description" : "Size of data temporarily written to disk while executing the query, in bytes.",
            "type" : "integer"
          },
          "compilation_time_ms" : {
            "example" : 1000,
            "description" : "Time spent loading metadata and optimizing the query, in milliseconds.",
            "type" : "integer"
          },
          "read_files_count" : {
            "example" : 1,
            "description" : "Number of files read after pruning.",
            "type" : "integer"
          },
          "network_sent_bytes" : {
            "example" : 1024,
            "description" : "Total amount of data sent over the network between executor nodes during shuffle, in bytes.",
            "type" : "integer"
          },
          "total_partitions_count" : {
            "example" : 10,
            "description" : "Number of partitions that would have been read without pruning.",
            "type" : "integer"
          },
          "read_partitions_count" : {
            "example" : 1,
            "description" : "Number of partitions read after pruning.",
            "type" : "integer"
          }
        },
        "description" : "Metrics about query execution.",
        "type" : "object"
      },
      "sql.QueryOptions" : {
        "properties" : {
          "moved_to_trash_at" : {
            "example" : "2020-12-15T18:39:15.837Z",
            "format" : "date-time",
            "description" : "The timestamp when this query was moved to trash. Only present when the `is_archived` property is `true`. Trashed items are deleted after thirty days.",
            "type" : "string"
          },
          "parameters" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.Parameter"
            }
          }
        },
        "type" : "object"
      },
      "sql.QueryPostContent" : {
        "properties" : {
          "parent" : {
            "default" : "folders/HOME",
            "example" : "folders/2025532471912059",
            "description" : "The identifier of the workspace folder containing the query. The default is the user's home folder.",
            "type" : "string"
          },
          "name" : {
            "example" : "Orders by month by customer",
            "description" : "The name or title of this query to display in list views.",
            "type" : "string"
          },
          "description" : {
            "example" : "Summarizes total order dollars for customers in the Europe/Asia region.",
            "description" : "General description that can convey additional information about this query such as usage notes.",
            "type" : "string"
          },
          "query" : {
            "example" : "SELECT field FROM table WHERE field = {{ param }}",
            "description" : "The text of the query.",
            "type" : "string"
          },
          "data_source_id" : {
            "example" : "2cca1687-60ff-4886-a445-0230578c864d",
            "format" : "UUID",
            "description" : "The ID of the data source / SQL warehouse where this query will run.",
            "type" : "string"
          },
          "options" : {
            "example" : {
              "parameters" : [ {
                "name" : "param",
                "title" : "customer",
                "type" : "text",
                "value" : "acme"
              } ]
            },
            "properties" : { },
            "description" : "Exclusively used for storing a list parameter definitions. A parameter is an object with `title`, `name`, `type`, and `value` properties. The `value` field here is the default value. It can be overridden at runtime.",
            "type" : "object"
          },
          "schedule" : {
            "example" : {
              "day_of_week" : "Wednesday",
              "interval" : 86400,
              "time" : "06:15",
              "until" : "1991-08-03"
            },
            "description" : "JSON object that describes the scheduled execution frequency. A schedule object includes `interval`, `time`, `day_of_week`, and `until` fields. If a scheduled is supplied, then only `interval` is required. All other field can be `null`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.QueryInterval"
          }
        },
        "type" : "object"
      },
      "sql.QueryStatementType" : {
        "example" : "SELECT",
        "description" : "Type of statement for this query",
        "type" : "string",
        "enum" : [ "OTHER", "ALTER", "ANALYZE", "COPY", "CREATE", "DELETE", "DESCRIBE", "DROP", "EXPLAIN", "GRANT", "INSERT", "MERGE", "OPTIMIZE", "REFRESH", "REPLACE", "REVOKE", "SELECT", "SET", "SHOW", "TRUNCATE", "UPDATE", "USE" ]
      },
      "sql.QueryStatus" : {
        "example" : "FINISHED",
        "description" : "This describes an enum",
        "type" : "string",
        "enum" : [ "QUEUED", "RUNNING", "CANCELED", "FAILED", "FINISHED" ],
        "x-databricks-enum-descriptions" : {
          "RUNNING" : "Query has started.",
          "QUEUED" : "Query has been received and queued.",
          "CANCELED" : "Query has been cancelled by the user.",
          "FAILED" : "Query has failed.",
          "FINISHED" : "Query has completed."
        }
      },
      "sql.RefreshSchedule" : {
        "properties" : {
          "cron" : {
            "example" : "0 0 1 * *",
            "format" : "cron",
            "description" : "Cron string representing the refresh schedule.",
            "type" : "string"
          },
          "data_source_id" : {
            "example" : "f7df1dfd-565d-4506-accb-8a1e0f8fad09",
            "format" : "UUID",
            "description" : "ID of the SQL warehouse to refresh with. If `null`, query's SQL warehouse will be used to refresh.",
            "type" : "string"
          },
          "id" : {
            "example" : "54bfe473-91d9-4991-987b-823277d68525",
            "format" : "UUID",
            "description" : "ID of the refresh schedule.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.RepeatedEndpointConfPairs" : {
        "properties" : {
          "config_pair" : {
            "description" : "Deprecated: Use configuration_pairs",
            "deprecated" : true,
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.EndpointConfPair"
            }
          },
          "configuration_pairs" : {
            "description" : "",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.EndpointConfPair"
            }
          }
        },
        "type" : "object"
      },
      "sql.ResultData" : {
        "properties" : {
          "byte_count" : {
            "format" : "int64",
            "description" : "Number of bytes in the result chunk.",
            "type" : "integer"
          },
          "next_chunk_internal_link" : {
            "description" : "When fetching, gives `internal_link` for the _next_ chunk; if absent, indicates there are no more chunks.",
            "type" : "string"
          },
          "row_count" : {
            "format" : "int64",
            "description" : "Number of rows within the result chunk.",
            "type" : "integer"
          },
          "external_links" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.ExternalLink"
            }
          },
          "next_chunk_index" : {
            "description" : "When fetching, gives `chunk_index` for the _next_ chunk; if absent, indicates there are no more chunks.",
            "type" : "integer"
          },
          "data_array" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.JsonArray"
          },
          "chunk_index" : {
            "description" : "Position within the sequence of result set chunks.",
            "type" : "integer"
          },
          "row_offset" : {
            "format" : "int64",
            "description" : "Starting row offset within the result set.",
            "type" : "integer"
          }
        },
        "description" : "Result data chunks are delivered in either the `chunk` field when using INLINE disposition,\nor in the `external_link` field when using EXTERNAL_LINKS disposition. Exactly one of these\nwill be set.\n",
        "type" : "object"
      },
      "sql.ResultManifest" : {
        "properties" : {
          "format" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Format"
          },
          "total_chunk_count" : {
            "description" : "Total number of chunks that the result set has been divided into.",
            "type" : "integer"
          },
          "chunks" : {
            "description" : "Array of result set chunk metadata.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.ChunkInfo"
            }
          },
          "schema" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.ResultSchema"
          },
          "total_byte_count" : {
            "format" : "int64",
            "description" : "Total number of bytes in the result set.",
            "type" : "integer"
          },
          "total_row_count" : {
            "format" : "int64",
            "description" : "Total number of rows in the result set.",
            "type" : "integer"
          }
        },
        "description" : "The result manifest provides schema and metadata for the result set.",
        "type" : "object"
      },
      "sql.ResultSchema" : {
        "properties" : {
          "column_count" : {
            "type" : "integer"
          },
          "columns" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.ColumnInfo"
            }
          }
        },
        "description" : "Schema is an ordered list of column descriptions.",
        "type" : "object"
      },
      "sql.ServiceError" : {
        "properties" : {
          "error_code" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.ServiceErrorCode"
          },
          "message" : {
            "description" : "Brief summary of error condition.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.ServiceErrorCode" : {
        "type" : "string",
        "enum" : [ "UNKNOWN", "INTERNAL_ERROR", "TEMPORARILY_UNAVAILABLE", "IO_ERROR", "BAD_REQUEST", "SERVICE_UNDER_MAINTENANCE", "WORKSPACE_TEMPORARILY_UNAVAILABLE", "DEADLINE_EXCEEDED", "CANCELLED", "RESOURCE_EXHAUSTED", "ABORTED", "NOT_FOUND", "ALREADY_EXISTS", "UNAUTHENTICATED" ]
      },
      "sql.SetWorkspaceWarehouseConfigRequest" : {
        "properties" : {
          "enable_databricks_compute" : {
            "description" : "Enable Serverless compute for SQL Endpoints\n\nDeprecated: Use enable_serverless_compute\n",
            "deprecated" : true,
            "type" : "boolean"
          },
          "enable_serverless_compute" : {
            "description" : "Enable Serverless compute for SQL Endpoints",
            "type" : "boolean"
          },
          "security_policy" : {
            "description" : "Security policy for endpoints",
            "type" : "string",
            "enum" : [ "NONE", "DATA_ACCESS_CONTROL", "PASSTHROUGH" ]
          },
          "channel" : {
            "description" : "Optional: Channel selection details",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Channel"
          },
          "google_service_account" : {
            "description" : "GCP only: Google Service Account used to pass to cluster to access Google Cloud Storage",
            "type" : "string"
          },
          "enabled_warehouse_types" : {
            "description" : "List of Warehouse Types allowed in this workspace (limits allowed value of the\ntype field in CreateWarehouse and EditWarehouse).\nNote: Some types cannot be disabled, they don't need to be specified\n    in SetWorkspaceWarehouseConfig.\nNote: Disabling a type may cause existing warehouses to be converted to another type.\nUsed by frontend to save specific type availability in the warehouse create and edit form UI.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.WarehouseTypePair"
            }
          },
          "data_access_config" : {
            "description" : "Spark confs for external hive metastore configuration\nJSON serialized size must be less than <= 512K",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/sql.EndpointConfPair"
            }
          },
          "global_param" : {
            "description" : "Deprecated: Use sql_configuration_parameters",
            "deprecated" : true,
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.RepeatedEndpointConfPairs"
          },
          "config_param" : {
            "description" : "Deprecated: Use sql_configuration_parameters",
            "deprecated" : true,
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.RepeatedEndpointConfPairs"
          },
          "sql_configuration_parameters" : {
            "description" : "SQL configuration parameters",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.RepeatedEndpointConfPairs"
          },
          "instance_profile_arn" : {
            "description" : "AWS Only: Instance profile used to pass IAM role to the cluster",
            "type" : "string"
          },
          "serverless_agreement" : {
            "description" : "Internal. Used by frontend to save Serverless Compute agreement value.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "sql.SetWorkspaceWarehouseConfigResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "sql.SpotInstancePolicy" : {
        "default" : "COST_OPTIMIZED",
        "description" : "Configurations whether the warehouse should use spot instances.",
        "type" : "string",
        "enum" : [ "POLICY_UNSPECIFIED", "COST_OPTIMIZED", "RELIABILITY_OPTIMIZED" ]
      },
      "sql.StartWarehouseResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "sql.State" : {
        "description" : "State of the warehouse",
        "type" : "string",
        "enum" : [ "STARTING", "RUNNING", "STOPPING", "STOPPED", "DELETING", "DELETED" ]
      },
      "sql.StatementState" : {
        "description" : "Statement execution state:\n- `PENDING`: waiting for warehouse\n- `RUNNING`: running\n- `SUCCEEDED`: execution was successful, result data available for fetch\n- `FAILED`: execution failed; reason for failure described in accomanying error message\n- `CANCELED`: user canceled; can come from explicit cancel call, or timeout with `on_wait_timeout=CANCEL`\n- `CLOSED`: execution successful, and statement closed; result no longer available for fetch\n",
        "type" : "string",
        "enum" : [ "PENDING", "RUNNING", "SUCCEEDED", "FAILED", "CANCELED", "CLOSED" ]
      },
      "sql.StatementStatus" : {
        "properties" : {
          "error" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.ServiceError"
          },
          "state" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.StatementState"
          }
        },
        "description" : "Status response includes execution state and if relevant, error information.",
        "type" : "object"
      },
      "sql.Status" : {
        "description" : "Health status of the endpoint.",
        "type" : "string",
        "enum" : [ "STATUS_UNSPECIFIED", "HEALTHY", "DEGRADED", "FAILED" ]
      },
      "sql.StopWarehouseResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "sql.Subscription" : {
        "properties" : {
          "alert_id" : {
            "example" : "4e443c27-9f61-4f2e-a12d-ea5668460bf1",
            "format" : "UUID",
            "description" : "ID of the alert.",
            "type" : "string"
          },
          "destination" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Destination"
          },
          "id" : {
            "example" : "41a4e45e-6f04-4f91-a502-384ad9ae0bd7",
            "format" : "UUID",
            "description" : "ID of the alert subscription.",
            "type" : "string"
          },
          "user" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.User"
          }
        },
        "type" : "object"
      },
      "sql.Success" : {
        "properties" : {
          "message" : {
            "type" : "string",
            "enum" : [ "Success" ]
          }
        },
        "type" : "object"
      },
      "sql.TagString" : {
        "example" : "Payroll",
        "description" : "Tags can be applied to dashboards and queries. They are used for filtering list views.",
        "type" : "string"
      },
      "sql.TerminationReason" : {
        "properties" : {
          "code" : {
            "description" : "status code indicating why the cluster was terminated",
            "type" : "string",
            "enum" : [ "UNKNOWN", "USER_REQUEST", "JOB_FINISHED", "INACTIVITY", "CLOUD_PROVIDER_SHUTDOWN", "COMMUNICATION_LOST", "CLOUD_PROVIDER_LAUNCH_FAILURE", "INIT_SCRIPT_FAILURE", "SPARK_STARTUP_FAILURE", "INVALID_ARGUMENT", "UNEXPECTED_LAUNCH_FAILURE", "INTERNAL_ERROR", "INSTANCE_UNREACHABLE", "REQUEST_REJECTED", "TRIAL_EXPIRED", "DRIVER_UNREACHABLE", "SPARK_ERROR", "DRIVER_UNRESPONSIVE", "METASTORE_COMPONENT_UNHEALTHY", "DBFS_COMPONENT_UNHEALTHY", "EXECUTION_COMPONENT_UNHEALTHY", "AZURE_RESOURCE_MANAGER_THROTTLING", "AZURE_RESOURCE_PROVIDER_THROTTLING", "NETWORK_CONFIGURATION_FAILURE", "CONTAINER_LAUNCH_FAILURE", "INSTANCE_POOL_CLUSTER_FAILURE", "SKIPPED_SLOW_NODES", "ATTACH_PROJECT_FAILURE", "UPDATE_INSTANCE_PROFILE_FAILURE", "DATABASE_CONNECTION_FAILURE", "REQUEST_THROTTLED", "SELF_BOOTSTRAP_FAILURE", "GLOBAL_INIT_SCRIPT_FAILURE", "SLOW_IMAGE_DOWNLOAD", "INVALID_SPARK_IMAGE", "NPIP_TUNNEL_TOKEN_FAILURE", "HIVE_METASTORE_PROVISIONING_FAILURE", "AZURE_INVALID_DEPLOYMENT_TEMPLATE", "AZURE_UNEXPECTED_DEPLOYMENT_TEMPLATE_FAILURE", "SUBNET_EXHAUSTED_FAILURE", "BOOTSTRAP_TIMEOUT", "STORAGE_DOWNLOAD_FAILURE", "CONTROL_PLANE_REQUEST_FAILURE", "BOOTSTRAP_TIMEOUT_CLOUD_PROVIDER_EXCEPTION", "AWS_INSUFFICIENT_INSTANCE_CAPACITY_FAILURE", "DOCKER_IMAGE_PULL_FAILURE", "AZURE_VNET_CONFIGURATION_FAILURE", "NPIP_TUNNEL_SETUP_FAILURE", "AWS_AUTHORIZATION_FAILURE", "NEPHOS_RESOURCE_MANAGEMENT", "STS_CLIENT_SETUP_FAILURE", "SECURITY_DAEMON_REGISTRATION_EXCEPTION", "AWS_REQUEST_LIMIT_EXCEEDED", "AWS_INSUFFICIENT_FREE_ADDRESSES_IN_SUBNET_FAILURE", "AWS_UNSUPPORTED_FAILURE", "AZURE_QUOTA_EXCEEDED_EXCEPTION", "AZURE_OPERATION_NOT_ALLOWED_EXCEPTION", "NFS_MOUNT_FAILURE", "K8S_AUTOSCALING_FAILURE", "K8S_DBR_CLUSTER_LAUNCH_TIMEOUT", "SPARK_IMAGE_DOWNLOAD_FAILURE", "AZURE_VM_EXTENSION_FAILURE", "WORKSPACE_CANCELLED_ERROR", "AWS_MAX_SPOT_INSTANCE_COUNT_EXCEEDED_FAILURE", "TEMPORARILY_UNAVAILABLE", "WORKER_SETUP_FAILURE", "IP_EXHAUSTION_FAILURE", "GCP_QUOTA_EXCEEDED", "CLOUD_PROVIDER_RESOURCE_STOCKOUT", "GCP_SERVICE_ACCOUNT_DELETED", "AZURE_BYOK_KEY_PERMISSION_FAILURE", "SPOT_INSTANCE_TERMINATION", "AZURE_EPHEMERAL_DISK_FAILURE", "ABUSE_DETECTED", "IMAGE_PULL_PERMISSION_DENIED", "WORKSPACE_CONFIGURATION_ERROR", "SECRET_RESOLUTION_ERROR", "UNSUPPORTED_INSTANCE_TYPE", "CLOUD_PROVIDER_DISK_SETUP_FAILURE" ]
          },
          "parameters" : {
            "properties" : { },
            "description" : "list of parameters that provide additional information about why the cluster was terminated",
            "type" : "object",
            "additionalProperties" : {
              "description" : "",
              "type" : "string"
            }
          },
          "type" : {
            "description" : "type of the termination",
            "type" : "string",
            "enum" : [ "SUCCESS", "CLIENT_ERROR", "SERVICE_FAULT", "CLOUD_FAILURE" ]
          }
        },
        "type" : "object"
      },
      "sql.TimeRange" : {
        "properties" : {
          "end_time_ms" : {
            "example" : 1595357086500,
            "description" : "Limit results to queries that started before this time.",
            "type" : "integer"
          },
          "start_time_ms" : {
            "example" : 1595357086200,
            "description" : "Limit results to queries that started after this time.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "sql.TimeoutAction" : {
        "description" : "When called in synchronous mode (`wait_timeout > 0s`), determines action when timeout reached:\n\n`CONTINUE`  statement execution continues asynchronously; call returns immediately.\n`CANCEL`  statement execution canceled; call returns immediately with `CANCELED` state.\n",
        "type" : "string",
        "enum" : [ "CONTINUE", "CANCEL" ]
      },
      "sql.User" : {
        "properties" : {
          "name" : {
            "example" : "user@example.com",
            "type" : "string"
          },
          "email" : {
            "example" : "user@example.com",
            "format" : "email",
            "type" : "string"
          },
          "profile_image_url" : {
            "example" : "https://www.gravatar.com/avatar/732f2b4824846f9a0253f233e0d79bf0?s=40&d=identicon",
            "format" : "url",
            "description" : "The URL for the gravatar profile picture tied to this user's email address.",
            "type" : "string"
          },
          "id" : {
            "example" : 899619779028913,
            "type" : "integer"
          },
          "is_db_admin" : {
            "example" : true,
            "description" : "Whether this user is an admin in the Databricks workspace.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "sql.Visualization" : {
        "properties" : {
          "name" : {
            "example" : "Chart",
            "description" : "The name of the visualization that appears on dashboards and the query screen.",
            "type" : "string"
          },
          "description" : {
            "example" : "A table of data from this query.",
            "description" : "A short description of this visualization. This is not displayed in the UI.",
            "type" : "string"
          },
          "options" : {
            "properties" : { },
            "description" : "The options object varies widely from one visualization type to the next and is unsupported. Databricks does not recommend modifying visualization settings in JSON.",
            "type" : "object"
          },
          "id" : {
            "description" : "The UUID for this visualization.",
            "type" : "string"
          },
          "created_at" : {
            "format" : "date-time",
            "type" : "string"
          },
          "type" : {
            "example" : "TABLE",
            "description" : "The type of visualization: chart, table, pivot table, and so on.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "date-time",
            "type" : "string"
          }
        },
        "description" : "The visualization description API changes frequently and is unsupported. You can duplicate a visualization by copying description objects received _from the API_ and then using them to create a new one with a POST request to the same endpoint. Databricks does not recommend constructing ad-hoc visualizations entirely in JSON.",
        "type" : "object"
      },
      "sql.WarehouseType" : {
        "type" : "string",
        "enum" : [ "TYPE_UNSPECIFIED", "CLASSIC", "PRO" ]
      },
      "sql.WarehouseTypePair" : {
        "properties" : {
          "enabled" : {
            "description" : "If set to false the specific warehouse type will not be be allowed as a value\nfor warehouse_type in CreateWarehouse and EditWarehouse",
            "type" : "boolean"
          },
          "warehouse_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.WarehouseType"
          }
        },
        "type" : "object"
      },
      "sql.Widget" : {
        "properties" : {
          "id" : {
            "example" : 11536,
            "description" : "The unique ID for this widget.",
            "type" : "integer"
          },
          "options" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.WidgetOptions"
          },
          "visualization" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/sql.Visualization"
          },
          "width" : {
            "example" : 1,
            "description" : "Unused field.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "sql.WidgetOptions" : {
        "properties" : {
          "dashboard_id" : {
            "example" : "a6d3a7a9-693d-472f-96b2-912622c2ffcd",
            "format" : "UUID",
            "description" : "The dashboard ID to which this widget belongs. Each widget can belong to one dashboard.",
            "type" : "string"
          },
          "isHidden" : {
            "default" : false,
            "example" : false,
            "description" : "Whether this widget is hidden on the dashboard.",
            "type" : "boolean"
          },
          "text" : {
            "description" : "If this is a textbox widget, the application displays this text. This field is ignored if the widget contains a visualization in the `visualization` field.",
            "type" : "string"
          },
          "parameterMappings" : {
            "example" : {
              "param" : {
                "name" : "param",
                "mapTo" : "param",
                "title" : "This is a parameter",
                "type" : "dashboard-level",
                "value" : null
              }
            },
            "properties" : { },
            "description" : "How parameters used by the visualization in this widget relate to other widgets on the dashboard. Databricks does not recommend modifying this definition in JSON.",
            "type" : "object"
          },
          "position" : {
            "properties" : { },
            "description" : "Coordinates of this widget on a dashboard. This portion of the API changes frequently and is unsupported.",
            "type" : "object"
          },
          "created_at" : {
            "format" : "date-time",
            "description" : "Timestamp when this object was created",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "date-time",
            "description" : "Timestamp of the last time this object was updated.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "sql.alert_state" : {
        "description" : "State of the alert. Possible values are: `unknown` (yet to be evaluated), `triggered` (evaluated and fulfilled trigger conditions), or `ok` (evaluated and did not fulfill trigger conditions).",
        "type" : "string",
        "enum" : [ "unknown", "triggered", "ok" ]
      },
      "sql.object_id" : {
        "example" : "2cca1687-60ff-4886-a445-0230578c864d",
        "format" : "uuid",
        "description" : "A UUID generated by the application.",
        "type" : "string"
      },
      "sql.object_type" : {
        "example" : "query",
        "description" : "A singular noun object type.",
        "type" : "string",
        "enum" : [ "alert", "dashboard", "data_source", "query" ]
      },
      "sql.object_type_plural" : {
        "example" : "queries",
        "description" : "Always a plural of the object type.",
        "type" : "string",
        "enum" : [ "alerts", "dashboards", "data_sources", "queries" ]
      },
      "sql.ownable_object_type" : {
        "example" : "query",
        "description" : "The singular form of the type of object which can be owned.",
        "type" : "string",
        "enum" : [ "alert", "dashboard", "query" ]
      },
      "sql.user_id" : {
        "example" : 1234567890123456,
        "description" : "The ID of the user who ran the query.",
        "type" : "integer"
      },
      "sql.user_ids" : {
        "example" : [ 1234567890123456, 6789012345678901 ],
        "description" : "A list of user IDs who ran the queries.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/sql.user_id"
        }
      },
      "sql.warehouse_id" : {
        "example" : "098765321fedcba",
        "description" : "Warehouse ID.",
        "type" : "string"
      },
      "sql.warehouse_ids" : {
        "example" : [ "098765321fedcba", "1234567890abcdef" ],
        "description" : "A list of warehouse IDs.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/sql.warehouse_id"
        }
      },
      "tokenmanagement.CreateOboTokenRequest" : {
        "required" : [ "application_id", "lifetime_seconds" ],
        "properties" : {
          "application_id" : {
            "example" : "6f5ccf28-d83a-4957-9bfb-5bbfac551410",
            "format" : "string",
            "description" : "Application ID of the service principal.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "comment" : {
            "example" : "This is for the ABC department automation scripts.",
            "format" : "string",
            "description" : "Comment that describes the purpose of the token.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          },
          "lifetime_seconds" : {
            "example" : 3600,
            "format" : "int64",
            "description" : "The number of seconds before the token expires.",
            "type" : "integer",
            "x-databricks-cloud" : "aws"
          }
        },
        "type" : "object",
        "x-databricks-cloud" : "aws"
      },
      "tokenmanagement.CreateOboTokenResponse" : {
        "properties" : {
          "token_info" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/tokenmanagement.TokenInfo"
          },
          "token_value" : {
            "example" : "dapicbd47dac9bba881f2f00727e8ab5b5cc",
            "format" : "string",
            "description" : "Value of the token.",
            "type" : "string",
            "x-databricks-cloud" : "aws"
          }
        },
        "type" : "object",
        "x-databricks-cloud" : "aws"
      },
      "tokenmanagement.ListTokensResponse" : {
        "properties" : {
          "token_infos" : {
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/tokenmanagement.TokenInfo"
            }
          }
        },
        "type" : "object"
      },
      "tokenmanagement.TokenInfo" : {
        "properties" : {
          "creation_time" : {
            "example" : 1580265020299,
            "format" : "int64",
            "description" : "Timestamp when the token was created.",
            "type" : "integer"
          },
          "created_by_id" : {
            "example" : 202480738464078,
            "format" : "int64",
            "description" : "User ID of the user that created the token.",
            "type" : "integer"
          },
          "owner_id" : {
            "example" : 202480738464078,
            "format" : "int64",
            "description" : "User ID of the user that owns the token.",
            "type" : "integer"
          },
          "comment" : {
            "example" : "This is for the ABC department automation scripts.",
            "description" : "Comment that describes the purpose of the token, specified by the token creator.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "token_id" : {
            "example" : "5684c955822ac792a51ae2aeb80190f13457bab3e2e2934c133a08b38454816c",
            "format" : "string",
            "description" : "ID of the token.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "expiry_time" : {
            "example" : 1580265020299,
            "format" : "int64",
            "description" : "Timestamp when the token expires.",
            "type" : "integer"
          },
          "created_by_username" : {
            "example" : "jsmith@example.com",
            "description" : "Username of the user that created the token.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "tokens.CreateTokenRequest" : {
        "properties" : {
          "comment" : {
            "description" : "Optional description to attach to the token.",
            "type" : "string"
          },
          "lifetime_seconds" : {
            "format" : "int64",
            "description" : "The lifetime of the token, in seconds.\n\nIf the ifetime is not specified, this token remains valid indefinitely.\n",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "tokens.CreateTokenResponse" : {
        "properties" : {
          "token_info" : {
            "description" : "The information for the new token.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/tokens.PublicTokenInfo"
          },
          "token_value" : {
            "description" : "The value of the new token.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "tokens.ListTokensResponse" : {
        "properties" : {
          "token_infos" : {
            "description" : "The information for each token.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/tokens.PublicTokenInfo"
            }
          }
        },
        "type" : "object"
      },
      "tokens.PublicTokenInfo" : {
        "properties" : {
          "comment" : {
            "description" : "Comment the token was created with, if applicable.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "creation_time" : {
            "format" : "int64",
            "description" : "Server time (in epoch milliseconds) when the token was created.",
            "type" : "integer"
          },
          "expiry_time" : {
            "format" : "int64",
            "description" : "Server time (in epoch milliseconds) when the token will expire, or -1 if not applicable.",
            "type" : "integer"
          },
          "token_id" : {
            "description" : "The ID of this token.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "tokens.RevokeTokenRequest" : {
        "required" : [ "token_id" ],
        "properties" : {
          "token_id" : {
            "description" : "The ID of the token to be revoked.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "tokens.RevokeTokenResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "unitycatalog.AuthenticationType" : {
        "description" : "The delta sharing authentication type.",
        "type" : "string",
        "enum" : [ "TOKEN", "DATABRICKS" ]
      },
      "unitycatalog.AwsIamRole" : {
        "required" : [ "role_arn" ],
        "properties" : {
          "external_id" : {
            "description" : "The external ID used in role assumption to prevent confused deputy problem..",
            "type" : "string"
          },
          "role_arn" : {
            "description" : "The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access.",
            "type" : "string"
          },
          "unity_catalog_iam_arn" : {
            "description" : "The Amazon Resource Name (ARN) of the AWS IAM user managed by Databricks. This is the identity that is going to assume the AWS IAM role.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-cloud" : "aws"
      },
      "unitycatalog.AzureServicePrincipal" : {
        "required" : [ "directory_id", "application_id", "client_secret" ],
        "properties" : {
          "application_id" : {
            "description" : "The application ID of the application registration within the referenced AAD tenant.",
            "type" : "string"
          },
          "client_secret" : {
            "description" : "The client secret generated for the above app ID in AAD.",
            "type" : "string"
          },
          "directory_id" : {
            "description" : "The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-cloud" : "azure"
      },
      "unitycatalog.CatalogInfo" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of catalog creator.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of catalog.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of user who last modified catalog.",
            "type" : "string"
          },
          "catalog_type" : {
            "readOnly" : true,
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.CatalogType"
          },
          "storage_root" : {
            "description" : "Storage root URL for managed tables within catalog.",
            "type" : "string"
          },
          "provider_name" : {
            "description" : "The name of delta sharing provider.\n\nA Delta Sharing catalog is a catalog that is based on a Delta share on a remote sharing server.\n",
            "type" : "string"
          },
          "storage_location" : {
            "readOnly" : true,
            "description" : "Storage Location URL (full path) for managed tables within catalog.",
            "type" : "string"
          },
          "properties" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.SecurablePropertiesMap"
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "share_name" : {
            "description" : "The name of the share under the share provider.",
            "type" : "string"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this catalog was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "owner" : {
            "description" : "Username of current owner of catalog.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this catalog was last modified, in epoch milliseconds.",
            "type" : "integer"
          },
          "metastore_id" : {
            "readOnly" : true,
            "description" : "Unique identifier of parent metastore.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.CatalogType" : {
        "description" : "The type of the catalog.",
        "type" : "string",
        "enum" : [ "MANAGED_CATALOG", "DELTASHARING_CATALOG", "SYSTEM_CATALOG" ]
      },
      "unitycatalog.ColumnInfo" : {
        "properties" : {
          "nullable" : {
            "default" : "true",
            "description" : "Whether field may be Null (default: true).",
            "type" : "boolean"
          },
          "name" : {
            "description" : "Name of Column.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "type_interval_type" : {
            "description" : "Format of IntervalType.",
            "type" : "string"
          },
          "mask" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.ColumnMask"
          },
          "type_scale" : {
            "format" : "int32",
            "description" : "Digits to right of decimal; Required for DecimalTypes.",
            "type" : "integer"
          },
          "type_text" : {
            "description" : "Full data type specification as SQL/catalogString text.",
            "type" : "string"
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "partition_index" : {
            "format" : "int32",
            "description" : "Partition index for column.",
            "type" : "integer"
          },
          "type_json" : {
            "description" : "Full data type specification, JSON-serialized.",
            "type" : "string"
          },
          "position" : {
            "format" : "int32",
            "description" : "Ordinal position of column (starting at position 0).",
            "type" : "integer"
          },
          "type_name" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.ColumnTypeName"
          },
          "type_precision" : {
            "format" : "int32",
            "description" : "Digits of precision; required for DecimalTypes.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "unitycatalog.ColumnMask" : {
        "properties" : {
          "function_name" : {
            "description" : "The full name of the column maks SQL UDF.",
            "type" : "string"
          },
          "using_column_names" : {
            "description" : "The list of additional table columns to be passed as input to the column mask function. The\nfirst arg of the mask function should be of the type of the column being masked and the\ntypes of the rest of the args should match the types of columns in 'using_column_names'.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ColumnTypeName" : {
        "description" : "Name of type (INT, STRUCT, MAP, etc.).",
        "type" : "string",
        "enum" : [ "BOOLEAN", "BYTE", "SHORT", "INT", "LONG", "FLOAT", "DOUBLE", "DATE", "TIMESTAMP", "STRING", "BINARY", "DECIMAL", "INTERVAL", "ARRAY", "STRUCT", "MAP", "CHAR", "NULL", "USER_DEFINED_TYPE", "TABLE_TYPE" ]
      },
      "unitycatalog.CreateCatalog" : {
        "required" : [ "name" ],
        "properties" : {
          "name" : {
            "description" : "Name of catalog.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "storage_root" : {
            "description" : "Storage root URL for managed tables within catalog.",
            "type" : "string"
          },
          "provider_name" : {
            "description" : "The name of delta sharing provider.\n\nA Delta Sharing catalog is a catalog that is based on a Delta share on a remote sharing server.\n",
            "type" : "string"
          },
          "properties" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.SecurablePropertiesMap"
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "share_name" : {
            "description" : "The name of the share under the share provider.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.CreateExternalLocation" : {
        "required" : [ "name", "url", "credential_name" ],
        "properties" : {
          "name" : {
            "description" : "Name of the external location.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "skip_validation" : {
            "description" : "Skips validation of the storage credential associated with the external location.",
            "type" : "boolean"
          },
          "url" : {
            "description" : "Path URL of the external location.",
            "type" : "string"
          },
          "read_only" : {
            "description" : "Indicates whether the external location is read-only.",
            "type" : "boolean"
          },
          "credential_name" : {
            "description" : "Name of the storage credential used with this location.",
            "type" : "string"
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.CreateFunction" : {
        "required" : [ "is_deterministic", "name", "specific_name", "catalog_name", "return_params", "routine_definition", "is_null_call", "input_params", "parameter_style", "schema_name", "routine_body", "data_type", "security_type", "routine_dependencies", "sql_data_access", "full_data_type" ],
        "properties" : {
          "is_deterministic" : {
            "description" : "Whether the function is deterministic.",
            "type" : "boolean"
          },
          "name" : {
            "description" : "Name of function, relative to parent schema.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "specific_name" : {
            "description" : "Specific name of the function; Reserved for future use.",
            "type" : "string"
          },
          "sql_path" : {
            "description" : "List of schemes whose objects can be referenced without qualification.",
            "type" : "string"
          },
          "external_language" : {
            "description" : "External function language.",
            "type" : "string"
          },
          "catalog_name" : {
            "description" : "Name of parent catalog.",
            "type" : "string"
          },
          "return_params" : {
            "description" : "Table function return parameters.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.FunctionParameterInfos"
          },
          "external_name" : {
            "description" : "External function name.",
            "type" : "string"
          },
          "routine_definition" : {
            "description" : "Function body.",
            "type" : "string"
          },
          "is_null_call" : {
            "description" : "Function null call.",
            "type" : "boolean"
          },
          "input_params" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.FunctionParameterInfos"
          },
          "parameter_style" : {
            "description" : "Function parameter style. **S** is the value for SQL.",
            "type" : "string",
            "enum" : [ "S" ]
          },
          "schema_name" : {
            "description" : "Name of parent schema relative to its parent catalog.",
            "type" : "string"
          },
          "routine_body" : {
            "description" : "Function language. When **EXTERNAL** is used, the language of the routine function should be specified in the __external_language__ field, \nand the __return_params__ of the function cannot be used (as **TABLE** return type is not supported),\nand the __sql_data_access__ field must be **NO_SQL**.\n",
            "type" : "string",
            "enum" : [ "SQL", "EXTERNAL" ]
          },
          "data_type" : {
            "description" : "Scalar function return data type.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.ColumnTypeName"
          },
          "properties" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.SecurablePropertiesMap"
          },
          "security_type" : {
            "description" : "Function security type.",
            "type" : "string",
            "enum" : [ "DEFINER" ]
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "routine_dependencies" : {
            "description" : "Function dependencies.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.DependencyList"
          },
          "sql_data_access" : {
            "description" : "Function SQL data access.",
            "type" : "string",
            "enum" : [ "CONTAINS_SQL", "READS_SQL_DATA", "NO_SQL" ]
          },
          "full_data_type" : {
            "description" : "Pretty printed function data type.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.CreateMetastore" : {
        "required" : [ "name", "storage_root" ],
        "properties" : {
          "name" : {
            "description" : "The user-specified name of the metastore.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "region" : {
            "description" : "Cloud region which the metastore serves (e.g., `us-west-2`, `westus`).\nIf this field is omitted, the region of the workspace receiving the request will be used.",
            "type" : "string"
          },
          "storage_root" : {
            "description" : "The storage root URL for metastore",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.CreateMetastoreAssignment" : {
        "required" : [ "metastore_id", "default_catalog_name" ],
        "properties" : {
          "default_catalog_name" : {
            "description" : "The name of the default catalog in the metastore.",
            "type" : "string"
          },
          "metastore_id" : {
            "description" : "The unique ID of the metastore.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.CreateProvider" : {
        "required" : [ "name", "authentication_type" ],
        "properties" : {
          "authentication_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.AuthenticationType"
          },
          "comment" : {
            "description" : "Description about the provider.",
            "type" : "string"
          },
          "name" : {
            "description" : "The name of the Provider.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "recipient_profile_str" : {
            "description" : "This field is required when the __authentication_type__ is **TOKEN** or not provided.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.CreateRecipient" : {
        "required" : [ "name", "authentication_type" ],
        "properties" : {
          "name" : {
            "description" : "Name of Recipient.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "authentication_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.AuthenticationType"
          },
          "data_recipient_global_metastore_id" : {
            "properties" : { },
            "description" : "The global Unity Catalog metastore id provided by the data recipient.\\n\nThis field is required when the __authentication_type__ is **DATABRICKS**.\\n\nThe identifier is of format __cloud__:__region__:__metastore-uuid__.\n",
            "type" : "object"
          },
          "ip_access_list" : {
            "description" : "IP Access List",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.IpAccessList"
          },
          "comment" : {
            "description" : "Description about the recipient.",
            "type" : "string"
          },
          "sharing_code" : {
            "description" : "The one-time sharing code provided by the data recipient. This field is required when the __authentication_type__ is **DATABRICKS**.",
            "type" : "string"
          },
          "properties_kvpairs" : {
            "properties" : { },
            "description" : "Recipient properties as map of string key-value pairs.\\n\n",
            "type" : "object"
          },
          "owner" : {
            "description" : "Username of the recipient owner.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.CreateSchema" : {
        "required" : [ "name", "catalog_name" ],
        "properties" : {
          "name" : {
            "description" : "Name of schema, relative to parent catalog.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "catalog_name" : {
            "description" : "Name of parent catalog.",
            "type" : "string"
          },
          "storage_root" : {
            "description" : "Storage root URL for managed tables within schema.",
            "type" : "string"
          },
          "properties" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.SecurablePropertiesMap"
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.CreateShare" : {
        "required" : [ "name" ],
        "properties" : {
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of the share.",
            "x-databricks-name" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.CreateStorageCredential" : {
        "required" : [ "name" ],
        "properties" : {
          "name" : {
            "description" : "The credential name. The name must be unique within the metastore.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "skip_validation" : {
            "default" : "false",
            "description" : "Supplying true to this argument skips validation of the created credential.",
            "type" : "boolean"
          },
          "read_only" : {
            "description" : "Whether the storage credential is only usable for read operations.",
            "type" : "boolean"
          },
          "azure_service_principal" : {
            "description" : "The Azure service principal configuration.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/unitycatalog.AzureServicePrincipal"
          },
          "gcp_service_account_key" : {
            "description" : "The GCP service account key configuration.",
            "deprecated" : true,
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/unitycatalog.GcpServiceAccountKey"
          },
          "comment" : {
            "description" : "Comment associated with the credential.",
            "type" : "string"
          },
          "aws_iam_role" : {
            "description" : "The AWS IAM role configuration.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/unitycatalog.AwsIamRole"
          }
        },
        "type" : "object"
      },
      "unitycatalog.CreateTableConstraint" : {
        "required" : [ "full_name_arg, constraint" ],
        "properties" : {
          "constraint" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.TableConstraint"
          },
          "full_name_arg" : {
            "description" : "The full name of the table referenced by the constraint.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.DataSourceFormat" : {
        "description" : "Data source format",
        "type" : "string",
        "enum" : [ "DELTA", "CSV", "JSON", "AVRO", "PARQUET", "ORC", "TEXT", "UNITY_CATALOG", "DELTASHARING" ]
      },
      "unitycatalog.Dependency" : {
        "properties" : {
          "function" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.FunctionDependency"
          },
          "table" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.TableDependency"
          }
        },
        "description" : "A dependency of a SQL object. Either the __table__ field or the __function__ field must be defined.",
        "type" : "object"
      },
      "unitycatalog.DependencyList" : {
        "description" : "Array of dependencies.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/unitycatalog.Dependency"
        }
      },
      "unitycatalog.EffectivePermissionsList" : {
        "properties" : {
          "privilege_assignments" : {
            "description" : "The privileges conveyed to each principal (either directly or via inheritance)",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.EffectivePrivilegeAssignment"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.EffectivePrivilege" : {
        "properties" : {
          "inherited_from_name" : {
            "description" : "The full name of the object that conveys this privilege via inheritance.\\n\nThis field is omitted when privilege is not inherited (it's assigned to the securable itself).\n",
            "type" : "string"
          },
          "inherited_from_type" : {
            "description" : "The type of the object that conveys this privilege via inheritance.\\n\nThis field is omitted when privilege is not inherited (it's assigned to the securable itself).\n",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.SecurableType"
          },
          "privilege" : {
            "description" : "The privilege assigned to the principal.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.Privilege"
          }
        },
        "type" : "object"
      },
      "unitycatalog.EffectivePrivilegeAssignment" : {
        "properties" : {
          "principal" : {
            "description" : "The principal (user email address or group name).",
            "type" : "string"
          },
          "privileges" : {
            "description" : "The privileges conveyed to the principal (either directly or via inheritance).",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.EffectivePrivilege"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ExternalLocationInfo" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of external location creator.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of the external location.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "credential_id" : {
            "readOnly" : true,
            "description" : "Unique ID of the location's storage credential.",
            "type" : "string"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of user who last modified the external location.",
            "type" : "string"
          },
          "url" : {
            "description" : "Path URL of the external location.",
            "type" : "string"
          },
          "read_only" : {
            "description" : "Indicates whether the external location is read-only.",
            "type" : "boolean"
          },
          "credential_name" : {
            "description" : "Name of the storage credential used with this location.",
            "type" : "string"
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this external location was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "owner" : {
            "description" : "The owner of the external location.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which external location this was last modified, in epoch milliseconds.",
            "type" : "integer"
          },
          "metastore_id" : {
            "readOnly" : true,
            "description" : "Unique identifier of metastore hosting the external location.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.ForeignKeyConstraint" : {
        "required" : [ "name", "child_columns", "parent_table", "parent_columns" ],
        "properties" : {
          "child_columns" : {
            "description" : "Column names for this constraint.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "name" : {
            "description" : "The name of the constraint.",
            "type" : "string"
          },
          "parent_columns" : {
            "description" : "Column names for this constraint.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "parent_table" : {
            "description" : "The full name of the parent constraint.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.FunctionDependency" : {
        "required" : [ "function_full_name" ],
        "properties" : {
          "function_full_name" : {
            "description" : "Full name of the dependent function, in the form of __catalog_name__.__schema_name__.__function_name__.",
            "type" : "string"
          }
        },
        "description" : "A function that is dependent on a SQL object.",
        "type" : "object"
      },
      "unitycatalog.FunctionInfo" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of function creator.",
            "type" : "string"
          },
          "is_deterministic" : {
            "description" : "Whether the function is deterministic.",
            "type" : "boolean"
          },
          "name" : {
            "description" : "Name of function, relative to parent schema.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "specific_name" : {
            "description" : "Specific name of the function; Reserved for future use.",
            "type" : "string"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of user who last modified function.",
            "type" : "string"
          },
          "sql_path" : {
            "description" : "List of schemes whose objects can be referenced without qualification.",
            "type" : "string"
          },
          "external_language" : {
            "description" : "External function language.",
            "type" : "string"
          },
          "full_name" : {
            "readOnly" : true,
            "description" : "Full name of function, in form of __catalog_name__.__schema_name__.__function__name__",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "catalog_name" : {
            "description" : "Name of parent catalog.",
            "type" : "string"
          },
          "return_params" : {
            "description" : "Table function return parameters.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.FunctionParameterInfos"
          },
          "external_name" : {
            "description" : "External function name.",
            "type" : "string"
          },
          "routine_definition" : {
            "description" : "Function body.",
            "type" : "string"
          },
          "is_null_call" : {
            "description" : "Function null call.",
            "type" : "boolean"
          },
          "input_params" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.FunctionParameterInfos"
          },
          "parameter_style" : {
            "description" : "Function parameter style. **S** is the value for SQL.",
            "type" : "string",
            "enum" : [ "S" ]
          },
          "schema_name" : {
            "description" : "Name of parent schema relative to its parent catalog.",
            "type" : "string"
          },
          "routine_body" : {
            "description" : "Function language. When **EXTERNAL** is used, the language of the routine function should be specified in the __external_language__ field, \nand the __return_params__ of the function cannot be used (as **TABLE** return type is not supported),\nand the __sql_data_access__ field must be **NO_SQL**.\n",
            "type" : "string",
            "enum" : [ "SQL", "EXTERNAL" ]
          },
          "function_id" : {
            "readOnly" : true,
            "description" : "Id of Function, relative to parent schema.",
            "type" : "string"
          },
          "data_type" : {
            "description" : "Scalar function return data type.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.ColumnTypeName"
          },
          "properties" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.SecurablePropertiesMap"
          },
          "security_type" : {
            "description" : "Function security type.",
            "type" : "string",
            "enum" : [ "DEFINER" ]
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "routine_dependencies" : {
            "description" : "Function dependencies.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.DependencyList"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this function was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "owner" : {
            "description" : "Username of current owner of function.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this function was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "sql_data_access" : {
            "description" : "Function SQL data access.",
            "type" : "string",
            "enum" : [ "CONTAINS_SQL", "READS_SQL_DATA", "NO_SQL" ]
          },
          "full_data_type" : {
            "description" : "Pretty printed function data type.",
            "type" : "string"
          },
          "metastore_id" : {
            "readOnly" : true,
            "description" : "Unique identifier of parent metastore.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.FunctionParameterInfo" : {
        "required" : [ "name", "type_text", "type_name", "position" ],
        "properties" : {
          "name" : {
            "description" : "Name of parameter.",
            "type" : "string"
          },
          "type_interval_type" : {
            "description" : "Format of IntervalType.",
            "type" : "string"
          },
          "parameter_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.FunctionParameterType"
          },
          "parameter_default" : {
            "description" : "Default value of the parameter.",
            "type" : "string"
          },
          "parameter_mode" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.FunctionParameterMode"
          },
          "type_scale" : {
            "format" : "int32",
            "description" : "Digits to right of decimal; Required on Create for DecimalTypes.",
            "type" : "integer"
          },
          "type_text" : {
            "description" : "Full data type spec, SQL/catalogString text.",
            "type" : "string"
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "type_json" : {
            "description" : "Full data type spec, JSON-serialized.",
            "type" : "string"
          },
          "position" : {
            "format" : "int32",
            "description" : "Ordinal position of column (starting at position 0).",
            "type" : "integer"
          },
          "type_name" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.ColumnTypeName"
          },
          "type_precision" : {
            "format" : "int32",
            "description" : "Digits of precision; required on Create for DecimalTypes.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "unitycatalog.FunctionParameterInfos" : {
        "description" : "The array of __FunctionParameterInfo__ definitions of the function's parameters.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/unitycatalog.FunctionParameterInfo"
        }
      },
      "unitycatalog.FunctionParameterMode" : {
        "description" : "The mode of the function parameter.",
        "type" : "string",
        "enum" : [ "IN" ]
      },
      "unitycatalog.FunctionParameterType" : {
        "description" : "The type of function parameter.",
        "type" : "string",
        "enum" : [ "PARAM", "COLUMN" ]
      },
      "unitycatalog.GcpServiceAccountKey" : {
        "required" : [ "email", "private_key_id", "private_key" ],
        "properties" : {
          "email" : {
            "description" : "The email of the service account.",
            "type" : "string"
          },
          "private_key" : {
            "description" : "The service account's RSA private key.",
            "type" : "string"
          },
          "private_key_id" : {
            "description" : "The ID of the service account's private key.",
            "type" : "string"
          }
        },
        "type" : "object",
        "x-databricks-cloud" : "gcp"
      },
      "unitycatalog.GetActivationUrlInfoResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "unitycatalog.GetMetastoreSummaryResponse" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of metastore creator.",
            "type" : "string"
          },
          "delta_sharing_scope" : {
            "description" : "The scope of Delta Sharing enabled for the metastore.",
            "type" : "string",
            "enum" : [ "INTERNAL", "INTERNAL_AND_EXTERNAL" ]
          },
          "name" : {
            "description" : "The user-specified name of the metastore.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of user who last modified the metastore.",
            "type" : "string"
          },
          "default_data_access_config_id" : {
            "description" : "Unique identifier of the metastore's (Default) Data Access Configuration.",
            "deprecated" : true,
            "type" : "string"
          },
          "storage_root" : {
            "description" : "The storage root URL for metastore",
            "type" : "string"
          },
          "storage_root_credential_id" : {
            "description" : "UUID of storage credential to access the metastore storage_root.",
            "type" : "string"
          },
          "privilege_model_version" : {
            "description" : "Privilege model version of the metastore, of the form `major.minor` (e.g., `1.0`).",
            "type" : "string"
          },
          "delta_sharing_recipient_token_lifetime_in_seconds" : {
            "format" : "int64",
            "description" : "The lifetime of delta sharing recipient token in seconds.",
            "type" : "integer"
          },
          "delta_sharing_organization_name" : {
            "description" : "The organization name of a Delta Sharing entity, to be used in Databricks-to-Databricks Delta Sharing as the official name.",
            "type" : "string"
          },
          "storage_root_credential_name" : {
            "readOnly" : true,
            "description" : "Name of the storage credential to access the metastore storage_root.",
            "type" : "string"
          },
          "cloud" : {
            "readOnly" : true,
            "description" : "Cloud vendor of the metastore home shard (e.g., `aws`, `azure`, `gcp`).",
            "type" : "string"
          },
          "global_metastore_id" : {
            "readOnly" : true,
            "description" : "Globally unique metastore ID across clouds and regions, of the form `cloud:region:metastore_id`.",
            "type" : "string"
          },
          "region" : {
            "description" : "Cloud region which the metastore serves (e.g., `us-west-2`, `westus`).",
            "type" : "string"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this metastore was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "owner" : {
            "description" : "The owner of the metastore.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which the metastore was last modified, in epoch milliseconds.",
            "type" : "integer"
          },
          "metastore_id" : {
            "readOnly" : true,
            "description" : "Unique identifier of metastore.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.GetRecipientSharePermissionsResponse" : {
        "properties" : {
          "permissions_out" : {
            "description" : "An array of data share permissions for a recipient.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.ShareToPrivilegeAssignment"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.IpAccessList" : {
        "properties" : {
          "allowed_ip_addresses" : {
            "description" : "Allowed IP Addresses in CIDR notation. Limit of 100.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ListCatalogsResponse" : {
        "properties" : {
          "catalogs" : {
            "description" : "An array of catalog information objects.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.CatalogInfo"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ListExternalLocationsResponse" : {
        "properties" : {
          "external_locations" : {
            "description" : "An array of external locations.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.ExternalLocationInfo"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ListFunctionsResponse" : {
        "properties" : {
          "schemas" : {
            "description" : "An array of function information objects.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.FunctionInfo"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ListMetastoreAssignmentsResponse" : {
        "description" : "An array of metastore assignments.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/unitycatalog.MetastoreAssignment"
        }
      },
      "unitycatalog.ListMetastoresResponse" : {
        "properties" : {
          "metastores" : {
            "description" : "An array of metastore information objects.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.MetastoreInfo"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ListProviderSharesResponse" : {
        "properties" : {
          "shares" : {
            "description" : "An array of provider shares.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.ProviderShare"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ListProvidersResponse" : {
        "properties" : {
          "providers" : {
            "description" : "An array of provider information objects.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.ProviderInfo"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ListRecipientsResponse" : {
        "properties" : {
          "recipients" : {
            "description" : "An array of recipient information objects.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.RecipientInfo"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ListSchemasResponse" : {
        "properties" : {
          "schemas" : {
            "description" : "An array of schema information objects.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.SchemaInfo"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ListSharesResponse" : {
        "properties" : {
          "shares" : {
            "description" : "An array of data share information objects.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.ShareInfo"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ListStorageCredentialsResponse" : {
        "description" : "An array of metastore storage credentials.",
        "type" : "array",
        "items" : {
          "extRef" : true,
          "ref" : true,
          "$ref" : "#/components/schemas/unitycatalog.StorageCredentialInfo"
        }
      },
      "unitycatalog.ListTableSummariesResponse" : {
        "properties" : {
          "next_page_token" : {
            "description" : "Opaque token for pagination. Omitted if there are no more results.",
            "type" : "string"
          },
          "tables" : {
            "description" : "List of table summaries.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.TableSummary"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ListTablesResponse" : {
        "properties" : {
          "tables" : {
            "description" : "An array of table information objects.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.TableInfo"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.MetastoreAssignment" : {
        "required" : [ "metastore_id", "workspace_id" ],
        "properties" : {
          "default_catalog_name" : {
            "description" : "The name of the default catalog in the metastore.",
            "type" : "string"
          },
          "metastore_id" : {
            "description" : "The unique ID of the metastore.",
            "type" : "string"
          },
          "workspace_id" : {
            "description" : "The unique ID of the Databricks workspace.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.MetastoreInfo" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of metastore creator.",
            "type" : "string"
          },
          "delta_sharing_scope" : {
            "description" : "The scope of Delta Sharing enabled for the metastore.",
            "type" : "string",
            "enum" : [ "INTERNAL", "INTERNAL_AND_EXTERNAL" ]
          },
          "name" : {
            "description" : "The user-specified name of the metastore.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of user who last modified the metastore.",
            "type" : "string"
          },
          "default_data_access_config_id" : {
            "description" : "Unique identifier of the metastore's (Default) Data Access Configuration.",
            "deprecated" : true,
            "type" : "string"
          },
          "storage_root" : {
            "description" : "The storage root URL for metastore",
            "type" : "string"
          },
          "storage_root_credential_id" : {
            "description" : "UUID of storage credential to access the metastore storage_root.",
            "type" : "string"
          },
          "privilege_model_version" : {
            "description" : "Privilege model version of the metastore, of the form `major.minor` (e.g., `1.0`).",
            "type" : "string"
          },
          "delta_sharing_recipient_token_lifetime_in_seconds" : {
            "format" : "int64",
            "description" : "The lifetime of delta sharing recipient token in seconds.",
            "type" : "integer"
          },
          "delta_sharing_organization_name" : {
            "description" : "The organization name of a Delta Sharing entity, to be used in Databricks-to-Databricks Delta Sharing as the official name.",
            "type" : "string"
          },
          "storage_root_credential_name" : {
            "readOnly" : true,
            "description" : "Name of the storage credential to access the metastore storage_root.",
            "type" : "string"
          },
          "cloud" : {
            "readOnly" : true,
            "description" : "Cloud vendor of the metastore home shard (e.g., `aws`, `azure`, `gcp`).",
            "type" : "string"
          },
          "global_metastore_id" : {
            "readOnly" : true,
            "description" : "Globally unique metastore ID across clouds and regions, of the form `cloud:region:metastore_id`.",
            "type" : "string"
          },
          "region" : {
            "description" : "Cloud region which the metastore serves (e.g., `us-west-2`, `westus`).",
            "type" : "string"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this metastore was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "owner" : {
            "description" : "The owner of the metastore.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which the metastore was last modified, in epoch milliseconds.",
            "type" : "integer"
          },
          "metastore_id" : {
            "readOnly" : true,
            "description" : "Unique identifier of metastore.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.NamedTableConstraint" : {
        "required" : [ "name" ],
        "properties" : {
          "name" : {
            "description" : "The name of the constraint.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.Partition" : {
        "properties" : {
          "values" : {
            "description" : "An array of partition values.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.PartitionValue"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.PartitionValue" : {
        "properties" : {
          "name" : {
            "description" : "The name of the partition column.",
            "type" : "string"
          },
          "op" : {
            "description" : "The operator to apply for the value.",
            "type" : "string",
            "enum" : [ "EQUAL", "LIKE" ]
          },
          "recipient_property_key" : {
            "description" : "The key of a Delta Sharing recipient's property. For example `databricks-account-id`. When this field is set, field `value` can not be set.",
            "type" : "string"
          },
          "value" : {
            "description" : "The value of the partition column. When this value is not set, it means `null` value. When this field is set, field `recipient_property_key` can not be set.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.PermissionsChange" : {
        "properties" : {
          "add" : {
            "description" : "The set of privileges to add.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.Privilege"
            }
          },
          "principal" : {
            "description" : "The principal whose privileges we are changing.",
            "type" : "string"
          },
          "remove" : {
            "description" : "The set of privileges to remove.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.Privilege"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.PermissionsList" : {
        "properties" : {
          "privilege_assignments" : {
            "description" : "The privileges assigned to each principal",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.PrivilegeAssignment"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.PrimaryKeyConstraint" : {
        "required" : [ "name", "child_columns" ],
        "properties" : {
          "child_columns" : {
            "description" : "Column names for this constraint.",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "name" : {
            "description" : "The name of the constraint.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.Privilege" : {
        "type" : "string",
        "enum" : [ "READ_PRIVATE_FILES", "WRITE_PRIVATE_FILES", "CREATE", "USAGE", "USE_CATALOG", "USE_SCHEMA", "CREATE_SCHEMA", "CREATE_VIEW", "CREATE_EXTERNAL_TABLE", "CREATE_MATERIALIZED_VIEW", "CREATE_FUNCTION", "CREATE_CATALOG", "CREATE_MANAGED_STORAGE", "CREATE_EXTERNAL_LOCATION", "CREATE_STORAGE_CREDENTIAL", "CREATE_SHARE", "CREATE_RECIPIENT", "CREATE_PROVIDER", "USE_SHARE", "USE_RECIPIENT", "USE_PROVIDER", "SET_SHARE_PERMISSION", "SELECT", "MODIFY", "REFRESH", "EXECUTE", "READ_FILES", "WRITE_FILES", "CREATE_TABLE", "ALL_PRIVILEGES" ]
      },
      "unitycatalog.PrivilegeAssignment" : {
        "properties" : {
          "principal" : {
            "description" : "The principal (user email address or group name).",
            "type" : "string"
          },
          "privileges" : {
            "description" : "The privileges assigned to the principal.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.Privilege"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ProviderInfo" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of Provider creator.",
            "type" : "string"
          },
          "name" : {
            "description" : "The name of the Provider.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of user who last modified Share.",
            "type" : "string"
          },
          "data_provider_global_metastore_id" : {
            "description" : "The global UC metastore id of the data provider. This field is only present when the __authentication_type__ is **DATABRICKS**. The identifier is of format <cloud>:<region>:<metastore-uuid>.",
            "type" : "string"
          },
          "authentication_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.AuthenticationType"
          },
          "recipient_profile_str" : {
            "description" : "This field is only present when the authentication_type is `TOKEN` or not provided.",
            "type" : "string"
          },
          "recipient_profile" : {
            "description" : "The recipient profile. This field is only present when the authentication_type is `TOKEN`.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.RecipientProfile"
          },
          "cloud" : {
            "readOnly" : true,
            "description" : "Cloud vendor of the provider's UC metastore. This field is only present when the __authentication_type__ is **DATABRICKS**.",
            "type" : "string"
          },
          "comment" : {
            "description" : "Description about the provider.",
            "type" : "string"
          },
          "region" : {
            "readOnly" : true,
            "description" : "Cloud region of the provider's UC metastore. This field is only present when the __authentication_type__ is **DATABRICKS**.",
            "type" : "string"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this Provider was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "owner" : {
            "description" : "Username of Provider owner.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this Provider was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "metastore_id" : {
            "readOnly" : true,
            "description" : "UUID of the provider's UC metastore. This field is only present when the __authentication_type__ is **DATABRICKS**.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.ProviderShare" : {
        "properties" : {
          "name" : {
            "description" : "The name of the Provider Share.",
            "x-databricks-name" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.RecipientInfo" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of recipient creator.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of Recipient.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "activated" : {
            "readOnly" : true,
            "description" : "A boolean status field showing whether the Recipient's activation URL has been exercised or not.",
            "deprecated" : true,
            "type" : "boolean"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of recipient updater.",
            "type" : "string"
          },
          "authentication_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.AuthenticationType"
          },
          "data_recipient_global_metastore_id" : {
            "properties" : { },
            "description" : "The global Unity Catalog metastore id provided by the data recipient.\\n\nThis field is only present when the __authentication_type__ is **DATABRICKS**.\\n\nThe identifier is of format __cloud__:__region__:__metastore-uuid__.\n",
            "type" : "object"
          },
          "ip_access_list" : {
            "description" : "IP Access List",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.IpAccessList"
          },
          "cloud" : {
            "readOnly" : true,
            "description" : "Cloud vendor of the recipient's Unity Catalog Metstore.\nThis field is only present when the __authentication_type__ is **DATABRICKS**`.",
            "type" : "string"
          },
          "comment" : {
            "description" : "Description about the recipient.",
            "type" : "string"
          },
          "sharing_code" : {
            "description" : "The one-time sharing code provided by the data recipient. This field is only present when the __authentication_type__ is **DATABRICKS**.",
            "type" : "string"
          },
          "properties_kvpairs" : {
            "properties" : { },
            "description" : "Recipient properties as map of string key-value pairs.\\n\n",
            "type" : "object"
          },
          "region" : {
            "readOnly" : true,
            "description" : "Cloud region of the recipient's Unity Catalog Metstore.\nThis field is only present when the __authentication_type__ is **DATABRICKS**.",
            "type" : "string"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this recipient was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "tokens" : {
            "readOnly" : true,
            "description" : "This field is only present when the __authentication_type__ is **TOKEN**.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.RecipientTokenInfo"
            }
          },
          "owner" : {
            "description" : "Username of the recipient owner.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which the recipient was updated, in epoch milliseconds.",
            "type" : "integer"
          },
          "activation_url" : {
            "readOnly" : true,
            "description" : "Full activation url to retrieve the access token. It will be empty if the token is already retrieved.",
            "deprecated" : true,
            "type" : "string"
          },
          "metastore_id" : {
            "readOnly" : true,
            "description" : "Unique identifier of recipient's Unity Catalog metastore.\nThis field is only present when the __authentication_type__ is **DATABRICKS**",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.RecipientProfile" : {
        "properties" : {
          "bearer_token" : {
            "description" : "The token used to authorize the recipient.",
            "type" : "string"
          },
          "endpoint" : {
            "description" : "The endpoint for the share to be used by the recipient.",
            "type" : "string"
          },
          "share_credentials_version" : {
            "format" : "int32",
            "description" : "The version number of the recipient's credentials on a share.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "unitycatalog.RecipientTokenInfo" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of recipient token creator.",
            "type" : "string"
          },
          "expiration_time" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Expiration timestamp of the token in epoch milliseconds.",
            "type" : "integer"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of recipient Token updater.",
            "type" : "string"
          },
          "id" : {
            "readOnly" : true,
            "description" : "Unique ID of the recipient token.",
            "type" : "string"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this recipient Token was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this recipient Token was updated, in epoch milliseconds.",
            "type" : "integer"
          },
          "activation_url" : {
            "readOnly" : true,
            "description" : "Full activation URL to retrieve the access token.\nIt will be empty if the token is already retrieved.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.RetrieveTokenResponse" : {
        "properties" : {
          "bearerToken" : {
            "readOnly" : true,
            "description" : "The token used to authorize the recipient.",
            "type" : "string"
          },
          "endpoint" : {
            "readOnly" : true,
            "description" : "The endpoint for the share to be used by the recipient.",
            "type" : "string"
          },
          "expirationTime" : {
            "readOnly" : true,
            "description" : "Expiration timestamp of the token in epoch milliseconds.",
            "type" : "string"
          },
          "shareCredentialsVersion" : {
            "format" : "int32",
            "readOnly" : true,
            "description" : "These field names must follow the delta sharing protocol.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "unitycatalog.RotateRecipientToken" : {
        "required" : [ "existing_token_expire_in_seconds" ],
        "properties" : {
          "existing_token_expire_in_seconds" : {
            "format" : "int64",
            "description" : "The expiration time of the bearer token in ISO 8601 format. This will set the expiration_time of existing token only to a smaller timestamp, it cannot extend the expiration_time. Use 0 to expire the existing token immediately, negative number will return an error.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "unitycatalog.SchemaInfo" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of schema creator.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of schema, relative to parent catalog.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of user who last modified schema.",
            "type" : "string"
          },
          "full_name" : {
            "readOnly" : true,
            "description" : "Full name of schema, in form of __catalog_name__.__schema_name__.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "catalog_type" : {
            "readOnly" : true,
            "description" : "The type of the parent catalog.",
            "type" : "string"
          },
          "catalog_name" : {
            "description" : "Name of parent catalog.",
            "type" : "string"
          },
          "storage_root" : {
            "description" : "Storage root URL for managed tables within schema.",
            "type" : "string"
          },
          "storage_location" : {
            "readOnly" : true,
            "description" : "Storage location for managed tables within schema.",
            "type" : "string"
          },
          "properties" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.SecurablePropertiesMap"
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this schema was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "owner" : {
            "description" : "Username of current owner of schema.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this schema was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "metastore_id" : {
            "readOnly" : true,
            "description" : "Unique identifier of parent metastore.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.SecurablePropertiesMap" : {
        "properties" : { },
        "description" : "A map of key-value properties attached to the securable.",
        "type" : "object",
        "additionalProperties" : {
          "type" : "string"
        }
      },
      "unitycatalog.SecurableType" : {
        "description" : "The type of Unity Catalog securable",
        "type" : "string",
        "enum" : [ "CATALOG", "SCHEMA", "TABLE", "STORAGE_CREDENTIAL", "EXTERNAL_LOCATION", "FUNCTION", "SHARE", "PROVIDER", "RECIPIENT", "METASTORE" ]
      },
      "unitycatalog.ShareInfo" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of share creator.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of the share.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of share updater.",
            "type" : "string"
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this share was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "owner" : {
            "description" : "Username of current owner of share.",
            "type" : "string"
          },
          "objects" : {
            "readOnly" : true,
            "description" : "A list of shared data objects within the share.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.SharedDataObject"
            }
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this share was updated, in epoch milliseconds.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "unitycatalog.ShareToPrivilegeAssignment" : {
        "properties" : {
          "privilege_assignments" : {
            "description" : "The privileges assigned to the principal.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.PrivilegeAssignment"
            }
          },
          "share_name" : {
            "description" : "The share name.",
            "x-databricks-name" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.SharedDataObject" : {
        "required" : [ "name" ],
        "properties" : {
          "name" : {
            "description" : "A fully qualified name that uniquely identifies a data object.\n\nFor example, a table's fully qualified name is in the format of `<catalog>.<schema>.<table>`.\n",
            "type" : "string"
          },
          "added_by" : {
            "readOnly" : true,
            "description" : "Username of the sharer.",
            "type" : "string"
          },
          "start_version" : {
            "format" : "int64",
            "description" : "The start version associated with the object.\nThis allows data providers to control the lowest object version that is accessible by clients.\nIf specified, clients can query snapshots or changes for versions >= start_version.\nIf not specified, clients can only query starting from the version of the object at the time\nit was added to the share.\n\nNOTE: The start_version should be <= the `current` version of the object.\n",
            "type" : "integer"
          },
          "cdf_enabled" : {
            "description" : "Whether to enable cdf or indicate if cdf is enabled on the shared object.",
            "type" : "boolean"
          },
          "status" : {
            "description" : "One of: **ACTIVE**, **PERMISSION_DENIED**.",
            "type" : "string",
            "enum" : [ "ACTIVE", "PERMISSION_DENIED" ]
          },
          "comment" : {
            "description" : "A user-provided comment when adding the data object to the share.\n[Update:OPT]",
            "type" : "string"
          },
          "data_object_type" : {
            "readOnly" : true,
            "description" : "The type of the data object.",
            "type" : "string"
          },
          "partitions" : {
            "description" : "Array of partitions for the shared data.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.Partition"
            }
          },
          "shared_as" : {
            "description" : "A user-provided new name for the data object within the share. If this new name is not provided, the object's original name will be used as the `shared_as` name. The `shared_as` name must be unique within a share. For tables, the new name must follow the format of `<schema>.<table>`.",
            "type" : "string"
          },
          "added_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "The time when this data object is added to the share, in epoch milliseconds.",
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "unitycatalog.SharedDataObjectUpdate" : {
        "properties" : {
          "action" : {
            "description" : "One of: **ADD**, **REMOVE**, **UPDATE**.",
            "type" : "string",
            "enum" : [ "ADD", "REMOVE", "UPDATE" ]
          },
          "data_object" : {
            "description" : "The data object that is being added, removed, or updated.",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.SharedDataObject"
          }
        },
        "type" : "object"
      },
      "unitycatalog.StorageCredentialInfo" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of credential creator.",
            "type" : "string"
          },
          "name" : {
            "description" : "The credential name. The name must be unique within the metastore.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of user who last modified the credential.",
            "type" : "string"
          },
          "read_only" : {
            "description" : "Whether the storage credential is only usable for read operations.",
            "type" : "boolean"
          },
          "used_for_managed_storage" : {
            "readOnly" : true,
            "description" : "Whether this credential is the current metastore's root storage credential.",
            "type" : "boolean"
          },
          "azure_service_principal" : {
            "description" : "The Azure service principal configuration.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/unitycatalog.AzureServicePrincipal"
          },
          "gcp_service_account_key" : {
            "description" : "The GCP service account key configuration.",
            "deprecated" : true,
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/unitycatalog.GcpServiceAccountKey"
          },
          "id" : {
            "readOnly" : true,
            "description" : "The unique identifier of the credential.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "comment" : {
            "description" : "Comment associated with the credential.",
            "type" : "string"
          },
          "aws_iam_role" : {
            "description" : "The AWS IAM role configuration.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/unitycatalog.AwsIamRole"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this Credential was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "owner" : {
            "description" : "Username of current owner of credential.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this credential was last modified, in epoch milliseconds.",
            "type" : "integer"
          },
          "metastore_id" : {
            "readOnly" : true,
            "description" : "Unique identifier of parent metastore.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.TableConstraint" : {
        "properties" : {
          "foreign_key_constraint" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.ForeignKeyConstraint"
          },
          "named_table_constraint" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.NamedTableConstraint"
          },
          "primary_key_constraint" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.PrimaryKeyConstraint"
          }
        },
        "description" : "A table constraint, as defined by *one* of the following fields being set:\n__primary_key_constraint__, __foreign_key_constraint__, __named_table_constraint__.\n",
        "type" : "object"
      },
      "unitycatalog.TableConstraintList" : {
        "properties" : {
          "table_constraints" : {
            "description" : "List of table constraints.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.TableConstraint"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.TableDependency" : {
        "required" : [ "table_full_name" ],
        "properties" : {
          "table_full_name" : {
            "description" : "Full name of the dependent table, in the form of __catalog_name__.__schema_name__.__table_name__.",
            "type" : "string"
          }
        },
        "description" : "A table that is dependent on a SQL object.",
        "type" : "object"
      },
      "unitycatalog.TableInfo" : {
        "properties" : {
          "created_by" : {
            "readOnly" : true,
            "description" : "Username of table creator.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of table, relative to parent schema.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "updated_by" : {
            "readOnly" : true,
            "description" : "Username of user who last modified the table.",
            "type" : "string"
          },
          "sql_path" : {
            "description" : "List of schemes whose objects can be referenced without qualification.",
            "type" : "string"
          },
          "data_source_format" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.DataSourceFormat"
          },
          "full_name" : {
            "readOnly" : true,
            "description" : "Full name of table, in form of __catalog_name__.__schema_name__.__table_name__",
            "type" : "string"
          },
          "delta_runtime_properties_kvpairs" : {
            "readOnly" : true,
            "properties" : { },
            "description" : "Information pertaining to current state of the delta table.",
            "type" : "object"
          },
          "catalog_name" : {
            "description" : "Name of parent catalog.",
            "type" : "string"
          },
          "table_constraints" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.TableConstraintList"
          },
          "schema_name" : {
            "description" : "Name of parent schema relative to its parent catalog.",
            "type" : "string"
          },
          "storage_location" : {
            "description" : "Storage root URL for table (for **MANAGED**, **EXTERNAL** tables)",
            "type" : "string"
          },
          "properties" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.SecurablePropertiesMap"
          },
          "columns" : {
            "description" : "The array of __ColumnInfo__ definitions of the table's columns.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.ColumnInfo"
            }
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "table_id" : {
            "readOnly" : true,
            "description" : "Name of table, relative to parent schema.",
            "x-databricks-id" : true,
            "type" : "string"
          },
          "table_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.TableType"
          },
          "created_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this table was created, in epoch milliseconds.",
            "type" : "integer"
          },
          "row_filter" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.TableRowFilter"
          },
          "owner" : {
            "description" : "Username of current owner of table.",
            "type" : "string"
          },
          "storage_credential_name" : {
            "description" : "Name of the storage credential, when a storage credential is configured for use with this table.",
            "type" : "string"
          },
          "updated_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this table was last modified, in epoch milliseconds.",
            "type" : "integer"
          },
          "view_definition" : {
            "description" : "View definition SQL (when __table_type__ is **VIEW**, **MATERIALIZED_VIEW**, or **STREAMING_TABLE**)",
            "type" : "string"
          },
          "view_dependencies" : {
            "description" : "View dependencies (when table_type == **VIEW** or **MATERIALIZED_VIEW**, **STREAMING_TABLE**)\n- when DependencyList is None, the dependency is not provided;\n- when DependencyList is an empty list, the dependency is provided but is empty;\n- when DependencyList is not an empty list, dependencies are provided and recorded.\n",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.DependencyList"
          },
          "data_access_configuration_id" : {
            "description" : "Unique ID of the Data Access Configuration to use with the table data.",
            "deprecated" : true,
            "type" : "string"
          },
          "deleted_at" : {
            "format" : "int64",
            "readOnly" : true,
            "description" : "Time at which this table was deleted, in epoch milliseconds. Field is omitted if table is not deleted.",
            "type" : "integer"
          },
          "metastore_id" : {
            "readOnly" : true,
            "description" : "Unique identifier of parent metastore.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.TableRowFilter" : {
        "required" : [ "function_name", "input_column_names" ],
        "properties" : {
          "input_column_names" : {
            "description" : "The list of table columns to be passed as input to the row filter function. The column types\nshould match the types of the filter function arguments.\n",
            "type" : "array",
            "items" : {
              "type" : "string"
            }
          },
          "name" : {
            "description" : "The full name of the row filter SQL UDF.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.TableSummary" : {
        "properties" : {
          "full_name" : {
            "description" : "The full name of the table.",
            "type" : "string"
          },
          "table_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.TableType"
          }
        },
        "type" : "object"
      },
      "unitycatalog.TableType" : {
        "type" : "string",
        "enum" : [ "MANAGED", "EXTERNAL", "VIEW", "MATERIALIZED_VIEW", "STREAMING_TABLE" ]
      },
      "unitycatalog.UpdateCatalog" : {
        "properties" : {
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of catalog.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "owner" : {
            "description" : "Username of current owner of catalog.",
            "type" : "string"
          },
          "properties" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.SecurablePropertiesMap"
          }
        },
        "type" : "object"
      },
      "unitycatalog.UpdateExternalLocation" : {
        "properties" : {
          "name" : {
            "description" : "Name of the external location.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "force" : {
            "description" : "Force update even if changing url invalidates dependent external tables or mounts.",
            "type" : "boolean"
          },
          "url" : {
            "description" : "Path URL of the external location.",
            "type" : "string"
          },
          "read_only" : {
            "description" : "Indicates whether the external location is read-only.",
            "type" : "boolean"
          },
          "credential_name" : {
            "description" : "Name of the storage credential used with this location.",
            "type" : "string"
          },
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "owner" : {
            "description" : "The owner of the external location.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.UpdateFunction" : {
        "properties" : {
          "owner" : {
            "description" : "Username of current owner of function.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.UpdateMetastore" : {
        "properties" : {
          "delta_sharing_scope" : {
            "description" : "The scope of Delta Sharing enabled for the metastore.",
            "type" : "string",
            "enum" : [ "INTERNAL", "INTERNAL_AND_EXTERNAL" ]
          },
          "name" : {
            "description" : "The user-specified name of the metastore.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "storage_root_credential_id" : {
            "description" : "UUID of storage credential to access the metastore storage_root.",
            "type" : "string"
          },
          "privilege_model_version" : {
            "description" : "Privilege model version of the metastore, of the form `major.minor` (e.g., `1.0`).",
            "type" : "string"
          },
          "delta_sharing_recipient_token_lifetime_in_seconds" : {
            "format" : "int64",
            "description" : "The lifetime of delta sharing recipient token in seconds.",
            "type" : "integer"
          },
          "delta_sharing_organization_name" : {
            "description" : "The organization name of a Delta Sharing entity, to be used in Databricks-to-Databricks Delta Sharing as the official name.",
            "type" : "string"
          },
          "owner" : {
            "description" : "The owner of the metastore.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.UpdateMetastoreAssignment" : {
        "properties" : {
          "default_catalog_name" : {
            "description" : "The name of the default catalog for the metastore.",
            "type" : "string"
          },
          "metastore_id" : {
            "description" : "The unique ID of the metastore.",
            "x-databricks-id" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.UpdatePermissions" : {
        "properties" : {
          "changes" : {
            "description" : "Array of permissions change objects.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.PermissionsChange"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.UpdateProvider" : {
        "properties" : {
          "comment" : {
            "description" : "Description about the provider.",
            "type" : "string"
          },
          "name" : {
            "description" : "The name of the Provider.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "owner" : {
            "description" : "Username of Provider owner.",
            "type" : "string"
          },
          "recipient_profile_str" : {
            "description" : "This field is required when the __authentication_type__ is **TOKEN** or not provided.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.UpdateRecipient" : {
        "properties" : {
          "name" : {
            "description" : "Name of Recipient.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "ip_access_list" : {
            "description" : "IP Access List",
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.IpAccessList"
          },
          "comment" : {
            "description" : "Description about the recipient.",
            "type" : "string"
          },
          "properties_kvpairs" : {
            "properties" : { },
            "description" : "Recipient properties as map of string key-value pairs.\\n\nWhen provided in update request, the specified properties will override the existing properties. To add and remove properties, one would need to perform a read-modify-write.\n",
            "type" : "object"
          },
          "owner" : {
            "description" : "Username of the recipient owner.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.UpdateSchema" : {
        "properties" : {
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of schema, relative to parent catalog.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "owner" : {
            "description" : "Username of current owner of schema.",
            "type" : "string"
          },
          "properties" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/unitycatalog.SecurablePropertiesMap"
          }
        },
        "type" : "object"
      },
      "unitycatalog.UpdateShare" : {
        "properties" : {
          "comment" : {
            "description" : "User-provided free-form text description.",
            "type" : "string"
          },
          "name" : {
            "description" : "Name of the share.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "owner" : {
            "description" : "Username of current owner of share.",
            "type" : "string"
          },
          "updates" : {
            "description" : "Array of shared data object updates.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.SharedDataObjectUpdate"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.UpdateSharePermissions" : {
        "properties" : {
          "changes" : {
            "description" : "Array of permission changes.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.PermissionsChange"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.UpdateStorageCredential" : {
        "properties" : {
          "name" : {
            "description" : "The credential name. The name must be unique within the metastore.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "force" : {
            "description" : "Force update even if there are dependent external locations or external tables.",
            "type" : "boolean"
          },
          "skip_validation" : {
            "default" : "false",
            "description" : "Supplying true to this argument skips validation of the updated credential.",
            "type" : "boolean"
          },
          "read_only" : {
            "description" : "Whether the storage credential is only usable for read operations.",
            "type" : "boolean"
          },
          "azure_service_principal" : {
            "description" : "The Azure service principal configuration.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/unitycatalog.AzureServicePrincipal"
          },
          "gcp_service_account_key" : {
            "description" : "The GCP service account key configuration.",
            "deprecated" : true,
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/unitycatalog.GcpServiceAccountKey"
          },
          "comment" : {
            "description" : "Comment associated with the credential.",
            "type" : "string"
          },
          "aws_iam_role" : {
            "description" : "The AWS IAM role configuration.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/unitycatalog.AwsIamRole"
          },
          "owner" : {
            "description" : "Username of current owner of credential.",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "unitycatalog.ValidateStorageCredential" : {
        "properties" : {
          "external_location_name" : {
            "description" : "The name of an existing external location to validate.",
            "type" : "string"
          },
          "url" : {
            "description" : "The external location url to validate.",
            "type" : "string"
          },
          "read_only" : {
            "description" : "Whether the storage credential is only usable for read operations.",
            "type" : "boolean"
          },
          "azure_service_principal" : {
            "description" : "The Azure service principal configuration.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "azure",
            "$ref" : "#/components/schemas/unitycatalog.AzureServicePrincipal"
          },
          "gcp_service_account_key" : {
            "description" : "The GCP service account key configuration.",
            "deprecated" : true,
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "gcp",
            "$ref" : "#/components/schemas/unitycatalog.GcpServiceAccountKey"
          },
          "aws_iam_role" : {
            "description" : "The AWS IAM role configuration.",
            "extRef" : true,
            "ref" : true,
            "x-databricks-cloud" : "aws",
            "$ref" : "#/components/schemas/unitycatalog.AwsIamRole"
          },
          "storage_credential_name" : {
            "properties" : { },
            "description" : "The name of the storage credential to validate.",
            "x-databricks-name" : true,
            "type" : "object"
          }
        },
        "type" : "object"
      },
      "unitycatalog.ValidateStorageCredentialResponse" : {
        "properties" : {
          "isDir" : {
            "readOnly" : true,
            "description" : "Whether the tested location is a directory in cloud storage.",
            "type" : "boolean"
          },
          "results" : {
            "readOnly" : true,
            "description" : "The results of the validation check.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/unitycatalog.ValidationResult"
            }
          }
        },
        "type" : "object"
      },
      "unitycatalog.ValidationResult" : {
        "properties" : {
          "message" : {
            "description" : "Error message would exist when the result does not equal to **PASS**.",
            "type" : "string"
          },
          "operation" : {
            "description" : "The operation tested.",
            "type" : "string",
            "enum" : [ "LIST", "READ", "WRITE", "DELETE" ]
          },
          "result" : {
            "description" : "The results of the tested operation.",
            "type" : "string",
            "enum" : [ "PASS", "FAIL", "SKIP" ]
          }
        },
        "type" : "object"
      },
      "workspace.Delete" : {
        "required" : [ "path" ],
        "properties" : {
          "path" : {
            "description" : "The absolute path of the notebook or directory.",
            "type" : "string"
          },
          "recursive" : {
            "default" : "false",
            "description" : "The flag that specifies whether to delete the object recursively. It is `false` by default.\nPlease note this deleting directory is not atomic. If it fails in the middle, some of objects\nunder this directory may be deleted and cannot be undone.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "workspace.DeleteResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "workspace.ExportFormat" : {
        "default" : "SOURCE",
        "description" : "This specifies the format of the file to be imported. By default, this is `SOURCE`. However it may be one of:\n`SOURCE`, `HTML`, `JUPYTER`, `DBC`. The value is case sensitive.",
        "type" : "string",
        "enum" : [ "SOURCE", "HTML", "JUPYTER", "DBC", "R_MARKDOWN" ]
      },
      "workspace.ExportResponse" : {
        "properties" : {
          "content" : {
            "format" : "string",
            "description" : "The base64-encoded content.\nIf the limit (10MB) is exceeded, exception with error code **MAX_NOTEBOOK_SIZE_EXCEEDED** will be thrown.",
            "x-databricks-base64" : true,
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "workspace.Import" : {
        "required" : [ "path" ],
        "properties" : {
          "format" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/workspace.ExportFormat"
          },
          "path" : {
            "description" : "The absolute path of the notebook or directory. Importing directory is only support for `DBC` format.",
            "type" : "string"
          },
          "language" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/workspace.Language"
          },
          "content" : {
            "format" : "string",
            "description" : "The base64-encoded content. This has a limit of 10 MB.\n\nIf the limit (10MB) is exceeded, exception with error code **MAX_NOTEBOOK_SIZE_EXCEEDED** will be thrown.\nThis parameter might be absent, and instead a posted file will be used.\n",
            "x-databricks-base64" : true,
            "type" : "string"
          },
          "overwrite" : {
            "default" : "false",
            "description" : "The flag that specifies whether to overwrite existing object. It is `false` by default.\nFor `DBC` format, `overwrite` is not supported since it may contain a directory.",
            "type" : "boolean"
          }
        },
        "type" : "object"
      },
      "workspace.ImportResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "workspace.Language" : {
        "description" : "The language of the object. This value is set only if the object type is `NOTEBOOK`.",
        "type" : "string",
        "enum" : [ "SCALA", "PYTHON", "SQL", "R" ]
      },
      "workspace.ListResponse" : {
        "properties" : {
          "objects" : {
            "description" : "List of objects.",
            "type" : "array",
            "items" : {
              "extRef" : true,
              "ref" : true,
              "$ref" : "#/components/schemas/workspace.ObjectInfo"
            }
          }
        },
        "type" : "object"
      },
      "workspace.Mkdirs" : {
        "required" : [ "path" ],
        "properties" : {
          "path" : {
            "description" : "The absolute path of the directory. If the parent directories do not exist, it will also create them.\nIf the directory already exists, this command will do nothing and succeed.\n",
            "type" : "string"
          }
        },
        "type" : "object"
      },
      "workspace.MkdirsResponse" : {
        "properties" : { },
        "type" : "object"
      },
      "workspace.ObjectInfo" : {
        "properties" : {
          "path" : {
            "description" : "The absolute path of the object.",
            "x-databricks-name" : true,
            "type" : "string"
          },
          "size" : {
            "format" : "int64",
            "description" : "<content needed>",
            "type" : "integer"
          },
          "object_type" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/workspace.ObjectType"
          },
          "language" : {
            "extRef" : true,
            "ref" : true,
            "$ref" : "#/components/schemas/workspace.Language"
          },
          "modified_at" : {
            "format" : "int64",
            "description" : "<content needed>",
            "type" : "integer"
          },
          "created_at" : {
            "format" : "int64",
            "description" : "<content needed>",
            "type" : "integer"
          },
          "object_id" : {
            "format" : "int64",
            "description" : "<content needed>",
            "x-databricks-id" : true,
            "type" : "integer"
          }
        },
        "type" : "object"
      },
      "workspace.ObjectType" : {
        "description" : "The type of the object in workspace.",
        "type" : "string",
        "enum" : [ "NOTEBOOK", "DIRECTORY", "LIBRARY", "FILE", "REPO" ]
      },
      "workspaceconf.WorkspaceConf" : {
        "properties" : { },
        "type" : "object",
        "additionalProperties" : {
          "type" : "string"
        }
      }
    }
  },
  "info" : {
    "title" : "Databricks Workspace REST API on ALL"
  },
  "tags" : [ {
    "name" : "Account Groups",
    "description" : "Groups simplify identity management, making it easier to assign access to Databricks Account, data,\nand other securable objects.\n\nIt is best practice to assign access to workspaces and access-control policies in\nUnity Catalog to groups, instead of to users individually. All Databricks Account identities can be\nassigned as members of groups, and members inherit permissions that are assigned to their\ngroup.\n",
    "x-databricks-terraform-resource" : "databricks_group",
    "x-databricks-package" : "scim",
    "x-databricks-service" : "AccountGroups",
    "x-databricks-is-accounts" : true
  }, {
    "name" : "Account Metastore Assignments",
    "description" : "These APIs manage metastore assignments to a workspace.\n",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "AccountMetastoreAssignments",
    "x-databricks-is-accounts" : true
  }, {
    "name" : "Account Metastores",
    "description" : "These APIs manage Unity Catalog metastores for an account. A metastore contains catalogs\nthat can be associated with workspaces\n",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "AccountMetastores",
    "x-databricks-is-accounts" : true
  }, {
    "name" : "Account Service Principals",
    "description" : "Identities for use with jobs, automated tools, and systems such as scripts, apps, and\nCI/CD platforms. Databricks recommends creating service principals to run production jobs\nor modify production data. If all processes that act on production data run with service\nprincipals, interactive users do not need any write, delete, or modify privileges in\nproduction. This eliminates the risk of a user overwriting production data by accident.\n",
    "x-databricks-terraform-resource" : "databricks_service_principal",
    "x-databricks-package" : "scim",
    "x-databricks-service" : "AccountServicePrincipals",
    "x-databricks-is-accounts" : true
  }, {
    "name" : "Account Storage Credentials",
    "description" : "These APIs manage storage credentials for a particular metastore.\n",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "AccountStorageCredentials",
    "x-databricks-is-accounts" : true
  }, {
    "name" : "Account Users",
    "description" : "User identities recognized by Databricks and represented by email addresses.\n\nDatabricks recommends using SCIM provisioning to sync users and groups automatically from\nyour identity provider to your Databricks Account. SCIM streamlines onboarding a new\nemployee or team by using your identity provider to create users and groups in Databricks Account\nand give them the proper level of access. When a user leaves your organization or no longer\nneeds access to Databricks Account, admins can terminate the user in your identity provider and that\nusers account will also be removed from Databricks Account. This ensures a consistent offboarding\nprocess and prevents unauthorized users from accessing sensitive data.\n",
    "x-databricks-terraform-resource" : "databricks_user",
    "x-databricks-package" : "scim",
    "x-databricks-service" : "AccountUsers",
    "x-databricks-is-accounts" : true
  }, {
    "name" : "Alerts",
    "description" : "The alerts API can be used to perform CRUD operations on alerts. An alert is a Databricks SQL\nobject that periodically runs a query, evaluates a condition of its result, and notifies one\nor more users and/or alert destinations if the condition was met.\n\n**Note**: Programmatic operations on refresh schedules via the Databricks SQL API are deprecated. Alert refresh schedules can be created, updated, fetched and deleted using Jobs API, e.g. :method:jobs/create.\n",
    "x-databricks-package" : "sql",
    "x-databricks-service" : "Alerts",
    "x-databricks-preview" : "PUBLIC"
  }, {
    "name" : "Billable usage download",
    "description" : "This API allows you to download billable usage logs for the specified account and date range.\nThis feature works with all account types.\n",
    "x-databricks-package" : "billing",
    "x-databricks-service" : "BillableUsage",
    "x-databricks-is-accounts" : true,
    "x-databricks-cloud" : "aws"
  }, {
    "name" : "Budgets",
    "description" : "These APIs manage budget configuration including notifications for exceeding a budget for\na period. They can also retrieve the status of each budget.\n",
    "x-databricks-package" : "billing",
    "x-databricks-service" : "Budgets",
    "x-databricks-is-accounts" : true,
    "x-databricks-preview" : "PRIVATE",
    "x-databricks-cloud" : "aws"
  }, {
    "name" : "Catalogs",
    "description" : "A catalog is the first layer of Unity Catalogs three-level namespace. Its used to organize\nyour data assets. Users can see all catalogs on which they have been assigned the USE_CATALOG\ndata permission.\n\nIn Unity Catalog, admins and data stewards manage users and their access to data centrally\nacross all of the workspaces in a Databricks account. Users in different workspaces can\nshare access to the same data, depending on privileges granted centrally in Unity Catalog.\n",
    "x-databricks-terraform-resource" : "databricks_catalog",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "Catalogs"
  }, {
    "name" : "Cluster Policies",
    "description" : "Cluster policy limits the ability to configure clusters based on a set of rules. The policy\nrules limit the attributes or attribute values available for cluster creation. Cluster\npolicies have ACLs that limit their use to specific users and groups.\n\nCluster policies let you limit users to create clusters with prescribed settings, simplify\nthe user interface and enable more users to create their own clusters (by fixing and hiding\nsome values), control cost by limiting per cluster maximum cost (by setting limits on\nattributes whose values contribute to hourly price).\n\nCluster policy permissions limit which policies a user can select in the Policy drop-down\nwhen the user creates a cluster:\n- A user who has cluster create permission can select the Unrestricted policy and create\n  fully-configurable clusters.\n- A user who has both cluster create permission and access to cluster policies can select\n  the Unrestricted policy and policies they have access to.\n- A user that has access to only cluster policies, can select the policies they have access to.\n\nIf no policies have been created in the workspace, the Policy drop-down does not display.\n\nOnly admin users can create, edit, and delete policies.\nAdmin users also have access to all policies.\n",
    "x-databricks-terraform-resource" : "databricks_cluster_policy",
    "x-databricks-package" : "clusterpolicies",
    "x-databricks-service" : "ClusterPolicies",
    "x-databricks-path-style" : "rpc",
    "x-databricks-group" : "admin"
  }, {
    "name" : "Clusters",
    "description" : "The Clusters API allows you to create, start, edit, list, terminate, and delete clusters.\n\nDatabricks maps cluster node instance types to compute units known as DBUs. See the instance\ntype pricing page for a list of the supported instance types and their corresponding DBUs.\n\nA Databricks cluster is a set of computation resources and configurations on which you run\ndata engineering, data science, and data analytics workloads, such as production\nETL pipelines, streaming analytics, ad-hoc analytics, and machine learning.\n\nYou run these workloads as a set of commands in a notebook or as an automated job.\nDatabricks makes a distinction between all-purpose clusters and job clusters. You use\nall-purpose clusters to analyze data collaboratively using interactive notebooks. You use\njob clusters to run fast and robust automated jobs.\n\nYou can create an all-purpose cluster using the UI, CLI, or REST API. You can manually\nterminate and restart an all-purpose cluster. Multiple users can share such clusters to do\ncollaborative interactive analysis.\n\nIMPORTANT: Databricks retains cluster configuration information for up to 200 all-purpose\nclusters terminated in the last 30 days and up to 30 job clusters recently terminated by\nthe job scheduler. To keep an all-purpose cluster configuration even after it has been\nterminated for more than 30 days, an administrator can pin a cluster to the cluster list.\n",
    "x-databricks-terraform-resource" : "databricks_cluster",
    "x-databricks-package" : "clusters",
    "x-databricks-service" : "Clusters",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "Command Execution",
    "description" : "This API allows execution of Python, Scala, SQL, or R commands on running\nDatabricks Clusters.\n",
    "x-databricks-package" : "commands",
    "x-databricks-service" : "CommandExecution"
  }, {
    "name" : "Credential configurations",
    "description" : "These APIs manage credential configurations for this workspace. Databricks needs access to\na cross-account service IAM role in your AWS account so that Databricks can deploy clusters\nin the appropriate VPC for the new workspace. A credential configuration encapsulates this\nrole information, and its ID is used when creating a new workspace.\n",
    "x-databricks-terraform-resource" : "databricks_mws_credentials",
    "x-databricks-package" : "deployment",
    "x-databricks-service" : "Credentials",
    "x-databricks-is-accounts" : true,
    "x-databricks-cloud" : "aws"
  }, {
    "name" : "CurrentUser",
    "description" : "This API allows retrieving information about currently authenticated user or\nservice principal.\n",
    "x-databricks-terraform-resource" : "databricks_current_user",
    "x-databricks-package" : "scim",
    "x-databricks-service" : "CurrentUser",
    "x-databricks-preview" : "PUBLIC"
  }, {
    "name" : "Dashboards",
    "description" : "In general, there is little need to modify dashboards using the API. However, it can be\nuseful to use dashboard objects to look-up a collection of related query IDs. The API can\nalso be used to duplicate multiple dashboards at once since you can get a dashboard\ndefinition with a GET request and then POST it to create a new one.\n\n**Note**: Programmatic operations on refresh schedules via the Databricks SQL API are deprecated. Dashboard refresh schedules can be created, updated, fetched and deleted using Jobs API, e.g. :method:jobs/create.\n",
    "x-databricks-terraform-resource" : "databricks_dashboard",
    "x-databricks-package" : "sql",
    "x-databricks-service" : "Dashboards"
  }, {
    "name" : "Data Sources",
    "description" : "This API is provided to assist you in making new query objects. When creating a query object,\nyou may optionally specify a `data_source_id` for the SQL warehouse against which it will run.\nIf you don't already know the `data_source_id` for your desired SQL warehouse, this API will\nhelp you find it.\n\nThis API does not support searches. It returns the full list of SQL warehouses in your\nworkspace. We advise you to use any text editor, REST client, or `grep` to search the\nresponse from this API for the name of your SQL warehouse as it appears in Databricks SQL.\n",
    "x-databricks-package" : "sql",
    "x-databricks-service" : "DataSources"
  }, {
    "name" : "Dbfs",
    "description" : "DBFS API makes it simple to interact with various data sources without having to include\na users credentials every time to read a file.\n",
    "x-databricks-terraform-resource" : "databricks_dbfs_file",
    "x-databricks-package" : "dbfs",
    "x-databricks-service" : "Dbfs",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "ACL / Permissions",
    "description" : "The SQL Permissions API is similar to the endpoints of the :method:permissions/set. However, this\nexposes only one endpoint, which gets the Access Control List for a given object. You cannot\nmodify any permissions using this API.\n\nThere are three levels of permission:\n\n- `CAN_VIEW`: Allows read-only access\n\n- `CAN_RUN`: Allows read access and run access (superset of `CAN_VIEW`)\n\n- `CAN_MANAGE`: Allows all actions: read, run, edit, delete, modify permissions (superset of `CAN_RUN`)\n",
    "x-databricks-package" : "sql",
    "x-databricks-service" : "DbsqlPermissions"
  }, {
    "name" : "Key configurations",
    "description" : "These APIs manage encryption key configurations for this workspace (optional). A key\nconfiguration encapsulates the AWS KMS key information and some information about how\nthe key configuration can be used. There are two possible uses for key configurations:\n\n* Managed services: A key configuration can be used to encrypt a workspace's notebook and\nsecret data in the control plane, as well as Databricks SQL queries and query history.\n* Storage: A key configuration can be used to encrypt a workspace's DBFS and EBS data in\nthe data plane.\n\nIn both of these cases, the key configuration's ID is used when creating a new workspace.\nThis Preview feature is available if your account is on the E2 version of the platform.\nUpdating a running workspace with workspace storage encryption requires that the workspace\nis on the E2 version of the platform. If you have an older workspace, it might not be on\nthe E2 version of the platform. If you are not sure, contact your Databricks reprsentative.\n",
    "x-databricks-terraform-resource" : "databricks_mws_customer_managed_keys",
    "x-databricks-package" : "deployment",
    "x-databricks-service" : "EncryptionKeys",
    "x-databricks-is-accounts" : true,
    "x-databricks-cloud" : "aws"
  }, {
    "name" : "MLflow Experiments",
    "x-databricks-package" : "mlflow",
    "x-databricks-service" : "Experiments",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "External Locations",
    "description" : "An external location is an object that combines a cloud storage path with a storage\ncredential that authorizes access to the cloud storage path. Each external location is\nsubject to Unity Catalog access-control policies that control which users and groups can\naccess the credential. If a user does not have access to an external location in Unity\nCatalog, the request fails and Unity Catalog does not attempt to authenticate to your cloud\ntenant on the users behalf.\n\nDatabricks recommends using external locations rather than using storage credentials\ndirectly.\n\nTo create external locations, you must be a metastore admin or a user with the\n**CREATE_EXTERNAL_LOCATION** privilege.\n",
    "x-databricks-terraform-resource" : "databricks_external_location",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "ExternalLocations"
  }, {
    "name" : "Functions",
    "description" : "Functions implement User-Defined Functions (UDFs) in Unity Catalog.\n\nThe function implementation can be any SQL expression or Query, and it can be invoked wherever a table reference is allowed in a query.\nIn Unity Catalog, a function resides at the same level as a table, so it can be referenced with the form __catalog_name__.__schema_name__.__function_name__.\n",
    "x-databricks-terraform-resource" : "databricks_function",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "Functions"
  }, {
    "name" : "Git Credentials",
    "description" : "Registers personal access token for Databricks to do operations on behalf of the user.\n\nSee [more info](https://docs.databricks.com/repos/get-access-tokens-from-git-provider.html).\n",
    "x-databricks-terraform-resource" : "databricks_git_credentials",
    "x-databricks-package" : "gitcredentials",
    "x-databricks-service" : "GitCredentials"
  }, {
    "name" : "Global Init Scripts",
    "description" : "The Global Init Scripts API enables Workspace administrators to configure global\ninitialization scripts for their workspace. These scripts run on every node in every cluster\nin the workspace.\n\n**Important:** Existing clusters must be restarted to pick up any changes made to global\ninit scripts.\nGlobal init scripts are run in order. If the init script returns with a bad exit code,\nthe Apache Spark container fails to launch and init scripts with later position are skipped.\nIf enough containers fail, the entire cluster fails with a `GLOBAL_INIT_SCRIPT_FAILURE`\nerror code.\n",
    "x-databricks-terraform-resource" : "databricks_global_init_script",
    "x-databricks-package" : "globalinitscripts",
    "x-databricks-service" : "GlobalInitScripts",
    "x-databricks-group" : "admin"
  }, {
    "name" : "Grants",
    "description" : "In Unity Catalog, data is secure by default. Initially, users have no access to data in\na metastore. Access can be granted by either a metastore admin, the owner of an object, or\nthe owner of the catalog or schema that contains the object. Securable objects in Unity\nCatalog are hierarchical and privileges are inherited downward.\n\nSecurable objects in Unity Catalog are hierarchical and privileges are inherited downward.\nThis means that granting a privilege on the catalog automatically grants the privilege to\nall current and future objects within the catalog. Similarly, privileges granted on a schema\nare inherited by all current and future objects within that schema.\n",
    "x-databricks-terraform-resource" : "databricks_grant",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "Grants"
  }, {
    "name" : "Groups",
    "description" : "Groups simplify identity management, making it easier to assign access to Databricks Workspace, data,\nand other securable objects.\n\nIt is best practice to assign access to workspaces and access-control policies in\nUnity Catalog to groups, instead of to users individually. All Databricks Workspace identities can be\nassigned as members of groups, and members inherit permissions that are assigned to their\ngroup.\n",
    "x-databricks-terraform-resource" : "databricks_group",
    "x-databricks-package" : "scim",
    "x-databricks-service" : "Groups",
    "x-databricks-preview" : "PUBLIC"
  }, {
    "name" : "Instance Pools",
    "description" : "Instance Pools API are used to create, edit, delete and list instance pools by using\nready-to-use cloud instances which reduces a cluster start and auto-scaling times.\n\nDatabricks pools reduce cluster start and auto-scaling times by maintaining a set of idle,\nready-to-use instances. When a cluster is attached to a pool, cluster nodes are created using\nthe pools idle instances. If the pool has no idle instances, the pool expands by allocating\na new instance from the instance provider in order to accommodate the clusters request.\nWhen a cluster releases an instance, it returns to the pool and is free for another cluster\nto use. Only clusters attached to a pool can use that pools idle instances.\n\nYou can specify a different pool for the driver node and worker nodes, or use the same pool\nfor both.\n\nDatabricks does not charge DBUs while instances are idle in the pool. Instance provider\nbilling does apply. See pricing.\n",
    "x-databricks-terraform-resource" : "databricks_instance_pool",
    "x-databricks-package" : "instancepools",
    "x-databricks-service" : "InstancePools",
    "x-databricks-path-style" : "rpc",
    "x-databricks-group" : "admin"
  }, {
    "name" : "Instance Profiles",
    "description" : "The Instance Profiles API allows admins to add, list, and remove instance profiles that users can launch\nclusters with. Regular users can list the instance profiles available to them.\nSee [Secure access to S3 buckets](https://docs.databricks.com/administration-guide/cloud-configurations/aws/instance-profiles.html) using\ninstance profiles for more information.\n",
    "x-databricks-terraform-resource" : "databricks_instance_profile",
    "x-databricks-package" : "clusters",
    "x-databricks-service" : "InstanceProfiles",
    "x-databricks-path-style" : "rpc",
    "x-databricks-cloud" : "aws"
  }, {
    "name" : "IP Access Lists",
    "description" : "IP Access List enables admins to configure IP access lists.\n\nIP access lists affect web application access and REST API access to this workspace only.\nIf the feature is disabled for a workspace, all access is allowed for this workspace.\nThere is support for allow lists (inclusion) and block lists (exclusion).\n\nWhen a connection is attempted:\n  1. **First, all block lists are checked.** If the connection IP address matches any block list, the connection is rejected.\n  2. **If the connection was not rejected by block lists**, the IP address is compared with the allow lists.\n\nIf there is at least one allow list for the workspace, the connection is allowed only if the IP address matches an allow list.\nIf there are no allow lists for the workspace, all IP addresses are allowed.\n\nFor all allow lists and block lists combined, the workspace supports a maximum of 1000 IP/CIDR values, where one CIDR counts as a single value.\n\nAfter changes to the IP access list feature, it can take a few minutes for changes to take effect.\n",
    "x-databricks-terraform-resource" : "databricks_ip_access_list",
    "x-databricks-package" : "ipaccesslists",
    "x-databricks-service" : "IpAccessLists",
    "x-databricks-group" : "admin"
  }, {
    "name" : "Jobs",
    "description" : "The Jobs API allows you to create, edit, and delete jobs.\n\nYou can use a Databricks job to run a data processing or data analysis task in a Databricks\ncluster with scalable resources. Your job can consist of a single task or can be a large,\nmulti-task workflow with complex dependencies. Databricks manages the task orchestration,\ncluster management, monitoring, and error reporting for all of your jobs. You can run your\njobs immediately or periodically through an easy-to-use scheduling system. You can implement\njob tasks using notebooks, JARS, Delta Live Tables pipelines, or Python, Scala, Spark\nsubmit, and Java applications.\n\nYou should never hard code secrets or store them in plain text. Use the :service:secrets to manage secrets in the\n[Databricks CLI](https://docs.databricks.com/dev-tools/cli/index.html).\nUse the [Secrets utility](https://docs.databricks.com/dev-tools/databricks-utils.html#dbutils-secrets) to reference secrets in notebooks and jobs.\n",
    "x-databricks-terraform-resource" : "databricks_job",
    "x-databricks-package" : "jobs",
    "x-databricks-service" : "Jobs"
  }, {
    "name" : "ManagedLibraries",
    "description" : "The Libraries API allows you to install and uninstall libraries and get the status of\nlibraries on a cluster.\n\nTo make third-party or custom code available to notebooks and jobs running on your clusters,\nyou can install a library. Libraries can be written in Python, Java, Scala, and R. You can\nupload Java, Scala, and Python libraries and point to external packages in PyPI, Maven, and\nCRAN repositories.\n\nCluster libraries can be used by all notebooks running on a cluster. You can install a cluster\nlibrary directly from a public repository such as PyPI or Maven, using a previously installed\nworkspace library, or using an init script.\n\nWhen you install a library on a cluster, a notebook already attached to that cluster will not\nimmediately see the new library. You must first detach and then reattach the notebook to\nthe cluster.\n\nWhen you uninstall a library from a cluster, the library is removed only when you restart\nthe cluster. Until you restart the cluster, the status of the uninstalled library appears\nas Uninstall pending restart.\n",
    "x-databricks-terraform-resource" : "databricks_library",
    "x-databricks-package" : "libraries",
    "x-databricks-service" : "Libraries",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "Log delivery configurations",
    "description" : "These APIs manage log delivery configurations for this account. The two supported log types\nfor this API are _billable usage logs_ and _audit logs_. This feature is in Public Preview.\nThis feature works with all account ID types.\n\nLog delivery works with all account types. However, if your account is on the E2 version of\nthe platform or on a select custom plan that allows multiple workspaces per account, you can\noptionally configure different storage destinations for each workspace. Log delivery status\nis also provided to know the latest status of log delivery attempts.\nThe high-level flow of billable usage delivery:\n\n1. **Create storage**: In AWS, [create a new AWS S3 bucket](https://docs.databricks.com/administration-guide/account-api/aws-storage.html)\nwith a specific bucket policy. Using Databricks APIs, call the Account API to create a [storage configuration object](#operation/create-storage-config)\nthat uses the bucket name.\n2. **Create credentials**: In AWS, create the appropriate AWS IAM role. For full details,\nincluding the required IAM role policies and trust relationship, see\n[Billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).\nUsing Databricks APIs, call the Account API to create a [credential configuration object](#operation/create-credential-config)\nthat uses the IAM role's ARN.\n3. **Create log delivery configuration**: Using Databricks APIs, call the Account API to\n[create a log delivery configuration](#operation/create-log-delivery-config) that uses\nthe credential and storage configuration objects from previous steps. You can specify if\nthe logs should include all events of that log type in your account (_Account level_ delivery)\nor only events for a specific set of workspaces (_workspace level_ delivery). Account level\nlog delivery applies to all current and future workspaces plus account level logs, while\nworkspace level log delivery solely delivers logs related to the specified workspaces.\nYou can create multiple types of delivery configurations per account.\n\nFor billable usage delivery:\n* For more information about billable usage logs, see\n[Billable usage log delivery](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html).\nFor the CSV schema, see the [Usage page](https://docs.databricks.com/administration-guide/account-settings/usage.html).\n* The delivery location is `<bucket-name>/<prefix>/billable-usage/csv/`, where `<prefix>` is\nthe name of the optional delivery path prefix you set up during log delivery configuration.\nFiles are named `workspaceId=<workspace-id>-usageMonth=<month>.csv`.\n* All billable usage logs apply to specific workspaces (_workspace level_ logs). You can\naggregate usage for your entire account by creating an _account level_ delivery\nconfiguration that delivers logs for all current and future workspaces in your account.\n* The files are delivered daily by overwriting the month's CSV file for each workspace.\n\nFor audit log delivery:\n* For more information about about audit log delivery, see\n[Audit log delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html),\nwhich includes information about the used JSON schema.\n* The delivery location is `<bucket-name>/<delivery-path-prefix>/workspaceId=<workspaceId>/date=<yyyy-mm-dd>/auditlogs_<internal-id>.json`.\nFiles may get overwritten with the same content multiple times to achieve exactly-once delivery.\n* If the audit log delivery configuration included specific workspace IDs, only\n_workspace-level_ audit logs for those workspaces are delivered. If the log delivery\nconfiguration applies to the entire account (_account level_ delivery configuration),\nthe audit log delivery includes workspace-level audit logs for all workspaces in the account\nas well as account-level audit logs. See\n[Audit log delivery](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html) for details.\n* Auditable events are typically available in logs within 15 minutes.\n",
    "x-databricks-terraform-resource" : "databricks_mws_log_delivery",
    "x-databricks-package" : "billing",
    "x-databricks-service" : "LogDelivery",
    "x-databricks-is-accounts" : true,
    "x-databricks-cloud" : "aws"
  }, {
    "name" : "MLflow Artifacts",
    "x-databricks-package" : "mlflow",
    "x-databricks-service" : "MLflowArtifacts",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "Databricks versions of MLflow API endpoints",
    "description" : "These endpoints are modified versions of the MLflow API that accept additional input parameters or return additional information.",
    "x-databricks-package" : "mlflow",
    "x-databricks-service" : "MLflowDatabricks",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "MLflow Metrics",
    "x-databricks-package" : "mlflow",
    "x-databricks-service" : "MLflowMetrics",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "MLflow Runs",
    "x-databricks-package" : "mlflow",
    "x-databricks-service" : "MLflowRuns",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "Metastores",
    "description" : "A metastore is the top-level container of objects in Unity Catalog. It stores data assets\n(tables and views) and the permissions that govern access to them. Databricks account admins\ncan create metastores and assign them to Databricks workspaces to control which workloads\nuse each metastore. For a workspace to use Unity Catalog, it must have a Unity Catalog\nmetastore attached.\n\nEach metastore is configured with a root storage location in a cloud storage account.\nThis storage location is used for metadata and managed tables data.\n\nNOTE: This metastore is distinct from the metastore included in Databricks workspaces\ncreated before Unity Catalog was released. If your workspace includes a legacy Hive\nmetastore, the data in that metastore is available in a catalog named hive_metastore.\n",
    "x-databricks-terraform-resource" : "databricks_metastore",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "Metastores"
  }, {
    "name" : "MLflow Model Version Comments",
    "x-databricks-package" : "mlflow",
    "x-databricks-service" : "ModelVersionComments",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "MLflow Model Versions",
    "description" : "",
    "x-databricks-package" : "mlflow",
    "x-databricks-service" : "ModelVersions",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "Network configurations",
    "description" : "These APIs manage network configurations for customer-managed VPCs (optional). Its ID is\nused when creating a new workspace if you use customer-managed VPCs.\n",
    "x-databricks-terraform-resource" : "databricks_mws_networks",
    "x-databricks-package" : "deployment",
    "x-databricks-service" : "Networks",
    "x-databricks-is-accounts" : true,
    "x-databricks-cloud" : "aws"
  }, {
    "name" : "Permissions",
    "description" : "Permissions API are used to create read, write, edit, update and manage access for various\nusers on different objects and endpoints.\n",
    "x-databricks-terraform-resource" : "databricks_permissions",
    "x-databricks-package" : "permissions",
    "x-databricks-service" : "Permissions",
    "x-databricks-group" : "admin"
  }, {
    "name" : "Delta Live Tables",
    "description" : "The Delta Live Tables API allows you to create, edit, delete, start, and view details about\npipelines.\n\nDelta Live Tables is a framework for building reliable, maintainable, and testable data\nprocessing pipelines. You define the transformations to perform on your data, and Delta Live\nTables manages task orchestration, cluster management, monitoring, data quality, and error\nhandling.\n\nInstead of defining your data pipelines using a series of separate Apache Spark tasks, Delta\nLive Tables manages how your data is transformed based on a target schema you define for each\nprocessing step. You can also enforce data quality with Delta Live Tables expectations.\nExpectations allow you to define expected data quality and specify how to handle records that\nfail those expectations.\n",
    "x-databricks-terraform-resource" : "databricks_pipeline",
    "x-databricks-package" : "pipelines",
    "x-databricks-service" : "Pipelines"
  }, {
    "name" : "Policy Families",
    "description" : "View available policy families. A policy family contains a policy definition providing best\npractices for configuring clusters for a particular use case.\n\nDatabricks manages and provides policy families for several common cluster use cases. You\ncannot create, edit, or delete policy families.\n\nPolicy families cannot be used directly to create clusters. Instead, you create cluster\npolicies using a policy family. Cluster policies created using a policy family inherit the\npolicy family's policy definition.\n",
    "x-databricks-package" : "clusterpolicies",
    "x-databricks-service" : "PolicyFamilies",
    "x-databricks-path-style" : "rest",
    "x-databricks-group" : "admin",
    "x-databricks-not-cloud" : "gcp"
  }, {
    "name" : "Private Access Settings",
    "description" : "These APIs manage private access settings for this account. A private access settings object\nspecifies how your workspace is accessed using AWS PrivateLink. Each workspace that has any\nPrivateLink connections must include the ID for a private access settings object is in its\nworkspace configuration object. Your account must be enabled for PrivateLink to use these\nAPIs. Before configuring PrivateLink, it is important to read the\n[Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n",
    "x-databricks-terraform-resource" : "databricks_mws_private_access_settings",
    "x-databricks-package" : "deployment",
    "x-databricks-service" : "PrivateAccess",
    "x-databricks-is-accounts" : true,
    "x-databricks-preview" : "PUBLIC",
    "x-databricks-cloud" : "aws"
  }, {
    "name" : "Delta Sharing: Providers",
    "description" : "Databricks Delta Sharing: Providers REST API",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "Providers"
  }, {
    "name" : "Queries / Results",
    "description" : "These endpoints are used for CRUD operations on query definitions. Query definitions include\nthe target SQL warehouse, query text, name, description, tags, execution schedule,\nparameters, and visualizations.\n\n**Note**: Programmatic operations on refresh schedules via the Databricks SQL API are deprecated. Query refresh schedules can be created, updated, fetched and deleted using Jobs API, e.g. :method:jobs/create.\n",
    "x-databricks-terraform-resource" : "databricks_query",
    "x-databricks-package" : "sql",
    "x-databricks-service" : "Queries"
  }, {
    "name" : "Query History",
    "description" : "Access the history of queries through SQL warehouses.",
    "x-databricks-package" : "sql",
    "x-databricks-service" : "QueryHistory"
  }, {
    "name" : "Delta Sharing: Recipient Activation",
    "description" : "Databricks Delta Sharing: Recipient Activation REST API",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "RecipientActivation"
  }, {
    "name" : "Delta Sharing: Recipients",
    "description" : "Databricks Delta Sharing: Recipients REST API",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "Recipients"
  }, {
    "name" : "MLflow Registered Models",
    "description" : "",
    "x-databricks-package" : "mlflow",
    "x-databricks-service" : "RegisteredModels",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "MLflow Registry Webhooks",
    "x-databricks-package" : "mlflow",
    "x-databricks-service" : "RegistryWebhooks",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "Repos",
    "description" : "The Repos API allows users to manage their git repos. Users can use the API to access all\nrepos that they have manage permissions on.\n\nDatabricks Repos is a visual Git client in Databricks. It supports common Git operations\nsuch a cloning a repository, committing and pushing, pulling, branch management, and visual\ncomparison of diffs when committing.\n\nWithin Repos you can develop code in notebooks or other files and follow data science and\nengineering code development best practices using Git for version control, collaboration,\nand CI/CD.\n",
    "x-databricks-terraform-resource" : "databricks_repo",
    "x-databricks-package" : "repos",
    "x-databricks-service" : "Repos"
  }, {
    "name" : "Schemas",
    "description" : "A schema (also called a database) is the second layer of Unity Catalogs three-level\nnamespace. A schema organizes tables, views and functions. To access (or list) a table or view in\na schema, users must have the USE_SCHEMA data permission on the schema and its parent catalog,\nand they must have the SELECT permission on the table or view.\n",
    "x-databricks-terraform-resource" : "databricks_schema",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "Schemas"
  }, {
    "name" : "Secret",
    "description" : "The Secrets API allows you to manage secrets, secret scopes, and access permissions.\n\nSometimes accessing data requires that you authenticate to external data sources through JDBC.\nInstead of directly entering your credentials into a notebook, use Databricks secrets to store\nyour credentials and reference them in notebooks and jobs.\n\nAdministrators, secret creators, and users granted permission can read Databricks secrets.\nWhile Databricks makes an effort to redact secret values that might be displayed in notebooks,\nit is not possible to prevent such users from reading secrets.\n",
    "x-databricks-terraform-resource" : "databricks_secret",
    "x-databricks-package" : "secrets",
    "x-databricks-service" : "Secrets"
  }, {
    "name" : "Service Principals",
    "description" : "Identities for use with jobs, automated tools, and systems such as scripts, apps, and\nCI/CD platforms. Databricks recommends creating service principals to run production jobs\nor modify production data. If all processes that act on production data run with service\nprincipals, interactive users do not need any write, delete, or modify privileges in\nproduction. This eliminates the risk of a user overwriting production data by accident.\n",
    "x-databricks-terraform-resource" : "databricks_service_principal",
    "x-databricks-package" : "scim",
    "x-databricks-service" : "ServicePrincipals",
    "x-databricks-preview" : "PUBLIC"
  }, {
    "name" : "Serving endpoints",
    "description" : "The Serverless Real-Time Inference Serving Endpoints API allows you to create, update, and delete model serving endpoints.\n\nYou can use a serving endpoint to serve models from the Databricks Model Registry. Endpoints expose\nthe underlying models as scalable REST API endpoints using serverless compute. This means \nthe endpoints and associated compute resources are fully managed by Databricks and will not appear in \nyour cloud account. A serving endpoint can consist of one or more MLflow models from the Databricks \nModel Registry, called served models. A serving endpoint can have at most ten served models. You can configure \ntraffic settings to define how requests should be routed to your served models behind an endpoint. \nAdditionally, you can configure the scale of resources that should be applied to each served model.\n",
    "x-databricks-package" : "endpoints",
    "x-databricks-service" : "ServingEndpoints"
  }, {
    "name" : "Delta Sharing: Shares",
    "description" : "Databricks Delta Sharing: Shares REST API",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "Shares"
  }, {
    "name" : "Statement Execution",
    "description" : "The SQL Statement Execution API manages the execution of arbitrary SQL statements and the fetching of result data.\n\n**Release Status**\n\nThis feature is in [Private Preview](https://docs.databricks.com/release-notes/release-types.html). To try it, reach out to your Databricks\ncontact.\n\n**Getting started**\n\nWe suggest beginning with the [SQL Statement Execution API tutorial](https://docs.databricks.com/sql/api/sql-execution-tutorial.html).\n\n**Overview of statement execution and result fetching**\n\nStatement execution begins by calling :method:statementexecution/executeStatement with a valid\nSQL statement and warehouse ID, along with optional parameters such as the data catalog and output format.\n\nWhen submitting the statement, the call can behave synchronously or asynchronously, based on the `wait_timeout`\nsetting. When set between 5-50 seconds (default:\n10) the call behaves synchronously; when set to `0s`, the call is asynchronous and responds\nimmediately if accepted.\n\n**Call mode: synchronous**\n\nIn synchronous mode, when statement execution completes, making the result available within the wait timeout,\nresult data is returned in the response. This response will contain `statement_id`, `status`, `manifest`, and\n`result` fields. `status` will confirm success, and `manifest` contains both the result data column schema, and\nmetadata about the result set. `result` will contain the first chunk of result data according to the specified\ndisposition, and links to fetch any remaining chunks.\n\nIf execution does not complete before `wait_timeout`, a response will be returned immediately. The setting\n`on_wait_timeout` determines how the system responds.\n\nBy default, `on_wait_timeout=CONTINUE`, and after reaching timeout, a response is sent and statement execution\ncontinues asynchronously. The response will contain only `statement_id` and `status` fields, and caller must now\nfollow the flow described for asynchronous call mode to poll and fetch result.\n\nAlternatively, `on_wait_timeout` can also be set to `CANCEL`; in this case if the timeout is reached before\nexecution completes, the underlying statement execution is canceled, and a `CANCELED` status is returned in the\nresponse.\n\n**Call mode: asynchronous**\n\nIn asynchronous mode, or after a timed-out synchronous request continues, a `statement_id` and `status` will be\nreturned. In this case polling :method:statementexecution/getStatement calls are required to fetch result and metadata.\n\nNext a caller must poll until execution completes (SUCCEEDED, FAILED, etc.). Given a `statement_id`, poll by\ncalling :method:statementexecution/getStatement.\n\nWhen execution has succeeded, the response will contain `status`, `manifest`, and `result` fields. These fields\nand structure are identical to those in the response to a successful synchronous submission. `result` will\ncontain the first chunk of result data, either inline or as external links depending on disposition. Additional\nchunks of result data can be fetched by checking for the presence of the `next_chunk_internal_link` field, and\niteratively `GET` those paths until that field is unset: `GET https://$DATABRICKS_HOST/{next_chunk_internal_link}`.\n\n**Fetching result data: format and disposition**\n\nResult data from statement execution is available in two formats: JSON, and\n[Apache Arrow Columnar](https://arrow.apache.org/overview/). Statements producing a result set smaller than\n16 MiB can be fetched as `format=JSON_ARRAY`, using the `disposition=INLINE`. When a\nstatement executed in INLINE disposition exceeds this limit, execution is aborted, and no result can be fetched.\nUsing `format=ARROW_STREAM` and `disposition=EXTERNAL_LINKS` allows large result sets to be fetched, and with\nhigher throughput.\n\nThe API uses defaults of `format=JSON_ARRAY` and `disposition=INLINE`. We advise explicitly setting format and\ndisposition in all production use cases.\n\n**Statement response: statement_id, status, manifest, and result**\n\nThe base call :method:statementexecution/getStatement returns a single response combining statement_id, status, a\nresult manifest, and a result data chunk or link. The manifest contains the result schema definition, and result\nsummary metadata. When using EXTERNAL_LINKS disposition, it also contains a full listing of all chunks and\ntheir summary metadata.\n\n**Use case: small result sets with INLINE + JSON_ARRAY**\n\nFor flows which will generate small and predictable result sets (<= 16 MiB), INLINE\ndownloads of JSON_ARRAY result data is typically the simplest way to execute and fetch result data. In this\ncase, :method:statementexecution/executeStatement, along with a `warehouse_id` (required) and any other desired options. With default\nparameters, (noteably `wait_timeout=10s`), execution and result fetch are synchronous: a small result will be\nreturned in the response, if completed within 10 seconds. `wait_timeout` can be extended up to\n50 seconds.\n\nWhen result set in INLINE mode becomes larger, it will transfer results in chunks, each up to\n4 MiB. After receiving the initial chunk with :method:statementexecution/executeStatement or :method:statementexecution/getStatement, subsequent calls are\nrequired to iteratively fetch each chunk. Each result response contains link to the next chunk, when there are\nadditional chunks remaining; it can be found in the field `.next_chunk_internal_link`. This link is an absolute\n`path` to be joined with your `$DATABRICKS_HOST`, and of the form\n`/api/2.0/sql/statements/{statement_id}/result/chunks/...`. The next chunk can be fetched like this\n`GET https://$DATABRICKS_HOST/{next_chunk_internal_link}`.\n\nWhen using this mode, each chunk may be fetched once, and in order. If a chunk has no field\n`.next_chunk_internal_link`, that indicates it to be the last chunk, and all chunks have been fetched from the\nresult set.\n\n**Use case: large result sets with EXTERNAL_LINKS + ARROW_STREAM**\n\nUsing EXTERNAL_LINKS to fetch result data in Arrow format allows you to fetch large result sets efficiently.\nThe primary difference from using INLINE disposition is that fetched result chunks contain resolved\n`external_links` URLs, which can be fetched with standard HTTP.\n\n**Presigned URLs**\n\nExternal links point to data stored within your workspace's internal DBFS, in the form of a presigned URL. The\nURLs are valid for only a short period, <= 15 minutes. Alongside each external_link is an expiration field\nindicating the time at which the URL is no longer valid. In EXTERNAL_LINKS mode, chunks be resolved and\nfetched multiple time, and in parallel.\n\n----\n\n### **Warning: drop authorization header when fetching data through external links**\n\nExternal link URLs do not require an Authorization header or token, and thus all calls to fetch external links\nmust remove the Authorization header.\n\n----\n\nSimilar to INLINE mode, callers can iterate through the result set, by using the field\n`next_chunk_internal_link`. Each internal link response will contain an external link to the raw chunk data,\nand additionally contain the next_chunk_internal_link if there are more chunks.\n\nUnlike INLINE mode, when using EXTERNAL_LINKS, chunks may be fetched out of order, and in parallel to\nachieve higher throughput.\n\n**Limits and limitations**\n\n- All byte limits are calculated based on internal storage metrics, and will not match byte counts of actual\n  payloads.\n- INLINE mode statements limited to 16 MiB, and will abort when this limit is\n  exceeded.\n- Cancelation may silently fail: A successful response from a cancel request indicates that the cancel request\n  was successfully received and sent to the processing engine. However, for example, an outstanding statement\n  may complete execution during signal delivery, with cancel signal arriving too late to be meaningful. Polling\n  for status until a terminal state is reached a reliable way to see final state.\n- Wait timeouts are approximate, occur server-side, and cannot account for caller delays, network latency from\n  caller to service, and similarly.\n- After a statement has been submitted and a statement_id produced, that statement's status and result will\n  automatically close after either of 2 conditions:\n  - The last result chunk is fetched (or resolved to an external link).\n  - Ten (10) minutes pass with no calls to get status or fetch result data.\n    Best practice: in asynchronous clients, poll for status regularly (and with backoff) to keep the statement\n    open and alive.\n\n**Private Preview limitations**\n\n- `EXTERNAL_LINKS` mode will fail for result sets < 5MB.\n- After any cancel or close operation, the statement will no longer be visible from the API, specifically\n  - After fetching last result chunk (including `chunk_index=0`), the statement is closed; a short time after\n    closure, the statement will no longer be visible to the API, and further calls may return 404.\n    Thus calling :method:statementexecution/getStatement will return a 404 NOT FOUND error.\n  - In practice, this means that a CANCEL and subsequent poll will often return a NOT FOUND.\n\n",
    "x-databricks-package" : "sql",
    "x-databricks-service" : "StatementExecution",
    "x-databricks-path-style" : "rest",
    "x-databricks-group" : "sql",
    "x-databricks-preview" : "PUBLIC",
    "x-databricks-not-cloud" : "gcp"
  }, {
    "name" : "Storage configurations",
    "description" : "These APIs manage storage configurations for this workspace. A root storage S3 bucket in\nyour account is required to store objects like cluster logs, notebook revisions, and job\nresults. You can also use the root storage S3 bucket for storage of non-production DBFS\ndata. A storage configuration encapsulates this bucket information, and its ID is used when\ncreating a new workspace.\n",
    "x-databricks-terraform-resource" : "databricks_mws_storage_configurations",
    "x-databricks-package" : "deployment",
    "x-databricks-service" : "Storage",
    "x-databricks-is-accounts" : true,
    "x-databricks-cloud" : "aws"
  }, {
    "name" : "Storage Credentials",
    "description" : "A storage credential represents an authentication and authorization mechanism for accessing\ndata stored on your cloud tenant. Each storage credential is subject to\nUnity Catalog access-control policies that control which users and groups can access\nthe credential. If a user does not have access to a storage credential in Unity Catalog,\nthe request fails and Unity Catalog does not attempt to authenticate to your cloud tenant\non the users behalf.\n\nDatabricks recommends using external locations rather than using storage credentials\ndirectly.\n\nTo create storage credentials, you must be a Databricks account admin. The account admin\nwho creates the storage credential can delegate ownership to another user or group to\nmanage permissions on it.\n",
    "x-databricks-terraform-resource" : "databricks_storage_credential",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "StorageCredentials"
  }, {
    "name" : "Table Constraints",
    "description" : "Primary key and foreign key constraints encode relationships between fields in tables.\n\nPrimary and foreign keys are informational only and are not enforced. Foreign keys must reference a primary key in another table.\nThis primary key is the parent constraint of the foreign key and the table this primary key is on is the parent table of the foreign key.\nSimilarly, the foreign key is the child constraint of its referenced primary key; the table of the foreign key is the child table of the primary key.\n\nYou can declare primary keys and foreign keys as part of the table specification during table creation.\nYou can also add or drop constraints on existing tables.\n",
    "x-databricks-terraform-resource" : "databricks_table_constraints",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "TableConstraints"
  }, {
    "name" : "Tables",
    "description" : "A table resides in the third layer of Unity Catalogs three-level namespace. It contains\nrows of data. To create a table, users must have CREATE_TABLE and USE_SCHEMA permissions on the schema,\nand they must have the USE_CATALOG permission on its parent catalog. To query a table, users must\nhave the SELECT permission on the table, and they must have the USE_CATALOG permission on its\nparent catalog and the USE_SCHEMA permission on its parent schema.\n\nA table can be managed or external.\nFrom an API perspective, a __VIEW__ is a particular kind of table (rather than a managed or external table).\n",
    "x-databricks-terraform-resource" : "databricks_table",
    "x-databricks-package" : "unitycatalog",
    "x-databricks-service" : "Tables"
  }, {
    "name" : "Token management",
    "description" : "Enables administrators to get all tokens and delete tokens for other users. Admins can\neither get every token, get a specific token by ID, or get all tokens for a particular user.\n",
    "x-databricks-terraform-resource" : "databricks_obo_token",
    "x-databricks-package" : "tokenmanagement",
    "x-databricks-service" : "TokenManagement",
    "x-databricks-group" : "admin"
  }, {
    "name" : "Token",
    "description" : "The Token API allows you to create, list, and revoke tokens that can be used to authenticate and access Databricks REST APIs.\n",
    "x-databricks-terraform-resource" : "databricks_token",
    "x-databricks-package" : "tokens",
    "x-databricks-service" : "Tokens",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "MLflow Transition Requests",
    "x-databricks-package" : "mlflow",
    "x-databricks-service" : "TransitionRequests",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "Users",
    "description" : "User identities recognized by Databricks and represented by email addresses.\n\nDatabricks recommends using SCIM provisioning to sync users and groups automatically from\nyour identity provider to your Databricks Workspace. SCIM streamlines onboarding a new\nemployee or team by using your identity provider to create users and groups in Databricks Workspace\nand give them the proper level of access. When a user leaves your organization or no longer\nneeds access to Databricks Workspace, admins can terminate the user in your identity provider and that\nusers account will also be removed from Databricks Workspace. This ensures a consistent offboarding\nprocess and prevents unauthorized users from accessing sensitive data.\n",
    "x-databricks-terraform-resource" : "databricks_user",
    "x-databricks-package" : "scim",
    "x-databricks-service" : "Users",
    "x-databricks-preview" : "PUBLIC"
  }, {
    "name" : "VPC Endpoint Configurations",
    "description" : "These APIs manage VPC endpoint configurations for this account. This object registers an\nAWS VPC endpoint in your Databricks account so your workspace can use it with\nAWS PrivateLink. Your VPC endpoint connects to one of two VPC endpoint services -- one for\nworkspace (both for front-end connection and for back-end connection to REST APIs) and one\nfor the back-end secure cluster connectivity relay from the data plane. Your account must be\n enabled for PrivateLink to use these APIs. Before configuring PrivateLink, it is important\n to read the [Databricks article about PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n",
    "x-databricks-terraform-resource" : "databricks_mws_vpc_endpoint",
    "x-databricks-package" : "deployment",
    "x-databricks-service" : "VpcEndpoints",
    "x-databricks-is-accounts" : true,
    "x-databricks-preview" : "PUBLIC",
    "x-databricks-cloud" : "aws"
  }, {
    "name" : "SQL Warehouses",
    "description" : "A SQL warehouse is a compute resource that lets you run SQL commands on data objects within\nDatabricks SQL. Compute resources are infrastructure resources that provide processing\ncapabilities in the cloud.\n",
    "x-databricks-terraform-resource" : "databricks_sql_endpoint",
    "x-databricks-package" : "sql",
    "x-databricks-service" : "Warehouses"
  }, {
    "name" : "Workspace",
    "description" : "The Workspace API allows you to list, import, export, and delete notebooks and folders.\n\nA notebook is a web-based interface to a document that contains runnable code,\nvisualizations, and explanatory text.\n",
    "x-databricks-terraform-resource" : "databricks_notebook",
    "x-databricks-package" : "workspace",
    "x-databricks-service" : "Workspace",
    "x-databricks-path-style" : "rpc"
  }, {
    "name" : "Workspace Assignment",
    "description" : "Databricks Workspace Assignment REST API",
    "x-databricks-terraform-resource" : "databricks_mws_permission_assignment",
    "x-databricks-package" : "permissions",
    "x-databricks-service" : "WorkspaceAssignment",
    "x-databricks-is-accounts" : true,
    "x-databricks-not-cloud" : "gcp"
  }, {
    "name" : "Workspace Conf",
    "description" : "This API allows updating known workspace settings for advanced users.\n",
    "x-databricks-terraform-resource" : "databricks_workspace_conf",
    "x-databricks-package" : "workspaceconf",
    "x-databricks-service" : "WorkspaceConf",
    "x-databricks-group" : "admin"
  }, {
    "name" : "Workspaces",
    "description" : "These APIs manage workspaces for this account. A Databricks workspace is an environment for\naccessing all of your Databricks assets. The workspace organizes objects (notebooks,\nlibraries, and experiments) into folders, and provides access to data and computational\nresources such as clusters and jobs.\n\nThese endpoints are available if your account is on the E2 version of the platform or on\na select custom plan that allows multiple workspaces per account.\n",
    "x-databricks-terraform-resource" : "databricks_mws_workspaces",
    "x-databricks-package" : "deployment",
    "x-databricks-service" : "Workspaces",
    "x-databricks-is-accounts" : true,
    "x-databricks-cloud" : "aws"
  } ],
  "x-databricks-groups" : [ {
    "name" : "Databricks Workspace",
    "tags" : [ "Clusters", "Command Execution", "Dbfs", "Delta Live Tables", "Git Credentials", "Instance Profiles", "Jobs", "ManagedLibraries", "Repos", "Secret", "Serving endpoints", "Token", "Workspace" ]
  }, {
    "name" : "Databricks Account",
    "tags" : [ "Billable usage download", "Budgets", "Credential configurations", "Key configurations", "Log delivery configurations", "Network configurations", "Private Access Settings", "Storage configurations", "VPC Endpoint Configurations", "Workspace Assignment", "Workspaces" ]
  }, {
    "priority" : 10,
    "name" : "Databricks SQL",
    "tags" : [ "ACL / Permissions", "Alerts", "Dashboards", "Data Sources", "Queries / Results", "Query History", "SQL Warehouses", "Statement Execution" ]
  }, {
    "priority" : 20,
    "name" : "Unity Catalog",
    "tags" : [ "Account Metastore Assignments", "Account Metastores", "Account Storage Credentials", "Catalogs", "Delta Sharing: Providers", "Delta Sharing: Recipient Activation", "Delta Sharing: Recipients", "Delta Sharing: Shares", "External Locations", "Functions", "Grants", "Metastores", "Schemas", "Storage Credentials", "Table Constraints", "Tables" ]
  }, {
    "priority" : 30,
    "name" : "SCIM",
    "tags" : [ "Account Groups", "Account Service Principals", "Account Users", "CurrentUser", "Groups", "Service Principals", "Users" ]
  }, {
    "priority" : 35,
    "name" : "Administration",
    "tags" : [ "Cluster Policies", "Global Init Scripts", "IP Access Lists", "Instance Pools", "Permissions", "Policy Families", "Token management", "Workspace Conf" ]
  }, {
    "priority" : 40,
    "name" : "MLflow",
    "tags" : [ "Databricks versions of MLflow API endpoints", "MLflow Artifacts", "MLflow Experiments", "MLflow Metrics", "MLflow Model Version Comments", "MLflow Model Versions", "MLflow Registered Models", "MLflow Registry Webhooks", "MLflow Runs", "MLflow Transition Requests" ]
  } ]
}