{{$include_dlt := "no" -}}
from databricks.bundles.jobs import Job

"""
The main job for {{.project_name}}.

{{- /* Clarify what this job is for for DLT-only users. */}}
{{if and (eq $include_dlt "yes") (and (eq .include_notebook "no") (eq .include_python "no")) -}}
This job runs {{.project_name}}_pipeline on a schedule.
{{end -}}
"""


{{.project_name}}_job = Job.from_dict(
    {
        "name": "{{.project_name}}_job",
        "trigger": {
            # Run this job every day, exactly one day from the last run; see https://docs.databricks.com/api/workspace/jobs/create#trigger
            "periodic": {
                "interval": 1,
                "unit": "DAYS",
            },
        },
        # "email_notifications": {
        #     "on_failure": [
        #         "{{user_name}}",
        #     ],
        # },
        "tasks": [
            {{- if eq .include_notebook "yes" -}}
            {{- "\n            " -}}
            {
                "task_key": "notebook_task",
                "job_cluster_key": "job_cluster",
                "notebook_task": {
                    "notebook_path": "src/notebook.ipynb",
                },
            },
            {{- end -}}
            {{- if (eq $include_dlt "yes") -}}
            {{- "\n            " -}}
            {
                "task_key": "refresh_pipeline",
                {{- if (eq .include_notebook "yes" )}}
                "depends_on": [
                    {
                        "task_key": "notebook_task",
                    },
                ],
                {{- end}}
                "pipeline_task": {
                    {{- /* TODO: we should find a way that doesn't use magics for the below, like ./{{project_name}}.pipeline.yml */}}
                    "pipeline_id": "${resources.pipelines.{{.project_name}}_pipeline.id}",
                },
            },
            {{- end -}}
            {{- if (eq .include_python "yes") -}}
            {{- "\n            " -}}
            {
                "task_key": "main_task",
                {{- if (eq $include_dlt "yes") }}
                "depends_on": [
                    {
                        "task_key": "refresh_pipeline",
                    },
                ],
                {{- else if (eq .include_notebook "yes" )}}
                "depends_on": [
                    {
                        "task_key": "notebook_task",
                    },
                ],
                {{- end}}
                "job_cluster_key": "job_cluster",
                "python_wheel_task": {
                    "package_name": "{{.project_name}}",
                    "entry_point": "main",
                },
                "libraries": [
                    # By default we just include the .whl file generated for the {{.project_name}} package.
                    # See https://docs.databricks.com/dev-tools/bundles/library-dependencies.html
                    # for more information on how to add other libraries.
                    {
                        "whl": "dist/*.whl",
                    },
                ],
            },
            {{- end -}}
            {{""}}
        ],
        "job_clusters": [
            {
                "job_cluster_key": "job_cluster",
                "new_cluster": {
                    "spark_version": "{{template "latest_lts_dbr_version"}}",
                    "node_type_id": "{{smallest_node_type}}",
                    "data_security_mode": "SINGLE_USER",
                    "autoscale": {
                        "min_workers": 1,
                        "max_workers": 4,
                    },
                },
            },
        ],
    }
)
