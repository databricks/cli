resources:
  jobs:
    {{.project_name}}_job:
      name: {{.project_name}}_job

      trigger:
        # Run this job every day, exactly one day from the last run; see https://docs.databricks.com/api/workspace/jobs/create#trigger
        periodic:
          interval: 1
          unit: DAYS

      #email_notifications:
      #  on_failure:
      #    - your_email@example.com

      tasks:
        - task_key: dbt
          dbt_task:
            project_directory: ../
            # The default schema, catalog, etc. are defined in ../dbt_profiles/profiles.yml
            profiles_directory: dbt_profiles/
            commands:
{{- if (regexp "^yes").MatchString .personal_schemas}}
              # The dbt commands to run (see also dbt_profiles/profiles.yml; dev_schema is used in the dev profile)
              - 'dbt deps --target=${bundle.target}'
              - 'dbt seed --target=${bundle.target} --vars "{ dev_schema: ${workspace.current_user.short_name} }"'
              - 'dbt run --target=${bundle.target} --vars "{ dev_schema: ${workspace.current_user.short_name} }"'
{{- else}}
              # The dbt commands to run (see also the dev/prod profiles in dbt_profiles/profiles.yml)
              - 'dbt deps --target=${bundle.target}'
              - 'dbt seed --target=${bundle.target}'
              - 'dbt run --target=${bundle.target}'
{{- end}}

          libraries:
            - pypi:
                package: dbt-databricks>=1.8.0,<2.0.0

          new_cluster:
            spark_version: {{template "latest_lts_dbr_version"}}
            node_type_id: {{smallest_node_type}}
            data_security_mode: SINGLE_USER
            num_workers: 0
            spark_conf:
              spark.master: "local[*, 4]"
              spark.databricks.cluster.profile: singleNode
            custom_tags:
              ResourceClass: SingleNode
