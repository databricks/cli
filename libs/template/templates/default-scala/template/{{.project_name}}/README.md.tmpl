# {{.project_name}}

The '{{.project_name}}' project was generated by using the default-scala template.

## Getting started

1. Install the Databricks CLI from https://docs.databricks.com/dev-tools/cli/install.html. The version must be v{{template `databricks_cli_version` .}} or later.

2. Authenticate to your Databricks workspace (if you have not done so already):
    ```
    $ databricks configure
    ```

3. To deploy a development copy of this project, type:
    ```
    $ databricks bundle deploy --target dev
    ```
    (Note that "dev" is the default target, so the `--target` parameter
    is optional here.)

    This deploys everything that's defined for this project.
    For example, the default template would deploy a job called
    `[dev yourname] {{.project_name}}_job` to your workspace.
    You can find that job by opening your workspace and clicking on **Workflows**.

4. Similarly, to deploy a production copy, type:
   ```
   $ databricks bundle deploy --target prod
   ```

5. To run a job, use the "run" command:
   ```
   $ databricks bundle run
   ```

6. Optionally, install developer tools such as the Databricks extension for Visual Studio Code from
   https://docs.databricks.com/dev-tools/vscode-ext.html.

7. For documentation on the Databricks Asset Bundles format used
   for this project, and for CI/CD configuration, see
   https://docs.databricks.com/dev-tools/bundles/index.html.

## Local Devloop

### Prerequisites

Install the following tools:

- [sbt](https://www.scala-sbt.org/) v1.10.2 or later
- Java 17

### Running via sbt

1. On the terminal, navigate to the project's root directory. This is the directory where the `build.sbt` file is located.
2. Execute the project's default `Main` class by running `sbt run`.

### IntelliJ setup

Install the latest [IntelliJ IDEA](https://www.jetbrains.com/idea/) IDE, both Community and Professional Editions work.
Install the [Scala plugin](https://plugins.jetbrains.com/plugin/1347-scala) from the Jetbrains marketplace.

1. Import the current directory in your in IntelliJ where `build.sbt` is located.
2. Choose the correct Java version in IntelliJ, go to File -> Project Structure -> SDKs.

   Then Run -> Edit Configurations -> Set version to Java 17 from drop
3. You should now be able to run the code directly in the IDE via the ▶️ option.

#### JVM settings

If you see the following error message,

```
Failed to initialize MemoryUtil. You must start Java with --add-opens=java.base/java.nio=ALL-UNNAMED
```

add the following to your JVM settings: `--add-opens=java.base/java.nio=org.apache.arrow.memory.core,ALL-UNNAMED`.

See the IntelliJ instructions on how to [configure VM settings for a specific run
configuration](https://www.jetbrains.com/help/idea/run-debug-configuration-java-application.html#more_options)
or [configure it everywhere on your IDE](https://www.jetbrains.com/help/ide-services/configure-settings-via-profiles.html).

### Unit tests

The project comes with a sample set of unit tests: `NycTaxiSpec.scala` using the ScalaTest
framework.

Run the tests either directly in the IntelliJ IDE by clicking ▶️ on the tests, or via sbt
by running `sbt test`.

## Customizations

### Job configuration
{{- if eq .compute_type "serverless"}}
This project uses serverless compute. No cluster setup is required.

{{- else }}
This project uses standard compute for the job cluster. Standard compute requires Unity Catalog volume JAR paths to be allowlisted by your workspace admin.

If you want to use an existing cluster instead, replace `job_cluster_key` in the task configuration with `existing_cluster_id: <your_cluster_id>` in the job YAML file.

You can also customize the cluster configuration (node type, worker count, Spark version) in the `job_clusters` section of the job YAML file.
{{- end}}
