{{- /*
     * Guidance for exploring Databricks workspaces and resources.
     *
     * This guidance is offered by the explore tool to provide comprehensive
     * instructions for discovering and querying workspace resources like
     * jobs, clusters, catalogs, tables, and SQL warehouses.
     *
     */ -}}

Databricks Data Exploration Guide
=====================================

{{.WorkspaceInfo}}{{if .WarehouseName}}
Default SQL Warehouse: {{.WarehouseName}} ({{.WarehouseID}}){{else}}
Note: No SQL warehouse detected. SQL queries will require warehouse_id to be specified manually.{{end}}{{.ProfilesInfo}}

IMPORTANT: Use the invoke_databricks_cli tool to run all commands below!


1. EXECUTING SQL QUERIES
   Run queries with auto-wait (max 50s):
     invoke_databricks_cli 'api post /api/2.0/sql/statements --json '{"warehouse_id":"{{if .WarehouseID}}{{.WarehouseID}}{{else}}<warehouse_id>{{end}}","statement":"SELECT * FROM <catalog>.<schema>.<table> LIMIT 10","wait_timeout":"50s"}'

   Response has status.state:
   - "SUCCEEDED" â†’ Results in result.data_array (you're done!)
   - "PENDING" â†’ Warehouse starting or query slow. Poll with:
       invoke_databricks_cli 'api get /api/2.0/sql/statements/<statement_id>'
     Repeat every 5-10s until "SUCCEEDED"

   Note: First query on stopped warehouse takes 60-120s startup time


2. EXPLORING JOBS AND WORKFLOWS
   List all jobs:
     invoke_databricks_cli 'jobs list'

   Get job details:
     invoke_databricks_cli 'jobs get <job_id>'

   List job runs:
     invoke_databricks_cli 'jobs list-runs --job-id <job_id>'


3. EXPLORING CLUSTERS
   List all clusters:
     invoke_databricks_cli 'clusters list'

   Get cluster details:
     invoke_databricks_cli 'clusters get <cluster_id>'


4. EXPLORING UNITY CATALOG DATA
   Unity Catalog uses a three-level namespace: catalog.schema.table

   List all catalogs:
     invoke_databricks_cli 'catalogs list'

   List schemas in a catalog:
     invoke_databricks_cli 'schemas list <catalog_name>'

   List tables in a schema:
     invoke_databricks_cli 'tables list <catalog_name> <schema_name>'

   Get table details (schema, columns, properties):
     invoke_databricks_cli 'tables get <catalog>.<schema>.<table>'


5. EXPLORING WORKSPACE FILES
   List workspace files and notebooks:
     invoke_databricks_cli 'workspace list <path>'

   Export a notebook:
     invoke_databricks_cli 'workspace export <path>'


Getting Started:
- Use the commands above to explore what resources exist in the workspace
- All commands support --output json for programmatic access
- Remember to add --profile <name> when working with non-default workspaces


DATABRICKS ASSET BUNDLES (DABs) WORKFLOW
=========================================

Creating a New Bundle Project:
  When to use: Building a new project from scratch with deployment to multiple environments

  1. Initialize a new bundle (creates proper project structure):
     invoke_databricks_cli 'experimental apps-mcp tools init-template <app_name>'

  2. Validate the bundle configuration:
     invoke_databricks_cli 'bundle validate'

  3. Deploy to a target environment (dev/staging/prod):
     invoke_databricks_cli 'bundle deploy --target dev'

Working with Existing Bundle Project:
  When to use: databricks.yml file already exists in the directory

  1. Validate changes:
     invoke_databricks_cli 'bundle validate'

  2. Deploy to environment:
     invoke_databricks_cli 'bundle deploy --target <environment>'

  3. Run a resource (job/pipeline):
     invoke_databricks_cli 'bundle run <resource_name>'

  4. Destroy deployed resources:
     invoke_databricks_cli 'bundle destroy --target <environment>'

Bundle Commands Reference:
  - experimental apps-mcp tools init-template <name>  # Initialize new MCP app from template
  - bundle validate                # Validate bundle configuration
  - bundle deploy                  # Deploy bundle to workspace
  - bundle run <resource>          # Run a job or pipeline
  - bundle destroy                 # Remove deployed resources
  - bundle schema                  # Show bundle configuration schema

ðŸ’¡ Tip: Use 'invoke_databricks_cli experimental apps-mcp tools init-template <app_name>' to create a new MCP app


COMMON PATTERNS
===============

Multi-environment deployment:
  Deploy to different environments using targets in databricks.yml:
    invoke_databricks_cli 'bundle deploy --target dev'
    invoke_databricks_cli 'bundle deploy --target prod'

Working with pipelines/jobs in bundles:
  Add resources to databricks.yml, then:
    invoke_databricks_cli 'bundle validate'
    invoke_databricks_cli 'bundle deploy'
    invoke_databricks_cli 'bundle run <job_name>'


BEST PRACTICES
==============

âœ… DO use invoke_databricks_cli for all Databricks CLI commands
   (Better for user allowlisting and tracking)

âœ… DO validate bundles before deploying:
   invoke_databricks_cli 'bundle validate'

âœ… DO use bundle templates for new MCP app projects:
   invoke_databricks_cli 'experimental apps-mcp tools init-template <app_name>'

âœ… DO call explore during planning to get workspace context
