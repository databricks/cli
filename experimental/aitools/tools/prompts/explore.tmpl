{{- /*
     * Guidance for exploring Databricks workspaces and resources.
     *
     * This guidance is offered by the explore tool to provide comprehensive
     * instructions for discovering and querying workspace resources like
     * jobs, clusters, catalogs, tables, and SQL warehouses.
     *
     */ -}}

Databricks Data Exploration Guide
=====================================

{{.WorkspaceContext}}

IMPORTANT: Use the invoke_databricks_cli tool to run all commands below!
Use workspace_info(profile='<name>') to get details about other workspaces.


1. EXECUTING SQL QUERIES
   Run queries with auto-wait (max 50s):
     invoke_databricks_cli 'api post /api/2.0/sql/statements --json {"warehouse_id":"{{if .WarehouseID}}{{.WarehouseID}}{{else}}<warehouse_id>{{end}}","statement":"SELECT * FROM <catalog>.<schema>.<table> LIMIT 10","wait_timeout":"50s"}'

   Response has status.state:
   - "SUCCEEDED" ‚Üí Results in result.data_array (you're done!)
   - "PENDING" ‚Üí Warehouse starting or query slow. Poll with:
       invoke_databricks_cli 'api get /api/2.0/sql/statements/<statement_id>'
     Repeat every 5-10s until "SUCCEEDED"

   Note: First query on stopped warehouse takes 60-120s startup time


2. EXPLORING JOBS AND WORKFLOWS
   List all jobs:
     invoke_databricks_cli 'jobs list'

   Get job details:
     invoke_databricks_cli 'jobs get <job_id>'

   List job runs:
     invoke_databricks_cli 'jobs list-runs --job-id <job_id>'


3. EXPLORING CLUSTERS
   List all clusters:
     invoke_databricks_cli 'clusters list'

   Get cluster details:
     invoke_databricks_cli 'clusters get <cluster_id>'


4. EXPLORING UNITY CATALOG DATA
   Unity Catalog uses a three-level namespace: catalog.schema.table

   List all catalogs:
     invoke_databricks_cli 'catalogs list'

   List schemas in a catalog:
     invoke_databricks_cli 'schemas list <catalog_name>'

   List tables in a schema:
     invoke_databricks_cli 'tables list <catalog_name> <schema_name>'

   Get table details (schema, columns, properties):
     invoke_databricks_cli 'tables get <catalog>.<schema>.<table>'


5. EXPLORING WORKSPACE FILES
   List workspace files and notebooks:
     invoke_databricks_cli 'workspace list <path>'

   Export a notebook:
     invoke_databricks_cli 'workspace export <path>'


Getting Started:
- Use the commands above to explore what resources exist in the workspace
- All commands support --output json for programmatic access
- To use a different workspace: workspace_info(profile='<name>') then invoke_databricks_cli('--profile <name> <command>')

WORKFLOW PATTERNS FOR NOTEBOOKS
===============================

Create notebooks locally (.ipynb), fetch data from Databricks, generate visualizations.

## Setup (one-time)

```bash
# Install uv
curl -LsSf https://astral.sh/uv/install.sh | sh
```

**pyproject.toml** dev dependencies:
```toml
[dependency-groups]
dev = [
    "databricks-connect>=15.4.0",
    "papermill",
    "nbformat",
    "matplotlib",
]
```

```bash
# Install deps
uv sync

# Auto-init spark (no boilerplate in notebooks)
mkdir -p ~/.ipython/profile_default/startup
cat > ~/.ipython/profile_default/startup/00-databricks-spark.py << 'EOF'
try:
    from databricks.connect import DatabricksSession
    spark = DatabricksSession.builder.getOrCreate()
except: pass
EOF
```

## Workflow

**Create or update notebook:**

Add exploratory cells to an .ipynb notebook like

```python
import pandas as pd, matplotlib.pyplot as plt

# Aggregate in Spark, limit for viz
df = spark.sql("""
    SELECT category, COUNT(*) as count, AVG(value) as avg_value
    FROM catalog.schema.table
    GROUP BY category
    ORDER BY count DESC
""").limit(10000).toPandas()

df.plot(x='category', y='avg_value', kind='bar', figsize=(10, 6))
plt.title('Average Value by Category')
plt.show()
```

**Execute:**

Execute the notebook and produce results inline in the ipynb:

```bash
DATABRICKS_CONFIG_PROFILE=<profile> DATABRICKS_SERVERLESS_COMPUTE_ID=auto \
  uv run papermill notebook.ipynb notebook_executed.ipynb -k python3
```

**Iterate:**

Actually iterating over a notebook is MANDATORY. You can't assume that it will just be successful.

Papermill embeds all outputs (stdout, stderr, plots as base64, errors) into the executed .ipynb.

To iterate:
1. Read `notebook_executed.ipynb` to see results (outputs are in cell JSON)
2. Check for errors, review data/plots
4. Optionally, do any quick exploratory queries directly via the CLI
5. Modify `notebook.ipynb` based on results
6. Re-execute and repeat

**View in IDE:**
- `cursor notebook_executed.ipynb` or `code notebook_executed.ipynb`
- Or deploy: `databricks bundle deploy` and open in workspace browser

## Key Pattern

**Aggregate ‚Üí Limit ‚Üí Pandas ‚Üí Visualize ‚Üí Read outputs ‚Üí Iterate**

Always aggregate in Spark (GROUP BY, AVG, COUNT), then `.limit(10000)` before `.toPandas()`. Execute with papermill, read the executed .ipynb to see results, iterate.



WORKFLOW PATTERNS FOR DATABRICKS PROJECTS
==========================================

Creating a New Databricks Project:
  When to use: Building a new project from scratch, setting up deployment to multiple environments
  Tools sequence:
    1. init_project (creates proper project structure with templates)
    2. add_project_resource (for each resource you need: pipeline/job/app/dashboard)
    3. analyze_project (provides deployment commands)
    4. invoke_databricks_cli 'bundle validate'
  üí° Tip: Use init_project even if you know YAML syntax - it uses templates and best practices

Working with Existing Databricks Project:
  When to use: databricks.yml file already exists in the directory
  Tools sequence:
    1. analyze_project (MANDATORY FIRST STEP - provides specialized commands)
    2. [Make your changes to project files]
    3. invoke_databricks_cli 'bundle validate'
  üí° Tip: ALWAYS call analyze_project before making changes - Databricks projects
          require specialized commands that differ from standard Python/Node.js workflows

Adding Resources to Existing Project:
  When to use: Adding pipelines, jobs, apps, or dashboards to an existing project
  Tools sequence:
    1. add_project_resource (with type: 'pipeline', 'job', 'app', or 'dashboard')
    2. analyze_project (to get updated deployment commands)
    3. invoke_databricks_cli 'bundle validate'


PATTERN MATCHING: If Your Task Mentions...
===========================================

"new project" / "create a project" / "Databricks project" / "project structure"
  ‚Üí Use init_project first (don't create files manually!)
  ‚Üí Then add_project_resource for each resource (pipeline/job/app/dashboard)

"SQL pipeline" / "data pipeline" / "materialized views" / "ETL" / "DLT"
  ‚Üí Use add_project_resource with type='pipeline' or type='job'

"Databricks app" / "application" / "build an app"
  ‚Üí Use add_project_resource with type='app'

"dashboard" / "Lakeview dashboard" / "visualization"
  ‚Üí Use add_project_resource with type='dashboard'

"Databricks job" / "scheduled job" / "workflow"
  ‚Üí Use add_project_resource with type='job'

"deploy to dev and prod" / "multiple environments" / "dev/staging/prod"
  ‚Üí Use init_project (sets up multi-environment structure automatically)

"databricks.yml" / "bundle configuration" / "Asset Bundle"
  ‚Üí If creating new: use init_project (don't create manually!)
  ‚Üí If exists already: use analyze_project FIRST before making changes


ANTI-PATTERNS TO AVOID
=======================

‚ùå DON'T manually create databricks.yml files
   ‚úÖ DO use init_project instead

‚ùå DON'T run bundle commands without calling analyze_project first
   ‚úÖ DO call analyze_project to get the correct specialized commands

‚ùå DON'T use regular Bash to run databricks CLI commands
   ‚úÖ DO use invoke_databricks_cli (better for user allowlisting)

‚ùå DON'T skip explore when planning Databricks work
   ‚úÖ DO call explore during planning to get workflow recommendations
