{{- /*
     * Guidance for exploring Databricks workspaces and resources.
     *
     * This guidance is offered by the explore tool to provide comprehensive
     * instructions for discovering and querying workspace resources like
     * jobs, clusters, catalogs, tables, and SQL warehouses.
     *
     */ -}}

Databricks Data Exploration Guide
=====================================

{{.WorkspaceInfo}}{{if .WarehouseName}}
Default SQL Warehouse: {{.WarehouseName}} ({{.WarehouseID}}){{else}}
Note: No SQL warehouse detected. SQL queries will require warehouse_id to be specified manually.{{end}}{{.ProfilesInfo}}

IMPORTANT: Use the invoke_databricks_cli tool to run all commands below!


1. EXECUTING SQL QUERIES
   Run queries with auto-wait (max 50s):
     invoke_databricks_cli 'api post /api/2.0/sql/statements --json {"warehouse_id":"{{if .WarehouseID}}{{.WarehouseID}}{{else}}<warehouse_id>{{end}}","statement":"SELECT * FROM <catalog>.<schema>.<table> LIMIT 10","wait_timeout":"50s"}'

   Response has status.state:
   - "SUCCEEDED" ‚Üí Results in result.data_array (you're done!)
   - "PENDING" ‚Üí Warehouse starting or query slow. Poll with:
       invoke_databricks_cli 'api get /api/2.0/sql/statements/<statement_id>'
     Repeat every 5-10s until "SUCCEEDED"

   Note: First query on stopped warehouse takes 60-120s startup time


2. EXPLORING JOBS AND WORKFLOWS
   List all jobs:
     invoke_databricks_cli 'jobs list'

   Get job details:
     invoke_databricks_cli 'jobs get <job_id>'

   List job runs:
     invoke_databricks_cli 'jobs list-runs --job-id <job_id>'


3. EXPLORING CLUSTERS
   List all clusters:
     invoke_databricks_cli 'clusters list'

   Get cluster details:
     invoke_databricks_cli 'clusters get <cluster_id>'


4. EXPLORING UNITY CATALOG DATA
   Unity Catalog uses a three-level namespace: catalog.schema.table

   List all catalogs:
     invoke_databricks_cli 'catalogs list'

   List schemas in a catalog:
     invoke_databricks_cli 'schemas list <catalog_name>'

   List tables in a schema:
     invoke_databricks_cli 'tables list <catalog_name> <schema_name>'

   Get table details (schema, columns, properties):
     invoke_databricks_cli 'tables get <catalog>.<schema>.<table>'


5. EXPLORING WORKSPACE FILES
   List workspace files and notebooks:
     invoke_databricks_cli 'workspace list <path>'

   Export a notebook:
     invoke_databricks_cli 'workspace export <path>'


Getting Started:
- Use the commands above to explore what resources exist in the workspace
- All commands support --output json for programmatic access
- Remember to add --profile <name> when working with non-default workspaces


WORKFLOW PATTERNS FOR DATABRICKS PROJECTS
==========================================

Creating a New Databricks Project:
  When to use: Building a new project from scratch, setting up deployment to multiple environments
  Tools sequence:
    1. init_project (creates proper project structure with templates)
    2. add_project_resource (for each resource you need: pipeline/job/app/dashboard)
    3. analyze_project (provides deployment commands)
    4. invoke_databricks_cli 'bundle validate'
  üí° Tip: Use init_project even if you know YAML syntax - it uses templates and best practices

Working with Existing Databricks Project:
  When to use: databricks.yml file already exists in the directory
  Tools sequence:
    1. analyze_project (MANDATORY FIRST STEP - provides specialized commands)
    2. [Make your changes to project files]
    3. invoke_databricks_cli 'bundle validate'
  üí° Tip: ALWAYS call analyze_project before making changes - Databricks projects
          require specialized commands that differ from standard Python/Node.js workflows

Adding Resources to Existing Project:
  When to use: Adding pipelines, jobs, apps, or dashboards to an existing project
  Tools sequence:
    1. add_project_resource (with type: 'pipeline', 'job', 'app', or 'dashboard')
    2. analyze_project (to get updated deployment commands)
    3. invoke_databricks_cli 'bundle validate'


PATTERN MATCHING: If Your Task Mentions...
===========================================

"new project" / "create a project" / "Databricks project" / "project structure"
  ‚Üí Use init_project first (don't create files manually!)
  ‚Üí Then add_project_resource for each resource (pipeline/job/app/dashboard)

"SQL pipeline" / "data pipeline" / "materialized views" / "ETL" / "DLT"
  ‚Üí Use add_project_resource with type='pipeline' or type='job'

"Databricks app" / "application" / "build an app"
  ‚Üí Use add_project_resource with type='app'

"dashboard" / "Lakeview dashboard" / "visualization"
  ‚Üí Use add_project_resource with type='dashboard'

"Databricks job" / "scheduled job" / "workflow"
  ‚Üí Use add_project_resource with type='job'

"deploy to dev and prod" / "multiple environments" / "dev/staging/prod"
  ‚Üí Use init_project (sets up multi-environment structure automatically)

"databricks.yml" / "bundle configuration" / "Asset Bundle"
  ‚Üí If creating new: use init_project (don't create manually!)
  ‚Üí If exists already: use analyze_project FIRST before making changes


ANTI-PATTERNS TO AVOID
=======================

‚ùå DON'T manually create databricks.yml files
   ‚úÖ DO use init_project instead

‚ùå DON'T run bundle commands without calling analyze_project first
   ‚úÖ DO call analyze_project to get the correct specialized commands

‚ùå DON'T use regular Bash to run databricks CLI commands
   ‚úÖ DO use invoke_databricks_cli (better for user allowlisting)

‚ùå DON'T skip explore when planning Databricks work
   ‚úÖ DO call explore during planning to get workflow recommendations
