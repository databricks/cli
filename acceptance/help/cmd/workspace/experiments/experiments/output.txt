
>>> $CLI experiments create-experiment --help
Create experiment.
  
  Creates an experiment with a name. Returns the ID of the newly created
  experiment. Validates that another experiment with the same name does not
  already exist and fails if another experiment with the same name already
  exists.
  
  Throws RESOURCE_ALREADY_EXISTS if a experiment with the given name exists.

  Arguments:
    NAME: Experiment name.

Usage:
  databricks experiments create-experiment NAME [flags]

Flags:
      --artifact-location string   Location where all artifacts for the experiment are stored.
  -h, --help                       help for create-experiment
      --json JSON                  either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments create-run --help
Create a run.
  
  Creates a new run within an experiment. A run is usually a single execution of
  a machine learning or data ETL pipeline. MLflow uses runs to track the
  mlflowParam, mlflowMetric and mlflowRunTag associated with a single
  execution.

Usage:
  databricks experiments create-run [flags]

Flags:
      --experiment-id string   ID of the associated experiment.
  -h, --help                   help for create-run
      --json JSON              either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --start-time int         Unix timestamp in milliseconds of when the run started.
      --user-id string         ID of the user executing the run.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments delete-experiment --help
Delete an experiment.
  
  Marks an experiment and associated metadata, runs, metrics, params, and tags
  for deletion. If the experiment uses FileStore, artifacts associated with
  experiment are also deleted.

  Arguments:
    EXPERIMENT_ID: ID of the associated experiment.

Usage:
  databricks experiments delete-experiment EXPERIMENT_ID [flags]

Flags:
  -h, --help        help for delete-experiment
      --json JSON   either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments delete-run --help
Delete a run.
  
  Marks a run for deletion.

  Arguments:
    RUN_ID: ID of the run to delete.

Usage:
  databricks experiments delete-run RUN_ID [flags]

Flags:
  -h, --help        help for delete-run
      --json JSON   either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments delete-runs --help
Delete runs by creation time.
  
  Bulk delete runs in an experiment that were created prior to or at the
  specified timestamp. Deletes at most max_runs per request. To call this API
  from a Databricks Notebook in Python, you can use the client code snippet on
  https://learn.microsoft.com/en-us/azure/databricks/mlflow/runs#bulk-delete.

  Arguments:
    EXPERIMENT_ID: The ID of the experiment containing the runs to delete.
    MAX_TIMESTAMP_MILLIS: The maximum creation timestamp in milliseconds since the UNIX epoch for
      deleting runs. Only runs created prior to or at this timestamp are
      deleted.

Usage:
  databricks experiments delete-runs EXPERIMENT_ID MAX_TIMESTAMP_MILLIS [flags]

Flags:
  -h, --help           help for delete-runs
      --json JSON      either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --max-runs int   An optional positive integer indicating the maximum number of runs to delete.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments delete-tag --help
Delete a tag.
  
  Deletes a tag on a run. Tags are run metadata that can be updated during a run
  and after a run completes.

  Arguments:
    RUN_ID: ID of the run that the tag was logged under. Must be provided.
    KEY: Name of the tag. Maximum size is 255 bytes. Must be provided.

Usage:
  databricks experiments delete-tag RUN_ID KEY [flags]

Flags:
  -h, --help        help for delete-tag
      --json JSON   either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments get-by-name --help
Get metadata.
  
  Gets metadata for an experiment.
  
  This endpoint will return deleted experiments, but prefers the active
  experiment if an active and deleted experiment share the same name. If
  multiple deleted experiments share the same name, the API will return one of
  them.
  
  Throws RESOURCE_DOES_NOT_EXIST if no experiment with the specified name
  exists.

  Arguments:
    EXPERIMENT_NAME: Name of the associated experiment.

Usage:
  databricks experiments get-by-name EXPERIMENT_NAME [flags]

Flags:
  -h, --help   help for get-by-name

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments get-experiment --help
Get an experiment.
  
  Gets metadata for an experiment. This method works on deleted experiments.

  Arguments:
    EXPERIMENT_ID: ID of the associated experiment.

Usage:
  databricks experiments get-experiment EXPERIMENT_ID [flags]

Flags:
  -h, --help   help for get-experiment

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments get-history --help
Get history of a given metric within a run.
  
  Gets a list of all values for the specified metric for a given run.

  Arguments:
    METRIC_KEY: Name of the metric.

Usage:
  databricks experiments get-history METRIC_KEY [flags]

Flags:
  -h, --help                help for get-history
      --max-results int     Maximum number of Metric records to return per paginated request.
      --page-token string   Token indicating the page of metric histories to fetch.
      --run-id string       ID of the run from which to fetch metric values.
      --run-uuid string     [Deprecated, use run_id instead] ID of the run from which to fetch metric values.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments get-permission-levels --help
Get experiment permission levels.
  
  Gets the permission levels that a user can have on an object.

  Arguments:
    EXPERIMENT_ID: The experiment for which to get or manage permissions.

Usage:
  databricks experiments get-permission-levels EXPERIMENT_ID [flags]

Flags:
  -h, --help   help for get-permission-levels

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments get-permissions --help
Get experiment permissions.
  
  Gets the permissions of an experiment. Experiments can inherit permissions
  from their root object.

  Arguments:
    EXPERIMENT_ID: The experiment for which to get or manage permissions.

Usage:
  databricks experiments get-permissions EXPERIMENT_ID [flags]

Flags:
  -h, --help   help for get-permissions

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments get-run --help
Get a run.
  
  Gets the metadata, metrics, params, and tags for a run. In the case where
  multiple metrics with the same key are logged for a run, return only the value
  with the latest timestamp.
  
  If there are multiple values with the latest timestamp, return the maximum of
  these values.

  Arguments:
    RUN_ID: ID of the run to fetch. Must be provided.

Usage:
  databricks experiments get-run RUN_ID [flags]

Flags:
  -h, --help              help for get-run
      --run-uuid string   [Deprecated, use run_id instead] ID of the run to fetch.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments list-artifacts --help
Get all artifacts.
  
  List artifacts for a run. Takes an optional artifact_path prefix. If it is
  specified, the response contains only artifacts with the specified prefix.
  This API does not support pagination when listing artifacts in UC Volumes. A
  maximum of 1000 artifacts will be retrieved for UC Volumes. Please call
  /api/2.0/fs/directories{directory_path} for listing artifacts in UC Volumes,
  which supports pagination. See [List directory contents | Files
  API](/api/workspace/files/listdirectorycontents).

Usage:
  databricks experiments list-artifacts [flags]

Flags:
  -h, --help                help for list-artifacts
      --page-token string   Token indicating the page of artifact results to fetch.
      --path string         Filter artifacts matching this path (a relative path from the root artifact directory).
      --run-id string       ID of the run whose artifacts to list.
      --run-uuid string     [Deprecated, use run_id instead] ID of the run whose artifacts to list.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments list-experiments --help
List experiments.
  
  Gets a list of all experiments.

Usage:
  databricks experiments list-experiments [flags]

Flags:
  -h, --help                help for list-experiments
      --max-results int     Maximum number of experiments desired.
      --page-token string   Token indicating the page of experiments to fetch.
      --view-type string    Qualifier for type of experiments to be returned.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments log-batch --help
Log a batch.
  
  Logs a batch of metrics, params, and tags for a run. If any data failed to be
  persisted, the server will respond with an error (non-200 status code).
  
  In case of error (due to internal server error or an invalid request), partial
  data may be written.
  
  You can write metrics, params, and tags in interleaving fashion, but within a
  given entity type are guaranteed to follow the order specified in the request
  body.
  
  The overwrite behavior for metrics, params, and tags is as follows:
  
  * Metrics: metric values are never overwritten. Logging a metric (key, value,
  timestamp) appends to the set of values for the metric with the provided key.
  
  * Tags: tag values can be overwritten by successive writes to the same tag
  key. That is, if multiple tag values with the same key are provided in the
  same API request, the last-provided tag value is written. Logging the same tag
  (key, value) is permitted. Specifically, logging a tag is idempotent.
  
  * Parameters: once written, param values cannot be changed (attempting to
  overwrite a param value will result in an error). However, logging the same
  param (key, value) is permitted. Specifically, logging a param is idempotent.
  
  Request Limits ------------------------------- A single JSON-serialized API
  request may be up to 1 MB in size and contain:
  
  * No more than 1000 metrics, params, and tags in total * Up to 1000 metrics *
  Up to 100 params * Up to 100 tags
  
  For example, a valid request might contain 900 metrics, 50 params, and 50
  tags, but logging 900 metrics, 50 params, and 51 tags is invalid.
  
  The following limits also apply to metric, param, and tag keys and values:
  
  * Metric keys, param keys, and tag keys can be up to 250 characters in length
  * Parameter and tag values can be up to 250 characters in length

Usage:
  databricks experiments log-batch [flags]

Flags:
  -h, --help            help for log-batch
      --json JSON       either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --run-id string   ID of the run to log under.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments log-inputs --help
Log inputs to a run.
  
  **NOTE:** Experimental: This API may change or be removed in a future release
  without warning.

Usage:
  databricks experiments log-inputs [flags]

Flags:
  -h, --help            help for log-inputs
      --json JSON       either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --run-id string   ID of the run to log under.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments log-metric --help
Log a metric.
  
  Logs a metric for a run. A metric is a key-value pair (string key, float
  value) with an associated timestamp. Examples include the various metrics that
  represent ML model accuracy. A metric can be logged multiple times.

  Arguments:
    KEY: Name of the metric.
    VALUE: Double value of the metric being logged.
    TIMESTAMP: Unix timestamp in milliseconds at the time metric was logged.

Usage:
  databricks experiments log-metric KEY VALUE TIMESTAMP [flags]

Flags:
  -h, --help              help for log-metric
      --json JSON         either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --run-id string     ID of the run under which to log the metric.
      --run-uuid string   [Deprecated, use run_id instead] ID of the run under which to log the metric.
      --step int          Step at which to log the metric.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments log-model --help
Log a model.
  
  **NOTE:** Experimental: This API may change or be removed in a future release
  without warning.

Usage:
  databricks experiments log-model [flags]

Flags:
  -h, --help                help for log-model
      --json JSON           either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --model-json string   MLmodel file in json format.
      --run-id string       ID of the run to log under.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments log-param --help
Log a param.
  
  Logs a param used for a run. A param is a key-value pair (string key, string
  value). Examples include hyperparameters used for ML model training and
  constant dates and values used in an ETL pipeline. A param can be logged only
  once for a run.

  Arguments:
    KEY: Name of the param. Maximum size is 255 bytes.
    VALUE: String value of the param being logged. Maximum size is 500 bytes.

Usage:
  databricks experiments log-param KEY VALUE [flags]

Flags:
  -h, --help              help for log-param
      --json JSON         either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --run-id string     ID of the run under which to log the param.
      --run-uuid string   [Deprecated, use run_id instead] ID of the run under which to log the param.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments restore-experiment --help
Restores an experiment.
  
  Restore an experiment marked for deletion. This also restores associated
  metadata, runs, metrics, params, and tags. If experiment uses FileStore,
  underlying artifacts associated with experiment are also restored.
  
  Throws RESOURCE_DOES_NOT_EXIST if experiment was never created or was
  permanently deleted.

  Arguments:
    EXPERIMENT_ID: ID of the associated experiment.

Usage:
  databricks experiments restore-experiment EXPERIMENT_ID [flags]

Flags:
  -h, --help        help for restore-experiment
      --json JSON   either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments restore-run --help
Restore a run.
  
  Restores a deleted run.

  Arguments:
    RUN_ID: ID of the run to restore.

Usage:
  databricks experiments restore-run RUN_ID [flags]

Flags:
  -h, --help        help for restore-run
      --json JSON   either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments restore-runs --help
Restore runs by deletion time.
  
  Bulk restore runs in an experiment that were deleted no earlier than the
  specified timestamp. Restores at most max_runs per request. To call this API
  from a Databricks Notebook in Python, you can use the client code snippet on
  https://learn.microsoft.com/en-us/azure/databricks/mlflow/runs#bulk-restore.

  Arguments:
    EXPERIMENT_ID: The ID of the experiment containing the runs to restore.
    MIN_TIMESTAMP_MILLIS: The minimum deletion timestamp in milliseconds since the UNIX epoch for
      restoring runs. Only runs deleted no earlier than this timestamp are
      restored.

Usage:
  databricks experiments restore-runs EXPERIMENT_ID MIN_TIMESTAMP_MILLIS [flags]

Flags:
  -h, --help           help for restore-runs
      --json JSON      either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --max-runs int   An optional positive integer indicating the maximum number of runs to restore.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments search-experiments --help
Search experiments.
  
  Searches for experiments that satisfy specified search criteria.

Usage:
  databricks experiments search-experiments [flags]

Flags:
      --filter string                         String representing a SQL filter condition (e.g.
  -h, --help                                  help for search-experiments
      --json JSON                             either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --max-results int                       Maximum number of experiments desired.
      --page-token string                     Token indicating the page of experiments to fetch.
      --view-type SearchExperimentsViewType   Qualifier for type of experiments to be returned. Supported values: [ACTIVE_ONLY, ALL, DELETED_ONLY]

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments search-runs --help
Search for runs.
  
  Searches for runs that satisfy expressions.
  
  Search expressions can use mlflowMetric and mlflowParam keys.",

Usage:
  databricks experiments search-runs [flags]

Flags:
      --filter string                         A filter expression over params, metrics, and tags, that allows returning a subset of runs.
  -h, --help                                  help for search-runs
      --json JSON                             either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --max-results int                       Maximum number of runs desired.
      --page-token string                     Token for the current page of runs.
      --run-view-type SearchRunsRunViewType   Whether to display only active, only deleted, or all runs. Supported values: [ACTIVE_ONLY, ALL, DELETED_ONLY]

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments set-experiment-tag --help
Set a tag.
  
  Sets a tag on an experiment. Experiment tags are metadata that can be updated.

  Arguments:
    EXPERIMENT_ID: ID of the experiment under which to log the tag. Must be provided.
    KEY: Name of the tag. Maximum size depends on storage backend. All storage
      backends are guaranteed to support key values up to 250 bytes in size.
    VALUE: String value of the tag being logged. Maximum size depends on storage
      backend. All storage backends are guaranteed to support key values up to
      5000 bytes in size.

Usage:
  databricks experiments set-experiment-tag EXPERIMENT_ID KEY VALUE [flags]

Flags:
  -h, --help        help for set-experiment-tag
      --json JSON   either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments set-permissions --help
Set experiment permissions.
  
  Sets permissions on an object, replacing existing permissions if they exist.
  Deletes all direct permissions if none are specified. Objects can inherit
  permissions from their root object.

  Arguments:
    EXPERIMENT_ID: The experiment for which to get or manage permissions.

Usage:
  databricks experiments set-permissions EXPERIMENT_ID [flags]

Flags:
  -h, --help        help for set-permissions
      --json JSON   either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments set-tag --help
Set a tag.
  
  Sets a tag on a run. Tags are run metadata that can be updated during a run
  and after a run completes.

  Arguments:
    KEY: Name of the tag. Maximum size depends on storage backend. All storage
      backends are guaranteed to support key values up to 250 bytes in size.
    VALUE: String value of the tag being logged. Maximum size depends on storage
      backend. All storage backends are guaranteed to support key values up to
      5000 bytes in size.

Usage:
  databricks experiments set-tag KEY VALUE [flags]

Flags:
  -h, --help              help for set-tag
      --json JSON         either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --run-id string     ID of the run under which to log the tag.
      --run-uuid string   [Deprecated, use run_id instead] ID of the run under which to log the tag.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments update-experiment --help
Update an experiment.
  
  Updates experiment metadata.

  Arguments:
    EXPERIMENT_ID: ID of the associated experiment.

Usage:
  databricks experiments update-experiment EXPERIMENT_ID [flags]

Flags:
  -h, --help              help for update-experiment
      --json JSON         either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --new-name string   If provided, the experiment's name is changed to the new name.

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments update-permissions --help
Update experiment permissions.
  
  Updates the permissions on an experiment. Experiments can inherit permissions
  from their root object.

  Arguments:
    EXPERIMENT_ID: The experiment for which to get or manage permissions.

Usage:
  databricks experiments update-permissions EXPERIMENT_ID [flags]

Flags:
  -h, --help        help for update-permissions
      --json JSON   either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)

>>> $CLI experiments update-run --help
Update a run.
  
  Updates run metadata.

Usage:
  databricks experiments update-run [flags]

Flags:
      --end-time int             Unix timestamp in milliseconds of when the run ended.
  -h, --help                     help for update-run
      --json JSON                either inline JSON string or @path/to/file.json with request body (default JSON (0 bytes))
      --run-id string            ID of the run to update.
      --run-uuid string          [Deprecated, use run_id instead] ID of the run to update.
      --status UpdateRunStatus   Updated status of the run. Supported values: [FAILED, FINISHED, KILLED, RUNNING, SCHEDULED]

Global Flags:
      --debug            enable debug logging
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)
