
>>> [CLI] bundle deploy
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bundle/default/files...
Deploying resources...
Updating deployment state...
Deployment complete!

>>> print_requests
{
  "body": {
    "deployment": {
      "kind": "BUNDLE",
      "metadata_file_path": "/Workspace/Users/[USERNAME]/.bundle/test-bundle/default/state/metadata.json"
    },
    "edit_mode": "UI_LOCKED",
    "format": "MULTI_TASK",
    "job_clusters": [
      {
        "job_cluster_key": "key",
        "new_cluster": {
          "num_workers": 0,
          "spark_version": "13.3.x-scala2.12"
        }
      }
    ],
    "max_concurrent_runs": 1,
    "name": "foo",
    "queue": {
      "enabled": true
    },
    "trigger": {
      "pause_status": "UNPAUSED",
      "periodic": {
        "interval": 1,
        "unit": "DAYS"
      }
    }
  },
  "method": "POST",
  "path": "/api/2.2/jobs/create"
}
jobs foo id='[NUMID]' name='foo'

=== Update trigger.periodic.unit and re-deploy
>>> update_file.py databricks.yml DAYS HOURS

>>> [CLI] bundle deploy
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bundle/default/files...
Deploying resources...
Updating deployment state...
Deployment complete!

>>> print_requests
{
  "body": {
    "job_id": [NUMID],
    "new_settings": {
      "deployment": {
        "kind": "BUNDLE",
        "metadata_file_path": "/Workspace/Users/[USERNAME]/.bundle/test-bundle/default/state/metadata.json"
      },
      "edit_mode": "UI_LOCKED",
      "format": "MULTI_TASK",
      "job_clusters": [
        {
          "job_cluster_key": "key",
          "new_cluster": {
            "num_workers": 0,
            "spark_version": "13.3.x-scala2.12"
          }
        }
      ],
      "max_concurrent_runs": 1,
      "name": "foo",
      "queue": {
        "enabled": true
      },
      "trigger": {
        "pause_status": "UNPAUSED",
        "periodic": {
          "interval": 1,
          "unit": "HOURS"
        }
      }
    }
  },
  "method": "POST",
  "path": "/api/2.2/jobs/reset"
}
jobs foo id='[NUMID]' name='foo'

=== Fetch job ID and verify remote state
>>> [CLI] jobs get [NUMID]
{
  "job_id":[NUMID],
  "settings": {
    "deployment": {
      "kind":"BUNDLE",
      "metadata_file_path":"/Workspace/Users/[USERNAME]/.bundle/test-bundle/default/state/metadata.json"
    },
    "edit_mode":"UI_LOCKED",
    "format":"MULTI_TASK",
    "job_clusters": [
      {
        "job_cluster_key":"key",
        "new_cluster": {
          "num_workers":0,
          "spark_version":"13.3.x-scala2.12"
        }
      }
    ],
    "max_concurrent_runs":1,
    "name":"foo",
    "queue": {
      "enabled":true
    },
    "trigger": {
      "pause_status":"UNPAUSED",
      "periodic": {
        "interval":1,
        "unit":"HOURS"
      }
    }
  }
}

=== Destroy the job and verify that it's removed from the state and from remote
>>> [CLI] bundle destroy --auto-approve
The following resources will be deleted:
  delete job foo

All files and directories at the following location will be deleted: /Workspace/Users/[USERNAME]/.bundle/test-bundle/default

Deleting files...
Destroy complete!

>>> print_requests
{
  "body": {
    "job_id": [NUMID]
  },
  "method": "POST",
  "path": "/api/2.2/jobs/delete"
}
State not found for jobs.foo

>>> musterr [CLI] jobs get [NUMID]
Error: Not Found

Exit code (musterr): 1
