SCHEMA_NAME="main.qm_test_${UNIQUE_NAME}"
TABLE_NAME="${SCHEMA_NAME}.test_table"

trace create_table.py "$TABLE_NAME"

# Create second output schema for testing updates
trace $CLI schemas create "qm_test_${UNIQUE_NAME}_2" main -o json | jq '{browse_only, catalog_name, catalog_type, created_at, created_by, full_name, name, owner, updated_at, updated_by}' || true

cleanup() {
    trace $CLI bundle destroy --auto-approve
    # Clean up schemas and table (suppress errors since they may already be gone)
    $CLI schemas delete "$SCHEMA_NAME" --force 2>/dev/null || true
    $CLI schemas delete "qm_test_${UNIQUE_NAME}_2" --force 2>/dev/null || true
    rm -f out.requests.txt
}
trap cleanup EXIT

# Generate databricks.yml with all variables substituted
envsubst < databricks.yml.tmpl > databricks.yml

trace $CLI bundle deploy

trace $CLI bundle plan | contains.py "1 unchanged"

update_file.py databricks.yml "output_schema_name: main.qm_test_${UNIQUE_NAME}" "output_schema_name: main.qm_test_${UNIQUE_NAME}_2"

trace $CLI bundle plan | contains.py "1 to change"
trace $CLI bundle plan -o json > out.plan.$DATABRICKS_BUNDLE_ENGINE.json

rm out.requests.txt
trace $CLI bundle deploy
# dashboard_id is output only field that terraform adds
trace print_requests.py '^//import-file/' '^//workspace/' '^//telemetry-ext' | grep -v '"dashboard_id":' > out.deploy.requests.json

trace $CLI bundle plan | contains.py "1 unchanged"
