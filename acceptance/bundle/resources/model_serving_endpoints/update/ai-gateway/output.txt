
>>> [CLI] bundle deploy
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/[UNIQUE_NAME]/files...
Deploying resources...
Updating deployment state...
Deployment complete!

>>> [CLI] serving-endpoints get [ENDPOINT_ID]
{
  "inference_table_config": {
    "catalog_name": "first-inference-catalog"
  }
}

>>> update_file.py databricks.yml catalog_name: "first-inference-catalog" catalog_name: "second-inference-catalog"

>>> [CLI] bundle plan -o json

>>> [CLI] bundle deploy
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/[UNIQUE_NAME]/files...
Deploying resources...
Updating deployment state...
Deployment complete!

>>> print_requests.py //serving-endpoints
{
  "method": "POST",
  "path": "/api/2.0/serving-endpoints",
  "body": {
    "ai_gateway": {
      "inference_table_config": {
        "catalog_name": "first-inference-catalog"
      }
    },
    "config": {
      "served_entities": [
        {
          "external_model": {
            "name": "gpt-4o-mini",
            "openai_config": {
              "openai_api_key": "{{secrets/test-scope/openai-key}}"
            },
            "provider": "openai",
            "task": "llm/v1/chat"
          },
          "name": "prod"
        }
      ]
    },
    "name": "[ENDPOINT_ID]"
  }
}
{
  "method": "PUT",
  "path": "/api/2.0/serving-endpoints/[ENDPOINT_ID]/ai-gateway",
  "body": {
    "inference_table_config": {
      "catalog_name": "second-inference-catalog"
    }
  }
}

>>> [CLI] serving-endpoints get [ENDPOINT_ID]
{
  "inference_table_config": {
    "catalog_name": "second-inference-catalog"
  }
}

>>> [CLI] bundle destroy --auto-approve
The following resources will be deleted:
  delete resources.model_serving_endpoints.test_endpoint

All files and directories at the following location will be deleted: /Workspace/Users/[USERNAME]/.bundle/[UNIQUE_NAME]

Deleting files...
Destroy complete!
