envsubst < databricks.yml.tmpl > databricks.yml

cleanup() {
  trace $CLI bundle destroy --auto-approve
}
trap cleanup EXIT

trace $CLI bundle deploy

title "Assert the pipeline is created with catalog"
PIPELINE_ID=$($CLI bundle summary -o json | jq -r '.resources.pipelines.foo.id')
trace $CLI pipelines get "${PIPELINE_ID}" | jq "{spec}"

# Note: In Terraform provider v1.98.0+, changing catalog no longer triggers recreation.
# We switch to using storage location instead, which still triggers recreation.
title "Switch from catalog to storage (triggers recreation)"
trace update_file.py databricks.yml "catalog: main" "storage: dbfs:/my-storage"
# Remove target reference since storage-based pipelines can't use it
trace update_file.py databricks.yml "target: \${resources.schemas.bar.id}" "#target: \${resources.schemas.bar.id}"

trace $CLI bundle plan 2>&1

title "Redeploy the bundle (should recreate the pipeline)"
trace errcode $CLI bundle deploy --force-lock
