
=== Create a pre-defined job:
Created job with ID: [NUMID]

=== Expect binding to fail without an auto-approve flag:
>>> errcode [CLI] bundle deployment bind foo [NUMID]
databricks_job.foo: Refreshing state... [id=[NUMID]]

Terraform used the selected providers to generate the following execution
plan. Resource actions are indicated with the following symbols:
  ~ update in-place

Terraform will perform the following actions:

  # databricks_job.foo will be updated in-place
  ~ resource "databricks_job" "foo" {
      + always_running            = false
      + control_run_state         = false
      + edit_mode                 = "UI_LOCKED"
      + format                    = "MULTI_TASK"
        id                        = "[NUMID]"
      ~ max_concurrent_runs       = 0 -> 1
      ~ name                      = "test-job-bind-[UNIQUE_NAME]" -> "test-job-basic-[UNIQUE_NAME]"
        # (5 unchanged attributes hidden)

      + deployment {
          + kind               = "BUNDLE"
          + metadata_file_path = "/Workspace/Users/[USERNAME]/.bundle/test-bind-job-[UNIQUE_NAME]/state/metadata.json"
        }

      + queue {
          + enabled = true
        }

      ~ task {
            # (6 unchanged attributes hidden)

          ~ spark_python_task {
              ~ python_file = "/Workspace/Users/[USERNAME]/initial_hello_world.py" -> "/Workspace/Users/[USERNAME]/.bundle/test-bind-job-[UNIQUE_NAME]/files/hello_world.py"
                # (1 unchanged attribute hidden)
            }

            # (1 unchanged block hidden)
        }
    }

Plan: 0 to add, 1 to change, 0 to destroy.


Error: failed to bind the resource, err: This bind operation requires user confirmation, but the current console does not support prompting. Please specify --auto-approve if you would like to skip prompts and proceed.

Exit code: 1

=== Check that job is not bound and not updated with config from bundle:
>>> [CLI] jobs get [NUMID]
{
  "job_id": [NUMID],
  "settings": {
    "name": "test-job-bind-[UNIQUE_NAME]",
    "tasks": [
      {
        "task_key": "my_notebook_task",
        "spark_python_task": {
          "python_file": "/Workspace/Users/[USERNAME]/initial_hello_world.py"
        }
      }
    ]
  }
}

=== Bind job with auto-approve:
>>> [CLI] bundle deployment bind foo [NUMID] --auto-approve
Updating deployment state...
Successfully bound job with an id '[NUMID]'. Run 'bundle deploy' to deploy changes to your workspace

=== Remove .databricks directory to simulate fresh deployment:
>>> rm -rf .databricks

=== Deploy bundle:
>>> [CLI] bundle deploy --force-lock --auto-approve
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bind-job-[UNIQUE_NAME]/files...
Deploying resources...
Updating deployment state...
Deployment complete!

=== Read the pre-defined job:
>>> [CLI] jobs get [NUMID]
{
  "job_id": [NUMID],
  "settings": {
    "name": "test-job-basic-[UNIQUE_NAME]",
    "tasks": [
      {
        "task_key": "my_notebook_task",
        "spark_python_task": {
          "python_file": "/Workspace/Users/[USERNAME]/.bundle/test-bind-job-[UNIQUE_NAME]/files/hello_world.py"
        }
      }
    ]
  }
}

=== Unbind the job:
>>> [CLI] bundle deployment unbind foo
Updating deployment state...

=== Remove .databricks directory to simulate fresh deployment:
>>> rm -rf .databricks

=== Destroy the bundle:
>>> [CLI] bundle destroy --auto-approve
All files and directories at the following location will be deleted: /Workspace/Users/[USERNAME]/.bundle/test-bind-job-[UNIQUE_NAME]

Deleting files...
Destroy complete!

=== Read the pre-defined job again (expecting it still exists):
>>> [CLI] jobs get [NUMID]
{
  "job_id": [NUMID],
  "settings": {
    "name": "test-job-basic-[UNIQUE_NAME]",
    "tasks": [
      {
        "task_key": "my_notebook_task",
        "spark_python_task": {
          "python_file": "/Workspace/Users/[USERNAME]/.bundle/test-bind-job-[UNIQUE_NAME]/files/hello_world.py"
        }
      }
    ]
  }
}

=== Delete the pre-defined job [NUMID]:0
