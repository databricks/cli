
>>> [CLI] bundle init default-python --config-file ./input.json --output-dir .

Welcome to the default Python template for Databricks Asset Bundles!
Workspace to use (auto-detected, edit in 'my_default_python_[UNIQUE_NAME]/databricks.yml'): [DATABRICKS_URL]

âœ¨ Your new project has been created in the 'my_default_python_[UNIQUE_NAME]' directory!

Please refer to the README.md file for "getting started" instructions.
See also the documentation at https://docs.databricks.com/dev-tools/bundles/index.html.

>>> diff.py [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python my_default_python_[UNIQUE_NAME]
Only in my_default_python_[UNIQUE_NAME]: .gitignore
--- [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python/README.md
+++ my_default_python_[UNIQUE_NAME]/README.md
@@ -1,5 +1,5 @@
-# my_default_python
+# my_default_python_[UNIQUE_NAME]
 
-The 'my_default_python' project was generated by using the default-python template.
+The 'my_default_python_[UNIQUE_NAME]' project was generated by using the default-python template.
 
 ## Getting started
@@ -21,5 +21,5 @@
     This deploys everything that's defined for this project.
     For example, the default template would deploy a job called
-    `[dev yourname] my_default_python_job` to your workspace.
+    `[dev yourname] my_default_python_[UNIQUE_NAME]_job` to your workspace.
     You can find that job by opening your workpace and clicking on **Workflows**.
 
@@ -30,5 +30,5 @@
 
    Note that the default job from the template has a schedule that runs every day
-   (defined in resources/my_default_python.job.yml). The schedule
+   (defined in resources/my_default_python_[UNIQUE_NAME].job.yml). The schedule
    is paused when deploying in development mode (see
    https://docs.databricks.com/dev-tools/bundles/deployment-modes.html).
--- [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python/databricks.yml
+++ my_default_python_[UNIQUE_NAME]/databricks.yml
@@ -1,6 +1,6 @@
-# This is a Databricks asset bundle definition for my_default_python.
+# This is a Databricks asset bundle definition for my_default_python_[UNIQUE_NAME].
 # See https://docs.databricks.com/dev-tools/bundles/index.html for documentation.
 bundle:
-  name: my_default_python
+  name: my_default_python_[UNIQUE_NAME]
   uuid: [UUID]
 
@@ -26,4 +26,4 @@
       root_path: /Workspace/Users/[USERNAME]/.bundle/${bundle.name}/${bundle.target}
     permissions:
-      - user_name: [USERNAME]
+      - service_principal_name: [USERNAME]
         level: CAN_MANAGE
Only in [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python: out.gitignore
Only in [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python: resources/my_default_python.job.yml
Only in [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python: resources/my_default_python.pipeline.yml
Only in my_default_python_[UNIQUE_NAME]: resources/my_default_python_[UNIQUE_NAME].job.yml
Only in my_default_python_[UNIQUE_NAME]: resources/my_default_python_[UNIQUE_NAME].pipeline.yml
--- [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python/scratch/exploration.ipynb
+++ my_default_python_[UNIQUE_NAME]/scratch/exploration.ipynb
@@ -31,5 +31,5 @@
     "\n",
     "sys.path.append(\"../src\")\n",
-    "from my_default_python import main\n",
+    "from my_default_python_[UNIQUE_NAME] import main\n",
     "\n",
     "main.get_taxis(spark).show(10)"
--- [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python/setup.py
+++ my_default_python_[UNIQUE_NAME]/setup.py
@@ -4,5 +4,5 @@
 This file is primarily used by the setuptools library and typically should not
 be executed directly. See README.md for how to deploy, test, and run
-the my_default_python project.
+the my_default_python_[UNIQUE_NAME] project.
 """
 
@@ -14,21 +14,21 @@
 
 import datetime
-import my_default_python
+import my_default_python_[UNIQUE_NAME]
 
 local_version = datetime.datetime.utcnow().strftime("%Y%m%d.%H%M%S")
 
 setup(
-    name="my_default_python",
+    name="my_default_python_[UNIQUE_NAME]",
     # We use timestamp as Local version identifier (https://peps.python.org/pep-0440/#local-version-identifiers.)
     # to ensure that changes to wheel package are picked up when used on all-purpose clusters
-    version=my_default_python.__version__ + "+" + local_version,
+    version=my_default_python_[UNIQUE_NAME].__version__ + "+" + local_version,
     url="https://databricks.com",
     author="[USERNAME]",
-    description="wheel file based on my_default_python/src",
+    description="wheel file based on my_default_python_[UNIQUE_NAME]/src",
     packages=find_packages(where="./src"),
     package_dir={"": "src"},
     entry_points={
         "packages": [
-            "main=my_default_python.main:main",
+            "main=my_default_python_[UNIQUE_NAME].main:main",
         ],
     },
--- [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python/src/dlt_pipeline.ipynb
+++ my_default_python_[UNIQUE_NAME]/src/dlt_pipeline.ipynb
@@ -15,5 +15,5 @@
     "# DLT pipeline\n",
     "\n",
-    "This Delta Live Tables (DLT) definition is executed using a pipeline defined in resources/my_default_python.pipeline.yml."
+    "This Delta Live Tables (DLT) definition is executed using a pipeline defined in resources/my_default_python_[UNIQUE_NAME].pipeline.yml."
    ]
   },
@@ -32,5 +32,5 @@
    "outputs": [],
    "source": [
-    "# Import DLT and src/my_default_python\n",
+    "# Import DLT and src/my_default_python_[UNIQUE_NAME]\n",
     "import dlt\n",
     "import sys\n",
@@ -38,5 +38,5 @@
     "sys.path.append(spark.conf.get(\"bundle.sourcePath\", \".\"))\n",
     "from pyspark.sql.functions import expr\n",
-    "from my_default_python import main"
+    "from my_default_python_[UNIQUE_NAME] import main"
    ]
   },
Only in [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python: src/my_default_python/__init__.py
Only in [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python: src/my_default_python/main.py
Only in my_default_python_[UNIQUE_NAME]: src/my_default_python_[UNIQUE_NAME]/__init__.py
Only in my_default_python_[UNIQUE_NAME]: src/my_default_python_[UNIQUE_NAME]/main.py
--- [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python/src/notebook.ipynb
+++ my_default_python_[UNIQUE_NAME]/src/notebook.ipynb
@@ -15,5 +15,5 @@
     "# Default notebook\n",
     "\n",
-    "This default notebook is executed using Databricks Workflows as defined in resources/my_default_python.job.yml."
+    "This default notebook is executed using Databricks Workflows as defined in resources/my_default_python_[UNIQUE_NAME].job.yml."
    ]
   },
@@ -45,5 +45,5 @@
    "outputs": [],
    "source": [
-    "from my_default_python import main\n",
+    "from my_default_python_[UNIQUE_NAME] import main\n",
     "\n",
     "main.get_taxis(spark).show(10)"
--- [TESTROOT]/bundle/templates/default-python/integration_classic/../classic/output/my_default_python/tests/main_test.py
+++ my_default_python_[UNIQUE_NAME]/tests/main_test.py
@@ -1,3 +1,3 @@
-from my_default_python.main import get_taxis, get_spark
+from my_default_python_[UNIQUE_NAME].main import get_taxis, get_spark
 
 

>>> [CLI] bundle validate -t dev
Name: my_default_python_[UNIQUE_NAME]
Target: dev
Workspace:
  Host: [DATABRICKS_URL]
  User: [USERNAME]
  Path: /Workspace/Users/[USERNAME]/.bundle/my_default_python_[UNIQUE_NAME]/dev

Validation OK!

>>> [CLI] bundle validate -t dev -o json

>>> [CLI] bundle deploy -t dev
Building python_artifact...
Uploading my_default_python_[UNIQUE_NAME]-0.0.1+20250319.212055-py3-none-any.whl...
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/my_default_python_[UNIQUE_NAME]/dev/files...
Deploying resources...
Updating deployment state...
Deployment complete!

>>> [CLI] bundle summary -t dev
Name: my_default_python_[UNIQUE_NAME]
Target: dev
Workspace:
  Host: [DATABRICKS_URL]
  User: [USERNAME]
  Path: /Workspace/Users/[USERNAME]/.bundle/my_default_python_[UNIQUE_NAME]/dev
Resources:
  Jobs:
    my_default_python_[UNIQUE_NAME]_job:
      Name: [dev [USERNAME]] my_default_python_[UNIQUE_NAME]_job
      URL:  [DATABRICKS_URL]/jobs/261337390489690?o=470576644108500
  Pipelines:
    my_default_python_[UNIQUE_NAME]_pipeline:
      Name: [dev [USERNAME]] my_default_python_[UNIQUE_NAME]_pipeline
      URL:  [DATABRICKS_URL]/pipelines/[UUID]?o=470576644108500

>>> [CLI] bundle summary -t dev -o json

>>> [CLI] bundle destroy -t dev --auto-approve
The following resources will be deleted:
  delete job my_default_python_[UNIQUE_NAME]_job
  delete pipeline my_default_python_[UNIQUE_NAME]_pipeline

All files and directories at the following location will be deleted: /Workspace/Users/[USERNAME]/.bundle/my_default_python_[UNIQUE_NAME]/dev

Deleting files...
Destroy complete!

>>> [CLI] bundle validate -t prod
Name: my_default_python_[UNIQUE_NAME]
Target: prod
Workspace:
  Host: [DATABRICKS_URL]
  User: [USERNAME]
  Path: /Workspace/Users/[USERNAME]/.bundle/my_default_python_[UNIQUE_NAME]/prod

Validation OK!

>>> [CLI] bundle validate -t prod -o json

>>> [CLI] bundle deploy -t prod
Building python_artifact...
Uploading my_default_python_[UNIQUE_NAME]-0.0.1+20250319.212121-py3-none-any.whl...
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/my_default_python_[UNIQUE_NAME]/prod/files...
Deploying resources...
Updating deployment state...
Deployment complete!

>>> [CLI] bundle summary -t prod
Name: my_default_python_[UNIQUE_NAME]
Target: prod
Workspace:
  Host: [DATABRICKS_URL]
  User: [USERNAME]
  Path: /Workspace/Users/[USERNAME]/.bundle/my_default_python_[UNIQUE_NAME]/prod
Resources:
  Jobs:
    my_default_python_[UNIQUE_NAME]_job:
      Name: my_default_python_[UNIQUE_NAME]_job
      URL:  [DATABRICKS_URL]/jobs/903639763879380?o=470576644108500
  Pipelines:
    my_default_python_[UNIQUE_NAME]_pipeline:
      Name: my_default_python_[UNIQUE_NAME]_pipeline
      URL:  [DATABRICKS_URL]/pipelines/[UUID]?o=470576644108500

>>> [CLI] bundle summary -t prod -o json

>>> [CLI] bundle destroy -t prod --auto-approve
The following resources will be deleted:
  delete job my_default_python_[UNIQUE_NAME]_job
  delete pipeline my_default_python_[UNIQUE_NAME]_pipeline

All files and directories at the following location will be deleted: /Workspace/Users/[USERNAME]/.bundle/my_default_python_[UNIQUE_NAME]/prod

Deleting files...
Destroy complete!
