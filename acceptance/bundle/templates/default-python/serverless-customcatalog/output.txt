
>>> [CLI] bundle init default-python --config-file [TESTROOT]/bundle/templates/default-python/serverless-customcatalog/../serverless/input.json --output-dir output
Welcome to the default Python template for Databricks Asset Bundles!

Please answer the below to tailor your project to your preferences.
You can always change your mind and change your configuration in the databricks.yml file later.

Note that [DATABRICKS_URL] is used for initialization
(see https://docs.databricks.com/dev-tools/cli/profiles.html for how to change your profile).

âœ¨ Your new project has been created in the 'my_default_python' directory!

Please refer to the README.md file for "getting started" instructions.
See also the documentation at https://docs.databricks.com/dev-tools/bundles/index.html.

>>> diff.py [TESTROOT]/bundle/templates/default-python/serverless-customcatalog/../serverless/output output/
--- [TESTROOT]/bundle/templates/default-python/serverless-customcatalog/../serverless/output/my_default_python/databricks.yml
+++ output/my_default_python/databricks.yml
@@ -32,5 +32,5 @@
       host: [DATABRICKS_URL]
     variables:
-      catalog: hive_metastore
+      catalog: customcatalog
       schema: ${workspace.current_user.short_name}
   prod:
@@ -41,5 +41,5 @@
       root_path: /Workspace/Users/[USERNAME]/.bundle/${bundle.name}/${bundle.target}
     variables:
-      catalog: hive_metastore
+      catalog: customcatalog
       schema: prod
     permissions:
--- [TESTROOT]/bundle/templates/default-python/serverless-customcatalog/../serverless/output/my_default_python/resources/default_python_etl.pipeline.yml
+++ output/my_default_python/resources/default_python_etl.pipeline.yml
@@ -5,6 +5,5 @@
     default_python_etl:
       name: default_python_etl
-      ## Catalog is required for serverless compute
-      catalog: main
+      catalog: customcatalog
       schema: my_default_python_${bundle.target}
       serverless: true
--- [TESTROOT]/bundle/templates/default-python/serverless-customcatalog/../serverless/output/my_default_python/src/default_python_etl/explorations/sample_exploration.ipynb
+++ output/my_default_python/src/default_python_etl/explorations/sample_exploration.ipynb
@@ -38,5 +38,5 @@
     "# !!! Before performing any data analysis, make sure to run the pipeline to materialize the sample datasets. The tables referenced in this notebook depend on that step./n",
     "/n",
-    "display(spark.sql(/"SELECT * FROM hive_metastore.[USERNAME].sample_trips_jan_01_1034/"))"
+    "display(spark.sql(/"SELECT * FROM customcatalog.[USERNAME].sample_trips_jan_01_1034/"))"
    ]
   }
--- [TESTROOT]/bundle/templates/default-python/serverless-customcatalog/../serverless/output/my_default_python/src/sample_python_file.py
+++ output/my_default_python/src/sample_python_file.py
@@ -5,5 +5,5 @@
 def main():
     parser = argparse.ArgumentParser()
-    parser.add_argument("--catalog", default="hive_metastore")
+    parser.add_argument("--catalog", default="customcatalog")
     parser.add_argument("--schema", default="default")
     args = parser.parse_args()
