
>>> $CLI bundle init default-python --config-file ./input.json --output-dir output

Welcome to the default Python template for Databricks Asset Bundles!
Workspace to use (auto-detected, edit in 'my_default_python/databricks.yml'): http://$DATABRICKS_HOST

âœ¨ Your new project has been created in the 'my_default_python' directory!

Please refer to the README.md file for "getting started" instructions.
See also the documentation at https://docs.databricks.com/dev-tools/bundles/index.html.

>>> $CLI bundle validate -t dev
Name: my_default_python
Target: dev
Workspace:
  Host: http://$DATABRICKS_HOST
  User: $USERNAME
  Path: /Workspace/Users/$USERNAME/.bundle/my_default_python/dev

Validation OK!

>>> $CLI bundle validate -t prod
Name: my_default_python
Target: prod
Workspace:
  Host: http://$DATABRICKS_HOST
  User: $USERNAME
  Path: /Workspace/Users/$USERNAME/.bundle/my_default_python/prod

Validation OK!

>>> ruff format --diff
--- scratch/exploration.ipynb:cell 1
+++ scratch/exploration.ipynb:cell 1
--- scratch/exploration.ipynb:cell 2
+++ scratch/exploration.ipynb:cell 2
@@ -1,5 +1,6 @@
 import sys
-sys.path.append('../src')
+
+sys.path.append("../src")
 from my_default_python import main
 
 main.get_taxis(spark).show(10)

--- setup.py
+++ setup.py
@@ -5,11 +5,13 @@
 be executed directly. See README.md for how to deploy, test, and run
 the my_default_python project.
 """
+
 from setuptools import setup, find_packages
 
 import sys
-sys.path.append('./src')
 
+sys.path.append("./src")
+
 import datetime
 import my_default_python
 
@@ -17,17 +19,15 @@
     name="my_default_python",
     # We use timestamp as Local version identifier (https://peps.python.org/pep-0440/#local-version-identifiers.)
     # to ensure that changes to wheel package are picked up when used on all-purpose clusters
-    version=my_default_python.__version__ + "+" + datetime.datetime.utcnow().strftime("%Y%m%d.%H%M%S"),
+    version=my_default_python.__version__
+    + "+"
+    + datetime.datetime.utcnow().strftime("%Y%m%d.%H%M%S"),
     url="https://databricks.com",
     author="$USERNAME",
     description="wheel file based on my_default_python/src",
-    packages=find_packages(where='./src'),
-    package_dir={'': 'src'},
-    entry_points={
-        "packages": [
-            "main=my_default_python.main:main"
-        ]
-    },
+    packages=find_packages(where="./src"),
+    package_dir={"": "src"},
+    entry_points={"packages": ["main=my_default_python.main:main"]},
     install_requires=[
         # Dependencies in case the output wheel file is used as a library dependency.
         # For defining dependencies, when this package is used in Databricks, see:

--- src/dlt_pipeline.ipynb:cell 2
+++ src/dlt_pipeline.ipynb:cell 2
@@ -1,6 +1,7 @@
 # Import DLT and src/my_default_python
 import dlt
 import sys
+
 sys.path.append(spark.conf.get("bundle.sourcePath", "."))
 from pyspark.sql.functions import expr
 from my_default_python import main
--- src/dlt_pipeline.ipynb:cell 3
+++ src/dlt_pipeline.ipynb:cell 3
@@ -1,7 +1,8 @@
 @dlt.view
 def taxi_raw():
-  return main.get_taxis(spark)
+    return main.get_taxis(spark)
+
 
 @dlt.table
 def filtered_taxis():
-  return dlt.read("taxi_raw").filter(expr("fare_amount < 30"))
+    return dlt.read("taxi_raw").filter(expr("fare_amount < 30"))

--- src/my_default_python/main.py
+++ src/my_default_python/main.py
@@ -1,21 +1,25 @@
 from pyspark.sql import SparkSession, DataFrame
 
+
 def get_taxis(spark: SparkSession) -> DataFrame:
-  return spark.read.table("samples.nyctaxi.trips")
+    return spark.read.table("samples.nyctaxi.trips")
 
 
 # Create a new Databricks Connect session. If this fails,
 # check that you have configured Databricks Connect correctly.
 # See https://docs.databricks.com/dev-tools/databricks-connect.html.
 def get_spark() -> SparkSession:
-  try:
-    from databricks.connect import DatabricksSession
-    return DatabricksSession.builder.getOrCreate()
-  except ImportError:
-    return SparkSession.builder.getOrCreate()
+    try:
+        from databricks.connect import DatabricksSession
+
+        return DatabricksSession.builder.getOrCreate()
+    except ImportError:
+        return SparkSession.builder.getOrCreate()
+
 
 def main():
-  get_taxis(get_spark()).show(5)
+    get_taxis(get_spark()).show(5)
+
 
-if __name__ == '__main__':
-  main()
+if __name__ == "__main__":
+    main()

4 files would be reformatted, 3 files already formatted

Exit code: 1

>>> ruff clean
Removing cache at: .ruff_cache

Exit code: 0
