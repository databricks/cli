--- [TESTROOT]/bundle/templates/default-python/classic/../serverless/output/my_default_python/databricks.yml
+++ output/my_default_python/databricks.yml
@@ -34,4 +34,6 @@
       catalog: hive_metastore
       schema: ${workspace.current_user.short_name}
+    presets:
+      artifacts_dynamic_version: true
   prod:
     mode: production
--- [TESTROOT]/bundle/templates/default-python/classic/../serverless/output/my_default_python/resources/my_default_python_etl.pipeline.yml
+++ output/my_default_python/resources/my_default_python_etl.pipeline.yml
@@ -5,8 +5,7 @@
     my_default_python_etl:
       name: my_default_python_etl
-      # Catalog is required for serverless compute
-      catalog: main
+      ## Specify the 'catalog' field to configure this pipeline to make use of Unity Catalog:
+      # catalog: ${var.catalog}
       schema: ${var.schema}
-      serverless: true
       root_path: "../src/my_default_python_etl"

--- [TESTROOT]/bundle/templates/default-python/classic/../serverless/output/my_default_python/resources/sample_job.job.yml
+++ output/my_default_python/resources/sample_job.job.yml
@@ -26,4 +26,10 @@
           notebook_task:
             notebook_path: ../src/sample_notebook.ipynb
+          job_cluster_key: job_cluster
+          libraries:
+            # By default we just include the .whl file generated for the my_default_python package.
+            # See https://docs.databricks.com/dev-tools/bundles/library-dependencies.html
+            # for more information on how to add other libraries.
+            - whl: ../dist/*.whl
         - task_key: python_wheel_task
           depends_on:
@@ -37,5 +43,10 @@
               - "--schema"
               - "${var.schema}"
-          environment_key: default
+          job_cluster_key: job_cluster
+          libraries:
+            # By default we just include the .whl file generated for the my_default_python package.
+            # See https://docs.databricks.com/dev-tools/bundles/library-dependencies.html
+            # for more information on how to add other libraries.
+            - whl: ../dist/*.whl
         - task_key: refresh_pipeline
           depends_on:
@@ -44,11 +55,11 @@
             pipeline_id: ${resources.pipelines.my_default_python_etl.id}

-      environments:
-        - environment_key: default
-          spec:
-            environment_version: "2"
-            dependencies:
-              # By default we just include the .whl file generated for the my_default_python package.
-              # See https://docs.databricks.com/dev-tools/bundles/library-dependencies.html
-              # for more information on how to add other libraries.
-              - ../dist/*.whl
+      job_clusters:
+        - job_cluster_key: job_cluster
+          new_cluster:
+            spark_version: 16.4.x-scala2.12
+            node_type_id: [NODE_TYPE_ID]
+            data_security_mode: SINGLE_USER
+            autoscale:
+              min_workers: 1
+              max_workers: 4
