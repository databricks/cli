Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bundle-[UNIQUE_NAME]/default/files...
Deploying resources...
Updating deployment state...
Deployment complete!

=== Modify pipeline fields remotely
=== Detect and save changes
Detected changes in 1 resource(s):

Resource: resources.pipelines.my_pipeline
  configuration['key2']: add
  environment.dependencies: replace
  notifications[0].alerts: replace
  notifications[0].email_recipients: replace
  root_path: add
  schema: replace
  tags['foo']: add



=== Configuration changes

>>> diff.py databricks.yml.backup databricks.yml
--- databricks.yml.backup
+++ databricks.yml
@@ -7,18 +7,25 @@
       name: test-pipeline-[UNIQUE_NAME]
       catalog: main
-      schema: default
+      schema: prod
       configuration:
         key1: value1
+        key2: value2
       notifications:
         - email_recipients:
             - success@example.com
+            - admin@example.com
           alerts:
             - on-update-success
+            - on-update-failure
       environment:
         dependencies:
           - --editable /Workspace/foo
+          - --editable /Workspace/bar
       libraries:
         - notebook:
             path: /Users/{{workspace_user_name}}/notebook
+      tags:
+        foo: bar
+      root_path: ./pipeline_root

 targets:

>>> [CLI] bundle destroy --auto-approve
The following resources will be deleted:
  delete resources.pipelines.my_pipeline

This action will result in the deletion of the following Lakeflow Spark Declarative Pipelines along with the
Streaming Tables (STs) and Materialized Views (MVs) managed by them:
  delete resources.pipelines.my_pipeline

All files and directories at the following location will be deleted: /Workspace/Users/[USERNAME]/.bundle/test-bundle-[UNIQUE_NAME]/default

Deleting files...
Destroy complete!
