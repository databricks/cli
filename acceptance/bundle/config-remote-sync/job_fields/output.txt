Uploading dummy.whl...
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bundle-[UNIQUE_NAME]/default/files...
Deploying resources...
Updating deployment state...
Deployment complete!

=== Modify job fields remotely
=== Detect and save changes
Detected changes in 1 resource(s):

<<<<<<< HEAD
<<<<<<< HEAD
=======
>>>>>>> d9d740fa6 (Create parent nodes)
Resource: resources.jobs.my_job
  email_notifications.no_alert_for_skipped_runs: skip
  email_notifications.on_failure: skip
  parameters: update
  tags['team']: update
  trigger.periodic.interval: update


=== Configuration changes
<<<<<<< HEAD

>>> diff.py databricks.yml.backup databricks.yml
--- databricks.yml.backup
+++ databricks.yml
@@ -1,5 +1,4 @@
=======
>>>>>>> d9d740fa6 (Create parent nodes)
 bundle:
   name: test-bundle-[UNIQUE_NAME]
-
 resources:
   jobs:
<<<<<<< HEAD
@@ -8,15 +7,21 @@
=======
     my_job:
       email_notifications:
>>>>>>> d9d740fa6 (Create parent nodes)
         on_success:
           - success@example.com
+        no_alert_for_skipped_runs: true
+        on_failure:
+          - failure@example.com
       parameters:
-        - name: catalog
-          default: main
-        - name: env
-          default: dev
+        - default: main
+          name: catalog
+        - default: dev
+          name: env
+        - default: us-east-1
+          name: region
       trigger:
         periodic:
-          interval: 1
+          interval: 2
           unit: DAYS
<<<<<<< HEAD
       tags:
         env: dev
+        team: data
       environments:
         - environment_key: default
=======
Exit code: 1
>>>>>>> d9b1dcd7c (Capture incorrect behavior with the test)
=======
       environments:
         - environment_key: default
             spark_version: 13.3.x-snapshot-scala2.12
             node_type_id: [NODE_TYPE_ID]
             num_workers: 1
-
+      tags:
+        team: data
 targets:
   default:
     mode: development
>>>>>>> d9d740fa6 (Create parent nodes)
