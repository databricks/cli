Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bundle-[UNIQUE_NAME]/dev/files...
Deploying resources...
Updating deployment state...
Deployment complete!

=== Modify job_one: max_concurrent_runs, rename c_task
=== Add synced_task to job_one (notebook in sync root, resource in subdirectory)
=== Modify job_two: max_concurrent_runs, rename first task, add extra_task
=== Add extra_task to local config (not in saved state, triggers entity-level replace)
=== Detect and save changes
Detected changes in 2 resource(s):

Resource: resources.jobs.job_one
  max_concurrent_runs: replace
  tasks[task_key='a_task'].depends_on[0].task_key: replace
  tasks[task_key='c_task']: remove
  tasks[task_key='c_task_renamed']: add
  tasks[task_key='synced_task']: add

Resource: resources.jobs.job_two
  max_concurrent_runs: replace
  tasks[task_key='extra_task']: replace
  tasks[task_key='extra_task'].new_cluster: add
  tasks[task_key='run_pipeline']: remove
  tasks[task_key='run_pipeline_renamed']: add



=== Changes in job1.yml

>>> diff.py resources/job1.yml.backup resources/job1.yml
--- resources/job1.yml.backup
+++ resources/job1.yml
@@ -4,13 +4,13 @@
       max_concurrent_runs: 1
       tasks:
-        - task_key: c_task
+        - depends_on:
+            - task_key: b_task
+          new_cluster:
+            node_type_id: [NODE_TYPE_ID]
+            num_workers: 1
+            spark_version: 13.3.x-snapshot-scala2.12
           notebook_task:
             notebook_path: /Users/{{workspace_user_name}}/c_task
-          new_cluster:
-            spark_version: 13.3.x-snapshot-scala2.12
-            node_type_id: [NODE_TYPE_ID]
-            num_workers: 1
-          depends_on:
-            - task_key: b_task
+          task_key: c_task_renamed
         - task_key: a_task
           notebook_task:
@@ -21,3 +21,10 @@
             num_workers: 1
           depends_on:
-            - task_key: c_task
+            - task_key: c_task_renamed
+        - new_cluster:
+            node_type_id: [NODE_TYPE_ID]
+            num_workers: 1
+            spark_version: 13.3.x-snapshot-scala2.12
+          notebook_task:
+            notebook_path: ../synced_notebook.py
+          task_key: synced_task

=== Changes in job2.yml

>>> diff.py resources/job2.yml.backup resources/job2.yml
--- resources/job2.yml.backup
+++ resources/job2.yml
@@ -2,13 +2,13 @@
   jobs:
     job_two:
-      max_concurrent_runs: 2
+      max_concurrent_runs: 10
       tasks:
-        - task_key: run_pipeline
+        - new_cluster:
+            node_type_id: [NODE_TYPE_ID]
+            num_workers: 1
+            spark_version: 13.3.x-snapshot-scala2.12
           notebook_task:
             notebook_path: /Users/{{workspace_user_name}}/1
-          new_cluster:
-            spark_version: 13.3.x-snapshot-scala2.12
-            node_type_id: [NODE_TYPE_ID]
-            num_workers: 1
+          task_key: run_pipeline_renamed
         - task_key: etl_pipeline
           notebook_task:
@@ -18,5 +18,9 @@
             node_type_id: [NODE_TYPE_ID]
             num_workers: 2
-        - task_key: extra_task
+        - new_cluster:
+            node_type_id: [NODE_TYPE_ID]
+            num_workers: 1
+            spark_version: 13.3.x-snapshot-scala2.12
           notebook_task:
             notebook_path: /Users/{{workspace_user_name}}/extra
+          task_key: extra_task

>>> [CLI] bundle destroy --auto-approve
The following resources will be deleted:
  delete resources.jobs.job_one
  delete resources.jobs.job_two

All files and directories at the following location will be deleted: /Workspace/Users/[USERNAME]/.bundle/test-bundle-[UNIQUE_NAME]/dev

Deleting files...
Destroy complete!
