
>>> [CLI] bundle generate job --help
Generate bundle configuration for an existing Databricks job.

This command downloads an existing job's configuration and creates bundle files
that you can use to deploy the job to other environments or manage it as code.

Examples:
  # Import a production job for version control
  databricks bundle generate job --existing-job-id 12345 --key my_etl_job

  # Specify custom directories for organization
  databricks bundle generate job --existing-job-id 67890 \
    --key data_pipeline --config-dir resources --source-dir src

What gets generated:
- Job configuration YAML file in the resources directory
- Any associated notebook or Python files in the source directory

After generation, you can deploy this job to other targets using:
  databricks bundle deploy --target staging
  databricks bundle deploy --target prod

Usage:
  databricks bundle generate job [flags]

Flags:
  -d, --config-dir string     Dir path where the output config will be stored (default "resources")
      --existing-job-id int   Job ID of the job to generate config for
  -f, --force                 Force overwrite existing files in the output directory
  -h, --help                  help for job
  -s, --source-dir string     Dir path where the downloaded files will be stored (default "src")

Global Flags:
      --debug            enable debug logging
      --key string       resource key to use for the generated configuration
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)
      --var strings      set values for variables defined in bundle config. Example: --var="foo=bar"
