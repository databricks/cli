
>>> [CLI] bundle generate pipeline --help
Generate bundle configuration for an existing Delta Live Tables pipeline.

This command downloads an existing DLT pipeline's configuration and any associated
notebooks, creating bundle files that you can use to deploy the pipeline to other
environments or manage it as code.

EXAMPLES:
  # Import a production DLT pipeline
  databricks bundle generate pipeline --existing-pipeline-id abc123 --key etl_pipeline

  # Organize files in custom directories
  databricks bundle generate pipeline --existing-pipeline-id def456 \
    --key data_transformation --config-dir resources --source-dir src

WHAT GETS GENERATED:
- Pipeline configuration YAML file with settings and libraries
- Pipeline notebooks downloaded to the source directory
- Updated bundle configuration to reference the new pipeline resource

After generation, you can deploy to other environments and modify settings
like catalogs, schemas, and compute configurations per target.

Usage:
  databricks bundle generate pipeline [flags]

Flags:
  -d, --config-dir string             Dir path where the output config will be stored (default "resources")
      --existing-pipeline-id string   ID of the pipeline to generate config for
  -f, --force                         Force overwrite existing files in the output directory
  -h, --help                          help for pipeline
  -s, --source-dir string             Dir path where the downloaded files will be stored (default "src")

Global Flags:
      --debug            enable debug logging
      --key string       resource key to use for the generated configuration
  -o, --output type      output type: text or json (default text)
  -p, --profile string   ~/.databrickscfg profile
  -t, --target string    bundle target to use (if applicable)
      --var strings      set values for variables defined in bundle config. Example: --var="foo=bar"
