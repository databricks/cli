
>>> [CLI] bundle plan
create jobs.bar
create pipelines.foo

Plan: 2 to add, 0 to change, 0 to delete, 0 unchanged

>>> [CLI] bundle deploy
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bundle/default/files...
Deploying resources...
Updating deployment state...
Deployment complete!

>>> print_requests.py //jobs //pipelines
{
  "method": "POST",
  "path": "/api/2.0/pipelines",
  "body": {
    "catalog": "mycatalog",
    "channel": "CURRENT",
    "deployment": {
      "kind": "BUNDLE",
      "metadata_file_path": "/Workspace/Users/[USERNAME]/.bundle/test-bundle/default/state/metadata.json"
    },
    "edition": "ADVANCED",
    "name": "pipeline foo"
  }
}
{
  "method": "POST",
  "path": "/api/2.2/jobs/create",
  "body": {
    "deployment": {
      "kind": "BUNDLE",
      "metadata_file_path": "/Workspace/Users/[USERNAME]/.bundle/test-bundle/default/state/metadata.json"
    },
    "description": "depends on foo id [FOO_ID]",
    "edit_mode": "UI_LOCKED",
    "format": "MULTI_TASK",
    "max_concurrent_runs": 1,
    "name": "job bar",
    "queue": {
      "enabled": true
    }
  }
}

>>> [CLI] bundle plan
Plan: 0 to add, 0 to change, 0 to delete, 2 unchanged

=== Update catalog, triggering recreate for pipeline; this means updating downstream deps
>>> update_file.py databricks.yml mycatalog mynewcatalog

>>> [CLI] bundle plan
update jobs.bar
recreate pipelines.foo

Plan: 1 to add, 1 to change, 1 to delete, 0 unchanged

>>> [CLI] bundle deploy --auto-approve
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bundle/default/files...

This action will result in the deletion or recreation of the following Lakeflow Declarative Pipelines along with the
Streaming Tables (STs) and Materialized Views (MVs) managed by them. Recreating the pipelines will
restore the defined STs and MVs through full refresh. Note that recreation is necessary when pipeline
properties such as the 'catalog' or 'storage' are changed:
  recreate pipeline foo
Deploying resources...
Updating deployment state...
Deployment complete!

>>> print_requests.py //jobs //pipelines
{
  "method": "DELETE",
  "path": "/api/2.0/pipelines/[FOO_ID]"
}
{
  "method": "POST",
  "path": "/api/2.0/pipelines",
  "body": {
    "catalog": "mynewcatalog",
    "channel": "CURRENT",
    "deployment": {
      "kind": "BUNDLE",
      "metadata_file_path": "/Workspace/Users/[USERNAME]/.bundle/test-bundle/default/state/metadata.json"
    },
    "edition": "ADVANCED",
    "name": "pipeline foo"
  }
}
{
  "method": "POST",
  "path": "/api/2.2/jobs/reset",
  "body": {
    "job_id": [BAR_ID],
    "new_settings": {
      "deployment": {
        "kind": "BUNDLE",
        "metadata_file_path": "/Workspace/Users/[USERNAME]/.bundle/test-bundle/default/state/metadata.json"
      },
      "description": "depends on foo id [UUID]",
      "edit_mode": "UI_LOCKED",
      "email_notifications": {},
      "format": "MULTI_TASK",
      "max_concurrent_runs": 1,
      "name": "job bar",
      "queue": {
        "enabled": true
      },
      "run_as": {
        "user_name": "[USERNAME]"
      },
      "webhook_notifications": {}
    }
  }
}

=== Fetch resource IDs and verify remote state
>>> musterr [CLI] pipelines get [FOO_ID]
Error: The specified pipeline [FOO_ID] was not found.

>>> [CLI] pipelines get [UUID]
{
  "creator_user_name":"[USERNAME]",
  "last_modified":[UNIX_TIME_MILLIS][0],
  "name":"pipeline foo",
  "pipeline_id":"[UUID]",
  "run_as_user_name":"[USERNAME]",
  "spec": {
    "catalog":"mynewcatalog",
    "channel":"CURRENT",
    "deployment": {
      "kind":"BUNDLE",
      "metadata_file_path":"/Workspace/Users/[USERNAME]/.bundle/test-bundle/default/state/metadata.json"
    },
    "edition":"ADVANCED",
    "id":"[UUID]",
    "name":"pipeline foo"
  },
  "state":"IDLE"
}

>>> [CLI] jobs get [BAR_ID]
{
  "created_time":[UNIX_TIME_MILLIS][1],
  "creator_user_name":"[USERNAME]",
  "job_id":[BAR_ID],
  "run_as_user_name":"[USERNAME]",
  "settings": {
    "deployment": {
      "kind":"BUNDLE",
      "metadata_file_path":"/Workspace/Users/[USERNAME]/.bundle/test-bundle/default/state/metadata.json"
    },
    "description":"depends on foo id [UUID]",
    "edit_mode":"UI_LOCKED",
    "email_notifications": {},
    "format":"MULTI_TASK",
    "max_concurrent_runs":1,
    "name":"job bar",
    "queue": {
      "enabled":true
    },
    "run_as": {
      "user_name":"[USERNAME]"
    },
    "timeout_seconds":0,
    "webhook_notifications": {}
  }
}

=== Follow up plan & deploy do nothing
>>> [CLI] bundle plan
Plan: 0 to add, 0 to change, 0 to delete, 2 unchanged

>>> [CLI] bundle deploy
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bundle/default/files...
Deploying resources...
Updating deployment state...
Deployment complete!

>>> print_requests.py //jobs //pipelines

>>> [CLI] bundle destroy --auto-approve
The following resources will be deleted:
  delete job bar
  delete pipeline foo

This action will result in the deletion of the following Lakeflow Declarative Pipelines along with the
Streaming Tables (STs) and Materialized Views (MVs) managed by them:
  delete pipeline foo

All files and directories at the following location will be deleted: /Workspace/Users/[USERNAME]/.bundle/test-bundle/default

Deleting files...
Destroy complete!

>>> print_requests.py --sort //jobs //pipelines
{
  "method": "DELETE",
  "path": "/api/2.0/pipelines/[UUID]"
}
{
  "method": "POST",
  "path": "/api/2.2/jobs/delete",
  "body": {
    "job_id": [BAR_ID]
  }
}

>>> musterr [CLI] pipelines get [FOO_ID]
Error: The specified pipeline [FOO_ID] was not found.

>>> musterr [CLI] pipelines get [UUID]
Error: The specified pipeline [UUID] was not found.

>>> musterr [CLI] jobs get [BAR_ID]
Error: Not Found
