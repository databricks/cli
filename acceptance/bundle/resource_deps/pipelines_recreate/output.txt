
>>> [CLI] bundle plan
create jobs.bar
create pipelines.foo

Plan: 2 to add, 0 to change, 0 to delete, 0 unchanged

>>> [CLI] bundle deploy
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bundle/default/files...
Deploying resources...
Updating deployment state...
Deployment complete!

>>> print_requests.py //jobs //pipelines

>>> [CLI] bundle plan
Plan: 0 to add, 0 to change, 0 to delete, 2 unchanged

=== Update storage, triggering recreate for pipeline; this means updating downstream deps
>>> update_file.py databricks.yml storage: dbfs:/my-storage storage: dbfs:/my-new-storage

>>> [CLI] bundle plan
update jobs.bar
recreate pipelines.foo

Plan: 1 to add, 1 to change, 1 to delete, 0 unchanged

>>> [CLI] bundle deploy --auto-approve
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bundle/default/files...

This action will result in the deletion or recreation of the following Lakeflow Spark Declarative Pipelines along with the
Streaming Tables (STs) and Materialized Views (MVs) managed by them. Recreating the pipelines will
restore the defined STs and MVs through full refresh. Note that recreation is necessary when pipeline
properties such as the 'catalog' or 'storage' are changed:
  recreate resources.pipelines.foo
Deploying resources...
Updating deployment state...
Deployment complete!

>>> print_requests.py //jobs //pipelines

=== Fetch resource IDs and verify remote state
>>> musterr [CLI] pipelines get [FOO_ID]
Error: The specified pipeline [FOO_ID] was not found.

>>> [CLI] pipelines get [FOO_ID_2]
{
  "creator_user_name":"[USERNAME]",
  "last_modified":[UNIX_TIME_MILLIS][0],
  "name":"pipeline foo",
  "pipeline_id":"[FOO_ID_2]",
  "run_as_user_name":"[USERNAME]",
  "spec": {
    "channel":"CURRENT",
    "deployment": {
      "kind":"BUNDLE",
      "metadata_file_path":"/Workspace/Users/[USERNAME]/.bundle/test-bundle/default/state/metadata.json"
    },
    "edition":"ADVANCED",
    "id":"[FOO_ID_2]",
    "name":"pipeline foo",
    "storage":"dbfs:/my-new-storage"
  },
  "state":"IDLE"
}

>>> [CLI] jobs get [BAR_ID]
{
  "created_time": [UNIX_TIME_MILLIS][1],
  "creator_user_name": "[USERNAME]",
  "job_id": [BAR_ID],
  "run_as_user_name": "[USERNAME]",
  "settings": {
    "deployment": {
      "kind": "BUNDLE",
      "metadata_file_path": "/Workspace/Users/[USERNAME]/.bundle/test-bundle/default/state/metadata.json"
    },
    "description": "depends on foo id [FOO_ID_2]",
    "edit_mode": "UI_LOCKED",
    "email_notifications": {},
    "format": "MULTI_TASK",
    "max_concurrent_runs": 1,
    "name": "job bar",
    "queue": {
      "enabled": true
    },
    "timeout_seconds": 0,
    "webhook_notifications": {}
  }
}

=== Follow up plan & deploy do nothing
>>> [CLI] bundle plan
Plan: 0 to add, 0 to change, 0 to delete, 2 unchanged

>>> [CLI] bundle deploy
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/test-bundle/default/files...
Deploying resources...
Updating deployment state...
Deployment complete!

>>> print_requests.py //jobs //pipelines

>>> [CLI] bundle destroy --auto-approve
The following resources will be deleted:
  delete resources.jobs.bar
  delete resources.pipelines.foo

This action will result in the deletion of the following Lakeflow Spark Declarative Pipelines along with the
Streaming Tables (STs) and Materialized Views (MVs) managed by them:
  delete resources.pipelines.foo

All files and directories at the following location will be deleted: /Workspace/Users/[USERNAME]/.bundle/test-bundle/default

Deleting files...
Destroy complete!

>>> print_requests.py //jobs //pipelines
{
  "method": "POST",
  "path": "/api/2.2/jobs/delete",
  "body": {
    "job_id": [BAR_ID]
  }
}
{
  "method": "DELETE",
  "path": "/api/2.0/pipelines/[FOO_ID_2]"
}

>>> musterr [CLI] pipelines get [FOO_ID]
Error: The specified pipeline [FOO_ID] was not found.

>>> musterr [CLI] pipelines get [FOO_ID_2]
Error: The specified pipeline [FOO_ID_2] was not found.

>>> musterr [CLI] jobs get [BAR_ID]
Error: Not Found
