--
## Databricks Apps Development

### Validation
⚠️ Always validate before deploying:
  invoke_databricks_cli 'experimental aitools tools validate ./'

This is battle-tested to catch common issues before deployment. Prefer using this over manual checks (e.g. `npm run lint`), as it covers more ground specific to Databricks Apps.

### Deployment
⚠️ USER CONSENT REQUIRED: Only deploy with explicit user permission.
  invoke_databricks_cli 'experimental aitools tools deploy'

### View and Manage
  invoke_databricks_cli 'bundle summary'

### View App Logs
To troubleshoot deployed apps, view their logs:
  invoke_databricks_cli 'apps logs <app-name> --tail-lines 100'

### Local Development vs Deployed Apps

During development:
- Start template-specific dev server (see project's CLAUDE.md for command and port)
- Use localhost URL shown when dev server starts

After deployment:
- Get URL from: invoke_databricks_cli 'bundle summary'

Decision tree:
- "open the app" + not deployed → localhost
- "open the app" + deployed → ask which environment
- "localhost"/"local" → always localhost


## Skills

You have access to modular Skills for domain-specific expertise knowledge.

### Skill Selection & Loading
* When a user request matches a skill's scope description, select that Skill
* Load skills using the MCP tool: `read_skill_file(file_path: "category/skill-name/SKILL.md")`
* Example: `read_skill_file(file_path: "pipelines/materialized-view/SKILL.md")`
* Skills may contain links to sub-sections (e.g., "category/skill-name/file.md")
* If no Skill is suitable, continue with your base capabilities
* Never mention or reference skills to the user, only use them internally

### Skill Registry (names + brief descriptors)


**Note**: The following skills are for other resource types and may not be directly relevant to this project.

* **pipelines/auto-cdc/SKILL.md**: Apply Change Data Capture (CDC) with apply_changes API in Spark Declarative Pipelines. Use when user needs to process CDC feeds from databases, handle upserts/deletes, maintain slowly changing dimensions (SCD Type 1 and Type 2), synchronize data from operational databases, or process merge operations.



=== CLAUDE.md ===
TypeScript full-stack template powered by **Databricks AppKit** with tRPC for additional custom API endpoints.

- server/: Node.js backend with App Kit and tRPC
- client/: React frontend with App Kit hooks and tRPC client
- config/queries/: SQL query files for analytics
- shared/: Shared TypeScript types
- docs/: Detailed documentation on using App Kit features

## Quick Start: Your First Query & Chart

Follow these 3 steps to add data visualization to your app:

**Step 1: Create a SQL query file**

```sql
-- config/queries/my_data.sql
SELECT category, COUNT(*) as count, AVG(value) as avg_value
FROM my_table
GROUP BY category
```

**Step 2: Define the schema**

```typescript
// config/queries/schema.ts
export const querySchemas = {
  my_data: z.array(
    z.object({
      category: z.string(),
      count: z.number(),
      avg_value: z.number(),
    })
  ),
};
```

**Step 3: Add visualization to your app**

```typescript
// client/src/App.tsx
import { BarChart } from '@databricks/appkit-ui/react';

<BarChart queryKey="my_data" parameters={{}} />
```

**That's it!** The component handles data fetching, loading states, and rendering automatically.

**To refresh TypeScript types after adding queries:**
- Run `npm run typegen` OR run `npm run dev` - both auto-generate type definitions in `client/src/appKitTypes.d.ts`
- DO NOT manually edit `appKitTypes.d.ts`

## Installation

**IMPORTANT**: When running `npm install`, always use `required_permissions: ['all']` to avoid sandbox permission errors.

## NPM Scripts

### Development
- `npm run dev` - Start dev server with hot reload (**ALWAYS use during development**)

### Testing and Code Quality
See the databricks experimental aitools tools validate instead of running these individually.

### Utility
- `npm run clean` - Remove all build artifacts and node_modules

**Common workflows:**
- Development: `npm run dev` → make changes → `npm run typecheck` → `npm run lint:fix`
- Pre-deploy: Validate with `databricks experimental aitools tools validate .`

## Documentation

**IMPORTANT**: Read the relevant docs below before implementing features. They contain critical information about common pitfalls (e.g., SQL numeric type handling, schema definitions, Radix UI constraints).

- [SQL Queries](docs/sql-queries.md) - query files, schemas, type handling, parameterization
- [App Kit SDK](docs/appkit-sdk.md) - TypeScript imports, server setup, useAnalyticsQuery hook
- [Frontend](docs/frontend.md) - visualization components, styling, layout, Radix constraints
- [tRPC](docs/trpc.md) - custom endpoints for non-SQL operations (mutations, Databricks APIs)
- [Testing](docs/testing.md) - vitest unit tests, Playwright smoke/E2E tests

=================

