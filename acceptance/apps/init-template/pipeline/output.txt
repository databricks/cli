--
## Lakeflow Jobs Development

This guidance is for developing jobs in this project.

### Project Structure
- `src/` - Python notebooks (.ipynb) and source code
- `resources/` - Job definitions in databricks.yml format

### Configuring Tasks
Edit `resources/<job_name>.job.yml` to configure tasks:

```yaml
tasks:
  - task_key: my_notebook
    notebook_task:
      notebook_path: ../src/my_notebook.ipynb
  - task_key: my_python
    python_wheel_task:
      package_name: my_package
      entry_point: main
```

Task types: `notebook_task`, `python_wheel_task`, `spark_python_task`, `pipeline_task`, `sql_task`

### Job Parameters
Parameters defined at job level are passed to ALL tasks (no need to repeat per task). Example:
```yaml
resources:
  jobs:
    my_job:
      parameters:
        - name: catalog
          default: ${var.catalog}
        - name: schema
          default: ${var.schema}
```

### Writing Notebook Code
- Use `spark.read.table("catalog.schema.table")` to read tables
- Use `spark.sql("SELECT ...")` for SQL queries
- Use `dbutils.widgets` for parameters

### Unit Testing
Run unit tests locally with: `uv run pytest`

### Documentation
- Lakeflow Jobs: https://docs.databricks.com/jobs
- Task types: https://docs.databricks.com/jobs/configure-task
- Databricks Asset Bundles / yml format examples: https://docs.databricks.com/dev-tools/bundles/examples

## Lakeflow Spark Declarative Pipelines Development

This guidance is for developing pipelines in this project.

Lakeflow Spark Declarative Pipelines (formerly Delta Live Tables) is a framework for building batch and streaming data pipelines.

### Project Structure
- `src/` - Pipeline transformations (Python or SQL)
- `resources/` - Pipeline configuration in databricks.yml format

### Adding Transformations

**Python** - Create `.py` files in `src/`:
```python
from pyspark import pipelines as dp

@dp.table
def my_table():
    return spark.read.table("catalog.schema.source")
```

By convention, each dataset definition like the @dp.table definition above should be in a file named
like the dataset name, e.g. `src/my_table.py`.

**SQL** - Create `.sql` files in `src/`:
```sql
CREATE MATERIALIZED VIEW my_view AS
SELECT * FROM catalog.schema.source
```

This example would live in `src/my_view.sql`.

Use `CREATE STREAMING TABLE` for incremental ingestion, `CREATE MATERIALIZED VIEW` for transformations.

### Scheduling Pipelines
To schedule a pipeline, make sure you have a job that triggers it, like `resources/<name>.job.yml`:
```yaml
resources:
  jobs:
    my_pipeline_job:
      trigger:
        periodic:
          interval: 1
          unit: DAYS
      tasks:
        - task_key: refresh_pipeline
          pipeline_task:
            pipeline_id: ${resources.pipelines.my_pipeline.id}
```

### Documentation
- Lakeflow Spark Declarative Pipelines: https://docs.databricks.com/ldp
- Databricks Asset Bundles / yml format examples: https://docs.databricks.com/dev-tools/bundles/examples

## Adding Databricks Resources

Add resources by creating YAML files in resources/:

**Jobs** - `resources/my_job.job.yml`:
```yaml
resources:
  jobs:
    my_job:
      name: my_job
      tasks:
        - task_key: main
          notebook_task:
            notebook_path: ../src/notebook.py
```

**Pipelines** (Lakeflow Spark Declarative Pipelines) - `resources/my_pipeline.pipeline.yml`:
```yaml
resources:
  pipelines:
    my_pipeline:
      name: my_pipeline
      catalog: ${var.catalog}
      target: ${var.schema}
      libraries:
        - notebook:
            path: ../src/pipeline.py
```

**Dashboards** - `resources/my_dashboard.dashboard.yml`
**Alerts** - `resources/my_alert.alert.yml`
**Model Serving** - `resources/my_endpoint.yml`
**Apps** - `resources/my_app.app.yml`

**Other resource types**: clusters, schemas, volumes, registered_models, experiments, quality_monitors

### Deployment
For dev targets you can deploy without user consent. This allows you to run resources on the workspace too!

  invoke_databricks_cli 'bundle deploy --target dev'
  invoke_databricks_cli 'bundle run <resource_name> --target dev'

View status with `invoke_databricks_cli 'bundle summary'`.

### Documentation
- Resource types reference: https://docs.databricks.com/dev-tools/bundles/resources
- YAML examples: https://docs.databricks.com/dev-tools/bundles/examples


## Skills

You have access to modular Skills for domain-specific expertise knowledge.

### Skill Selection & Loading
* When a user request matches a skill's scope description, select that Skill
* Load skills using the MCP tool: `read_skill_file(file_path: "category/skill-name/SKILL.md")`
* Example: `read_skill_file(file_path: "pipelines/materialized-view/SKILL.md")`
* Skills may contain links to sub-sections (e.g., "category/skill-name/file.md")
* If no Skill is suitable, continue with your base capabilities
* Never mention or reference skills to the user, only use them internally

### Skill Registry (names + brief descriptors)
* **pipelines/auto-cdc/SKILL.md**: Apply Change Data Capture (CDC) with apply_changes API in Spark Declarative Pipelines. Use when user needs to process CDC feeds from databases, handle upserts/deletes, maintain slowly changing dimensions (SCD Type 1 and Type 2), synchronize data from operational databases, or process merge operations.


