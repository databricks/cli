
=== E2E Test: Complete pipeline lifecycle (init, deploy, run, stop, destroy)
=== Initialize pipeline project
>>> [PIPELINES] init --output-dir output
Welcome to the template for Lakeflow Spark Declarative Pipelines!

Please answer the below to tailor your project to your preferences.
You can always change your mind and change your configuration in the databricks.yml file later.

Note that [DATABRICKS_URL] is used for initialization
(see https://docs.databricks.com/dev-tools/cli/profiles.html for how to change your profile).

✨ Your new project has been created in the 'lakeflow_project' directory!

Please refer to the README.md file for "getting started" instructions.

=== Deploy pipeline
>>> [PIPELINES] deploy
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/lakeflow_project/dev/files...
Deploying resources...
Updating deployment state...
Deployment complete!
View your job sample_job here: [DATABRICKS_URL]/jobs/[NUMID]?o=[NUMID]
View your pipeline lakeflow_project_etl here: [DATABRICKS_URL]/pipelines/[UUID]?o=[NUMID]

=== Run pipeline
>>> [PIPELINES] run
Recommendation: This command runs the last deployed version of the code

If you've made local changes, run 'pipelines deploy' first to ensure they are included.

Update URL: [DATABRICKS_URL]/#joblist/pipelines/[UUID]/updates/[UUID]

Update ID: [UUID]

Update for pipeline completed successfully.

Pipeline configurations for this update:
• All tables are refreshed

=== Edit project by creating and running a new second pipeline
>>> [PIPELINES] deploy
Uploading bundle files to /Workspace/Users/[USERNAME]/.bundle/lakeflow_project/dev/files...
Deploying resources...
Updating deployment state...
Deployment complete!
View your job sample_job here: [DATABRICKS_URL]/jobs/[NUMID]?o=[NUMID]
View your pipeline lakeflow_project_etl here: [DATABRICKS_URL]/pipelines/[UUID]?o=[NUMID]
View your pipeline lakeflow_project_etl_2 here: [DATABRICKS_URL]/pipelines/[UUID]?o=[NUMID]

=== Assert the second pipeline is created
>>> [CLI] pipelines get [UUID]
{
  "creator_user_name":"[USERNAME]",
  "last_modified":[UNIX_TIME_MILLIS],
  "name":"[dev [USERNAME]] lakeflow_project_etl_2",
  "pipeline_id":"[UUID]",
  "run_as_user_name":"[USERNAME]",
  "spec": {
    "channel":"CURRENT",
    "deployment": {
      "kind":"BUNDLE",
      "metadata_file_path":"/Workspace/Users/[USERNAME]/.bundle/lakeflow_project/dev/state/metadata.json"
    },
    "development":true,
    "edition":"ADVANCED",
    "id":"[UUID]",
    "name":"[dev [USERNAME]] lakeflow_project_etl_2",
    "storage":"dbfs:/pipelines/[UUID]",
    "tags": {
      "dev":"[USERNAME]"
    }
  },
  "state":"IDLE"
}

>>> [PIPELINES] run lakeflow_project_etl_2
Recommendation: This command runs the last deployed version of the code

If you've made local changes, run 'pipelines deploy' first to ensure they are included.

Update URL: [DATABRICKS_URL]/#joblist/pipelines/[UUID]/updates/[UUID]

Update ID: [UUID]

Update for pipeline completed successfully.

Pipeline configurations for this update:
• All tables are refreshed

=== Stop both pipelines before destroy
>>> [PIPELINES] stop lakeflow_project_etl
Stopping lakeflow_project_etl...
lakeflow_project_etl has been stopped.

>>> [PIPELINES] stop lakeflow_project_etl_2
Stopping lakeflow_project_etl_2...
lakeflow_project_etl_2 has been stopped.

=== Destroy project
>>> [PIPELINES] destroy --auto-approve
The following resources will be deleted:
  delete resources.jobs.sample_job
  delete resources.pipelines.lakeflow_project_etl
  delete resources.pipelines.lakeflow_project_etl_2

This action will result in the deletion of the following Lakeflow Spark Declarative Pipelines along with the
Streaming Tables (STs) and Materialized Views (MVs) managed by them:
  delete resources.pipelines.lakeflow_project_etl
  delete resources.pipelines.lakeflow_project_etl_2

All files and directories at the following location will be deleted: /Workspace/Users/[USERNAME]/.bundle/lakeflow_project/dev

Deleting files...
Destroy complete!
